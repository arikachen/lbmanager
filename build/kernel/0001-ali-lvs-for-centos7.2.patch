From 5575ef323b8d213179908657be033be431032d21 Mon Sep 17 00:00:00 2001
From: ArikaChen <eaglesora@gmail.com>
Date: Mon, 13 Mar 2017 18:38:38 +0800
Subject: [PATCH] ali lvs

Signed-off-by: ArikaChen <eaglesora@gmail.com>
---
 include/net/ip_vs.h                     | 1707 +++++--------
 include/net/ip_vs_synproxy.h            |  135 +
 include/uapi/linux/ip_vs.h              |  351 +--
 net/core/secure_seq.c                   |    1 +
 net/ipv4/syncookies.c                   |   86 +
 net/ipv6/syncookies.c                   |   82 +
 net/netfilter/ipvs/Kconfig              |   66 +-
 net/netfilter/ipvs/Makefile             |   16 +-
 net/netfilter/ipvs/ip_vs_app.c          |  277 +-
 net/netfilter/ipvs/ip_vs_conn.c         | 1447 ++++++-----
 net/netfilter/ipvs/ip_vs_core.c         | 2034 ++++++---------
 net/netfilter/ipvs/ip_vs_ctl.c          | 4184 +++++++++++++++++--------------
 net/netfilter/ipvs/ip_vs_dh.c           |   94 +-
 net/netfilter/ipvs/ip_vs_est.c          |  144 +-
 net/netfilter/ipvs/ip_vs_ftp.c          |  284 +--
 net/netfilter/ipvs/ip_vs_lblc.c         |  249 +-
 net/netfilter/ipvs/ip_vs_lblcr.c        |  372 ++-
 net/netfilter/ipvs/ip_vs_lc.c           |   23 +-
 net/netfilter/ipvs/ip_vs_nfct.c         |  300 ---
 net/netfilter/ipvs/ip_vs_nq.c           |    5 +-
 net/netfilter/ipvs/ip_vs_pe.c           |  111 -
 net/netfilter/ipvs/ip_vs_pe_sip.c       |  171 --
 net/netfilter/ipvs/ip_vs_proto.c        |  186 +-
 net/netfilter/ipvs/ip_vs_proto_ah_esp.c |  149 +-
 net/netfilter/ipvs/ip_vs_proto_sctp.c   | 1137 ---------
 net/netfilter/ipvs/ip_vs_proto_tcp.c    | 1547 ++++++++++--
 net/netfilter/ipvs/ip_vs_proto_udp.c    |  549 ++--
 net/netfilter/ipvs/ip_vs_rr.c           |   66 +-
 net/netfilter/ipvs/ip_vs_sched.c        |   95 +-
 net/netfilter/ipvs/ip_vs_sed.c          |    7 +-
 net/netfilter/ipvs/ip_vs_sh.c           |  114 +-
 net/netfilter/ipvs/ip_vs_stats.c        |   99 +
 net/netfilter/ipvs/ip_vs_sync.c         | 1786 +++----------
 net/netfilter/ipvs/ip_vs_synproxy.c     | 1134 +++++++++
 net/netfilter/ipvs/ip_vs_wlc.c          |   27 +-
 net/netfilter/ipvs/ip_vs_wrr.c          |  197 +-
 net/netfilter/ipvs/ip_vs_xmit.c         | 2297 +++++++++++------
 37 files changed, 10668 insertions(+), 10861 deletions(-)
 create mode 100644 include/net/ip_vs_synproxy.h
 delete mode 100644 net/netfilter/ipvs/ip_vs_nfct.c
 delete mode 100644 net/netfilter/ipvs/ip_vs_pe.c
 delete mode 100644 net/netfilter/ipvs/ip_vs_pe_sip.c
 delete mode 100644 net/netfilter/ipvs/ip_vs_proto_sctp.c
 create mode 100644 net/netfilter/ipvs/ip_vs_stats.c
 create mode 100644 net/netfilter/ipvs/ip_vs_synproxy.c

diff --git a/include/net/ip_vs.h b/include/net/ip_vs.h
index cd8fbda..6f2bd5c 100644
--- a/include/net/ip_vs.h
+++ b/include/net/ip_vs.h
@@ -6,187 +6,49 @@
 #ifndef _NET_IP_VS_H
 #define _NET_IP_VS_H
 
-#include <linux/ip_vs.h>                /* definitions shared with userland */
+#include <linux/ip_vs.h>	/* definitions shared with userland */
 
-#include <asm/types.h>                  /* for __uXX types */
+/* old ipvsadm versions still include this file directly */
+#ifdef __KERNEL__
 
-#include <linux/list.h>                 /* for struct list_head */
-#include <linux/spinlock.h>             /* for struct rwlock_t */
-#include <linux/atomic.h>                 /* for struct atomic_t */
+#include <asm/types.h>		/* for __uXX types */
+
+#include <linux/sysctl.h>	/* for ctl_path */
+#include <linux/list.h>		/* for struct list_head */
+#include <linux/spinlock.h>	/* for struct rwlock_t */
+#include <asm/atomic.h>		/* for struct atomic_t */
 #include <linux/compiler.h>
 #include <linux/timer.h>
-#include <linux/bug.h>
 
 #include <net/checksum.h>
-#include <linux/netfilter.h>		/* for union nf_inet_addr */
+#include <linux/netfilter.h>	/* for union nf_inet_addr */
 #include <linux/ip.h>
-#include <linux/ipv6.h>			/* for struct ipv6hdr */
-#include <net/ipv6.h>
-#if IS_ENABLED(CONFIG_IP_VS_IPV6)
-#include <linux/netfilter_ipv6/ip6_tables.h>
-#endif
-#if IS_ENABLED(CONFIG_NF_CONNTRACK)
-#include <net/netfilter/nf_conntrack.h>
-#endif
-#include <net/net_namespace.h>		/* Netw namespace */
-
-/*
- * Generic access of ipvs struct
- */
-static inline struct netns_ipvs *net_ipvs(struct net* net)
-{
-	return net->ipvs;
-}
-/*
- * Get net ptr from skb in traffic cases
- * use skb_sknet when call is from userland (ioctl or netlink)
- */
-static inline struct net *skb_net(const struct sk_buff *skb)
-{
-#ifdef CONFIG_NET_NS
-#ifdef CONFIG_IP_VS_DEBUG
-	/*
-	 * This is used for debug only.
-	 * Start with the most likely hit
-	 * End with BUG
-	 */
-	if (likely(skb->dev && dev_net(skb->dev)))
-		return dev_net(skb->dev);
-	if (skb_dst(skb) && skb_dst(skb)->dev)
-		return dev_net(skb_dst(skb)->dev);
-	WARN(skb->sk, "Maybe skb_sknet should be used in %s() at line:%d\n",
-		      __func__, __LINE__);
-	if (likely(skb->sk && sock_net(skb->sk)))
-		return sock_net(skb->sk);
-	pr_err("There is no net ptr to find in the skb in %s() line:%d\n",
-		__func__, __LINE__);
-	BUG();
-#else
-	return dev_net(skb->dev ? : skb_dst(skb)->dev);
-#endif
-#else
-	return &init_net;
-#endif
-}
-
-static inline struct net *skb_sknet(const struct sk_buff *skb)
-{
-#ifdef CONFIG_NET_NS
-#ifdef CONFIG_IP_VS_DEBUG
-	/* Start with the most likely hit */
-	if (likely(skb->sk && sock_net(skb->sk)))
-		return sock_net(skb->sk);
-	WARN(skb->dev, "Maybe skb_net should be used instead in %s() line:%d\n",
-		       __func__, __LINE__);
-	if (likely(skb->dev && dev_net(skb->dev)))
-		return dev_net(skb->dev);
-	pr_err("There is no net ptr to find in the skb in %s() line:%d\n",
-		__func__, __LINE__);
-	BUG();
-#else
-	return sock_net(skb->sk);
-#endif
-#else
-	return &init_net;
-#endif
-}
-/*
- * This one needed for single_open_net since net is stored directly in
- * private not as a struct i.e. seq_file_net can't be used.
- */
-static inline struct net *seq_file_single_net(struct seq_file *seq)
-{
-#ifdef CONFIG_NET_NS
-	return (struct net *)seq->private;
-#else
-	return &init_net;
-#endif
-}
-
-/* Connections' size value needed by ip_vs_ctl.c */
-extern int ip_vs_conn_tab_size;
+#include <linux/ipv6.h>		/* for struct ipv6hdr */
+#include <net/ipv6.h>		/* for ipv6_addr_copy */
 
 struct ip_vs_iphdr {
-	__u32 len;	/* IPv4 simply where L4 starts
-			   IPv6 where L4 Transport Header starts */
-	__u16 fragoffs; /* IPv6 fragment offset, 0 if first frag (or not frag)*/
-	__s16 protocol;
-	__s32 flags;
+	int len;
+	__u8 protocol;
 	union nf_inet_addr saddr;
 	union nf_inet_addr daddr;
 };
 
-static inline void *frag_safe_skb_hp(const struct sk_buff *skb, int offset,
-				      int len, void *buffer,
-				      const struct ip_vs_iphdr *ipvsh)
-{
-	return skb_header_pointer(skb, offset, len, buffer);
-}
-
 static inline void
-ip_vs_fill_ip4hdr(const void *nh, struct ip_vs_iphdr *iphdr)
-{
-	const struct iphdr *iph = nh;
-
-	iphdr->len	= iph->ihl * 4;
-	iphdr->fragoffs	= 0;
-	iphdr->protocol	= iph->protocol;
-	iphdr->saddr.ip	= iph->saddr;
-	iphdr->daddr.ip	= iph->daddr;
-}
-
-/* This function handles filling *ip_vs_iphdr, both for IPv4 and IPv6.
- * IPv6 requires some extra work, as finding proper header position,
- * depend on the IPv6 extension headers.
- */
-static inline void
-ip_vs_fill_iph_skb(int af, const struct sk_buff *skb, struct ip_vs_iphdr *iphdr)
-{
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6) {
-		const struct ipv6hdr *iph =
-			(struct ipv6hdr *)skb_network_header(skb);
-		iphdr->saddr.in6 = iph->saddr;
-		iphdr->daddr.in6 = iph->daddr;
-		/* ipv6_find_hdr() updates len, flags */
-		iphdr->len	 = 0;
-		iphdr->flags	 = 0;
-		iphdr->protocol  = ipv6_find_hdr(skb, &iphdr->len, -1,
-						 &iphdr->fragoffs,
-						 &iphdr->flags);
-	} else
-#endif
-	{
-		const struct iphdr *iph =
-			(struct iphdr *)skb_network_header(skb);
-		iphdr->len	= iph->ihl * 4;
-		iphdr->fragoffs	= 0;
-		iphdr->protocol	= iph->protocol;
-		iphdr->saddr.ip	= iph->saddr;
-		iphdr->daddr.ip	= iph->daddr;
-	}
-}
-
-/* This function is a faster version of ip_vs_fill_iph_skb().
- * Where we only populate {s,d}addr (and avoid calling ipv6_find_hdr()).
- * This is used by the some of the ip_vs_*_schedule() functions.
- * (Mostly done to avoid ABI breakage of external schedulers)
- */
-static inline void
-ip_vs_fill_iph_addr_only(int af, const struct sk_buff *skb,
-			 struct ip_vs_iphdr *iphdr)
+ip_vs_fill_iphdr(int af, const void *nh, struct ip_vs_iphdr *iphdr)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6) {
-		const struct ipv6hdr *iph =
-			(struct ipv6hdr *)skb_network_header(skb);
+		const struct ipv6hdr *iph = nh;
+		iphdr->len = sizeof(struct ipv6hdr);
+		iphdr->protocol = iph->nexthdr;
 		iphdr->saddr.in6 = iph->saddr;
 		iphdr->daddr.in6 = iph->daddr;
 	} else
 #endif
 	{
-		const struct iphdr *iph =
-			(struct iphdr *)skb_network_header(skb);
+		const struct iphdr *iph = nh;
+		iphdr->len = iph->ihl * 4;
+		iphdr->protocol = iph->protocol;
 		iphdr->saddr.ip = iph->saddr;
 		iphdr->daddr.ip = iph->daddr;
 	}
@@ -200,22 +62,7 @@ static inline void ip_vs_addr_copy(int af, union nf_inet_addr *dst,
 		dst->in6 = src->in6;
 	else
 #endif
-	dst->ip = src->ip;
-}
-
-static inline void ip_vs_addr_set(int af, union nf_inet_addr *dst,
-				  const union nf_inet_addr *src)
-{
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6) {
-		dst->in6 = src->in6;
-		return;
-	}
-#endif
-	dst->ip = src->ip;
-	dst->all[1] = 0;
-	dst->all[2] = 0;
-	dst->all[3] = 0;
+		dst->ip = src->ip;
 }
 
 static inline int ip_vs_addr_equal(int af, const union nf_inet_addr *a,
@@ -231,7 +78,7 @@ static inline int ip_vs_addr_equal(int af, const union nf_inet_addr *a,
 #ifdef CONFIG_IP_VS_DEBUG
 #include <linux/net.h>
 
-int ip_vs_get_debug_level(void);
+extern int ip_vs_get_debug_level(void);
 
 static inline const char *ip_vs_dbg_addr(int af, char *buf, size_t buf_len,
 					 const union nf_inet_addr *addr,
@@ -240,7 +87,7 @@ static inline const char *ip_vs_dbg_addr(int af, char *buf, size_t buf_len,
 	int len;
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		len = snprintf(&buf[*idx], buf_len - *idx, "[%pI6c]",
+		len = snprintf(&buf[*idx], buf_len - *idx, "[%pI6]",
 			       &addr->in6) + 1;
 	else
 #endif
@@ -282,24 +129,24 @@ static inline const char *ip_vs_dbg_addr(int af, char *buf, size_t buf_len,
 		if (net_ratelimit())					\
 			printk(KERN_DEBUG pr_fmt(msg), ##__VA_ARGS__);	\
 	} while (0)
-#define IP_VS_DBG_PKT(level, af, pp, skb, ofs, msg)			\
+#define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)				\
 	do {								\
 		if (level <= ip_vs_get_debug_level())			\
-			pp->debug_packet(af, pp, skb, ofs, msg);	\
+			pp->debug_packet(pp, skb, ofs, msg);		\
 	} while (0)
-#define IP_VS_DBG_RL_PKT(level, af, pp, skb, ofs, msg)			\
+#define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)			\
 	do {								\
 		if (level <= ip_vs_get_debug_level() &&			\
 		    net_ratelimit())					\
-			pp->debug_packet(af, pp, skb, ofs, msg);	\
+			pp->debug_packet(pp, skb, ofs, msg);		\
 	} while (0)
-#else	/* NO DEBUGGING at ALL */
+#else				/* NO DEBUGGING at ALL */
 #define IP_VS_DBG_BUF(level, msg...)  do {} while (0)
 #define IP_VS_ERR_BUF(msg...)  do {} while (0)
 #define IP_VS_DBG(level, msg...)  do {} while (0)
 #define IP_VS_DBG_RL(msg...)  do {} while (0)
-#define IP_VS_DBG_PKT(level, af, pp, skb, ofs, msg)	do {} while (0)
-#define IP_VS_DBG_RL_PKT(level, af, pp, skb, ofs, msg)	do {} while (0)
+#define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)		do {} while (0)
+#define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)	do {} while (0)
 #endif
 
 #define IP_VS_BUG() BUG()
@@ -329,6 +176,7 @@ static inline const char *ip_vs_dbg_addr(int af, char *buf, size_t buf_len,
 #define LeaveFunction(level)   do {} while (0)
 #endif
 
+#define	IP_VS_WAIT_WHILE(expr)	while (expr) { cpu_relax(); }
 
 /*
  *      The port number of FTP service (in network order).
@@ -371,80 +219,27 @@ enum {
 };
 
 /*
- *	SCTP State Values
- */
-enum ip_vs_sctp_states {
-	IP_VS_SCTP_S_NONE,
-	IP_VS_SCTP_S_INIT_CLI,
-	IP_VS_SCTP_S_INIT_SER,
-	IP_VS_SCTP_S_INIT_ACK_CLI,
-	IP_VS_SCTP_S_INIT_ACK_SER,
-	IP_VS_SCTP_S_ECHO_CLI,
-	IP_VS_SCTP_S_ECHO_SER,
-	IP_VS_SCTP_S_ESTABLISHED,
-	IP_VS_SCTP_S_SHUT_CLI,
-	IP_VS_SCTP_S_SHUT_SER,
-	IP_VS_SCTP_S_SHUT_ACK_CLI,
-	IP_VS_SCTP_S_SHUT_ACK_SER,
-	IP_VS_SCTP_S_CLOSED,
-	IP_VS_SCTP_S_LAST
-};
-
-/*
  *	Delta sequence info structure
  *	Each ip_vs_conn has 2 (output AND input seq. changes).
  *      Only used in the VS/NAT.
  */
 struct ip_vs_seq {
-	__u32			init_seq;	/* Add delta from this seq */
-	__u32			delta;		/* Delta in sequence numbers */
-	__u32			previous_delta;	/* Delta in sequence numbers
-						   before last resized pkt */
-};
-
-/*
- * counters per cpu
- */
-struct ip_vs_counters {
-	__u32		conns;		/* connections scheduled */
-	__u32		inpkts;		/* incoming packets */
-	__u32		outpkts;	/* outgoing packets */
-	__u64		inbytes;	/* incoming bytes */
-	__u64		outbytes;	/* outgoing bytes */
-};
-/*
- * Stats per cpu
- */
-struct ip_vs_cpu_stats {
-	struct ip_vs_counters   ustats;
-	struct u64_stats_sync   syncp;
+	__u32 init_seq;		/* Add delta from this seq */
+	__u32 delta;		/* Delta in sequence numbers */
+	__u32 previous_delta;	/* Delta in sequence numbers
+				   before last resized pkt */
+	__u32 fdata_seq;	/* sequence of first data packet */
 };
 
 /*
  *	IPVS statistics objects
  */
-struct ip_vs_estimator {
-	struct list_head	list;
-
-	u64			last_inbytes;
-	u64			last_outbytes;
-	u32			last_conns;
-	u32			last_inpkts;
-	u32			last_outpkts;
-
-	u32			cps;
-	u32			inpps;
-	u32			outpps;
-	u32			inbps;
-	u32			outbps;
-};
-
 struct ip_vs_stats {
-	struct ip_vs_stats_user	ustats;		/* statistics */
-	struct ip_vs_estimator	est;		/* estimator */
-	struct ip_vs_cpu_stats __percpu	*cpustats;	/* per cpu counters */
-	spinlock_t		lock;		/* spin lock */
-	struct ip_vs_stats_user	ustats0;	/* reset values */
+	__u64 conns;		/* connections scheduled */
+	__u64 inpkts;		/* incoming packets */
+	__u64 outpkts;		/* outgoing packets */
+	__u64 inbytes;		/* incoming bytes */
+	__u64 outbytes;		/* outgoing bytes */
 };
 
 struct dst_entry;
@@ -452,190 +247,188 @@ struct iphdr;
 struct ip_vs_conn;
 struct ip_vs_app;
 struct sk_buff;
-struct ip_vs_proto_data;
 
 struct ip_vs_protocol {
-	struct ip_vs_protocol	*next;
-	char			*name;
-	u16			protocol;
-	u16			num_states;
-	int			dont_defrag;
+	struct ip_vs_protocol *next;
+	char *name;
+	u16 protocol;
+	u16 num_states;
+	int dont_defrag;
+	atomic_t appcnt;	/* counter of proto app incs */
+	int *timeout_table;	/* protocol timeout table */
 
-	void (*init)(struct ip_vs_protocol *pp);
+	void (*init) (struct ip_vs_protocol * pp);
 
-	void (*exit)(struct ip_vs_protocol *pp);
+	void (*exit) (struct ip_vs_protocol * pp);
 
-	int (*init_netns)(struct net *net, struct ip_vs_proto_data *pd);
-
-	void (*exit_netns)(struct net *net, struct ip_vs_proto_data *pd);
-
-	int (*conn_schedule)(int af, struct sk_buff *skb,
-			     struct ip_vs_proto_data *pd,
-			     int *verdict, struct ip_vs_conn **cpp,
-			     struct ip_vs_iphdr *iph);
+	int (*conn_schedule) (int af, struct sk_buff * skb,
+			      struct ip_vs_protocol * pp,
+			      int *verdict, struct ip_vs_conn ** cpp);
 
 	struct ip_vs_conn *
-	(*conn_in_get)(int af,
-		       const struct sk_buff *skb,
-		       const struct ip_vs_iphdr *iph,
-		       int inverse);
+	    (*conn_in_get) (int af,
+			    const struct sk_buff * skb,
+			    struct ip_vs_protocol * pp,
+			    const struct ip_vs_iphdr * iph,
+			    unsigned int proto_off, int inverse, int *res_dir);
 
 	struct ip_vs_conn *
-	(*conn_out_get)(int af,
-			const struct sk_buff *skb,
-			const struct ip_vs_iphdr *iph,
-			int inverse);
+	    (*conn_out_get) (int af,
+			     const struct sk_buff * skb,
+			     struct ip_vs_protocol * pp,
+			     const struct ip_vs_iphdr * iph,
+			     unsigned int proto_off, int inverse, int *res_dir);
+
+	int (*snat_handler) (struct sk_buff * skb,
+			     struct ip_vs_protocol * pp,
+			     struct ip_vs_conn * cp);
+
+	int (*dnat_handler) (struct sk_buff * skb,
+			     struct ip_vs_protocol * pp,
+			     struct ip_vs_conn * cp);
 
-	int (*snat_handler)(struct sk_buff *skb, struct ip_vs_protocol *pp,
-			    struct ip_vs_conn *cp, struct ip_vs_iphdr *iph);
+	int (*fnat_in_handler) (struct sk_buff ** skb_p,
+				struct ip_vs_protocol * pp,
+				struct ip_vs_conn * cp);
 
-	int (*dnat_handler)(struct sk_buff *skb, struct ip_vs_protocol *pp,
-			    struct ip_vs_conn *cp, struct ip_vs_iphdr *iph);
+	int (*fnat_out_handler) (struct sk_buff * skb,
+				 struct ip_vs_protocol * pp,
+				 struct ip_vs_conn * cp);
 
-	int (*csum_check)(int af, struct sk_buff *skb,
-			  struct ip_vs_protocol *pp);
+	int (*csum_check) (int af, struct sk_buff * skb,
+			   struct ip_vs_protocol * pp);
 
-	const char *(*state_name)(int state);
+	const char *(*state_name) (int state);
 
-	void (*state_transition)(struct ip_vs_conn *cp, int direction,
-				 const struct sk_buff *skb,
-				 struct ip_vs_proto_data *pd);
+	int (*state_transition) (struct ip_vs_conn * cp, int direction,
+				 const struct sk_buff * skb,
+				 struct ip_vs_protocol * pp);
 
-	int (*register_app)(struct net *net, struct ip_vs_app *inc);
+	int (*register_app) (struct ip_vs_app * inc);
 
-	void (*unregister_app)(struct net *net, struct ip_vs_app *inc);
+	void (*unregister_app) (struct ip_vs_app * inc);
 
-	int (*app_conn_bind)(struct ip_vs_conn *cp);
+	int (*app_conn_bind) (struct ip_vs_conn * cp);
 
-	void (*debug_packet)(int af, struct ip_vs_protocol *pp,
-			     const struct sk_buff *skb,
-			     int offset,
-			     const char *msg);
+	void (*debug_packet) (struct ip_vs_protocol * pp,
+			      const struct sk_buff * skb,
+			      int offset, const char *msg);
 
-	void (*timeout_change)(struct ip_vs_proto_data *pd, int flags);
+	void (*timeout_change) (struct ip_vs_protocol * pp, int flags);
+
+	int (*set_state_timeout) (struct ip_vs_protocol * pp, char *sname,
+				  int to);
+
+	void (*conn_expire_handler) (struct ip_vs_protocol * pp,
+				     struct ip_vs_conn * cp);
 };
 
+extern struct ip_vs_protocol *ip_vs_proto_get(unsigned short proto);
+
 /*
- * protocol data per netns
+ *      Connection Index Flags
  */
-struct ip_vs_proto_data {
-	struct ip_vs_proto_data	*next;
-	struct ip_vs_protocol	*pp;
-	int			*timeout_table;	/* protocol timeout table */
-	atomic_t		appcnt;		/* counter of proto app incs. */
-	struct tcp_states_t	*tcp_state_table;
-};
+#define IP_VS_CIDX_F_OUT2IN     0x0001	/* packet director, OUTside2INside */
+#define IP_VS_CIDX_F_IN2OUT     0x0002	/* packet director, INside2OUTside */
+#define IP_VS_CIDX_F_DIR_MASK	0x0003	/* packet director mask */
 
-struct ip_vs_protocol   *ip_vs_proto_get(unsigned short proto);
-struct ip_vs_proto_data *ip_vs_proto_data_get(struct net *net,
-					      unsigned short proto);
-
-struct ip_vs_conn_param {
-	struct net			*net;
-	const union nf_inet_addr	*caddr;
-	const union nf_inet_addr	*vaddr;
-	__be16				cport;
-	__be16				vport;
-	__u16				protocol;
-	u16				af;
-
-	const struct ip_vs_pe		*pe;
-	char				*pe_data;
-	__u8				pe_data_len;
+/*
+ *      Connection index in HASH TABLE, each connection has two index
+ */
+struct ip_vs_conn_idx {
+	struct list_head c_list;	/* hashed list heads */
+
+	u16 af;			/* address family */
+	__u16 protocol;		/* Which protocol (TCP/UDP) */
+	union nf_inet_addr s_addr;	/* source address */
+	union nf_inet_addr d_addr;	/* destination address */
+	__be16 s_port;		/* source port */
+	__be16 d_port;		/* destination port */
+
+	struct ip_vs_conn *cp;	/* point to connection */
+	volatile __u16 flags;	/* status flags */
 };
 
 /*
  *	IP_VS structure allocated for each dynamically scheduled connection
  */
 struct ip_vs_conn {
-	struct hlist_node	c_list;         /* hashed list heads */
+	struct ip_vs_conn_idx *in_idx;	/* client-vs hash index */
+	struct ip_vs_conn_idx *out_idx;	/* rs-vs hash index */
+
 	/* Protocol, addresses and port numbers */
-	__be16                  cport;
-	__be16                  dport;
-	__be16                  vport;
-	u16			af;		/* address family */
-	union nf_inet_addr      caddr;          /* client address */
-	union nf_inet_addr      vaddr;          /* virtual address */
-	union nf_inet_addr      daddr;          /* destination address */
-	volatile __u32          flags;          /* status flags */
-	__u16                   protocol;       /* Which protocol (TCP/UDP) */
-#ifdef CONFIG_NET_NS
-	struct net              *net;           /* Name space */
-#endif
+	u16 af;			/* address family */
+	__u16 protocol;		/* Which protocol (TCP/UDP) */
+	union nf_inet_addr caddr;	/* client address */
+	union nf_inet_addr vaddr;	/* virtual address */
+	union nf_inet_addr laddr;	/* local address */
+	union nf_inet_addr daddr;	/* destination address */
+	__be16 cport;
+	__be16 vport;
+	__be16 lport;
+	__be16 dport;
 
 	/* counter and timer */
-	atomic_t		refcnt;		/* reference count */
-	struct timer_list	timer;		/* Expiration timer */
-	volatile unsigned long	timeout;	/* timeout */
+	atomic_t refcnt;	/* reference count */
+	struct timer_list timer;	/* Expiration timer */
+	volatile unsigned long timeout;	/* timeout */
 
 	/* Flags and state transition */
-	spinlock_t              lock;           /* lock for state transition */
-	volatile __u16          state;          /* state info */
-	volatile __u16          old_state;      /* old state, to be used for
-						 * state transition triggerd
-						 * synchronization
-						 */
-	__u32			fwmark;		/* Fire wall mark from skb */
-	unsigned long		sync_endtime;	/* jiffies + sent_retries */
-
+	spinlock_t lock;	/* lock for state transition */
+	volatile __u16 flags;	/* status flags */
+	volatile __u16 state;	/* state info */
+	volatile __u16 old_state;	/* old state, to be used for
+					 * state transition triggerd
+					 * synchronization
+					 */
 	/* Control members */
-	struct ip_vs_conn       *control;       /* Master control connection */
-	atomic_t                n_control;      /* Number of controlled ones */
-	struct ip_vs_dest       *dest;          /* real server */
-	atomic_t                in_pkts;        /* incoming packet counter */
+	struct ip_vs_conn *control;	/* Master control connection */
+	atomic_t n_control;	/* Number of controlled ones */
+	struct ip_vs_dest *dest;	/* real server */
+	struct ip_vs_laddr *local;	/* local address */
+	atomic_t in_pkts;	/* incoming packet counter */
+
+	/* for fullnat */
+	struct ip_vs_seq fnat_seq;
 
 	/* packet transmitter for different forwarding methods.  If it
 	   mangles the packet, it must return NF_DROP or better NF_STOLEN,
 	   otherwise this must be changed to a sk_buff **.
-	   NF_ACCEPT can be returned when destination is local.
 	 */
-	int (*packet_xmit)(struct sk_buff *skb, struct ip_vs_conn *cp,
-			   struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
+	int (*packet_xmit) (struct sk_buff * skb, struct ip_vs_conn * cp,
+			    struct ip_vs_protocol * pp);
 
 	/* Note: we can group the following members into a structure,
 	   in order to save more space, and the following members are
 	   only used in VS/NAT anyway */
-	struct ip_vs_app        *app;           /* bound ip_vs_app object */
-	void                    *app_data;      /* Application private data */
-	struct ip_vs_seq        in_seq;         /* incoming seq. struct */
-	struct ip_vs_seq        out_seq;        /* outgoing seq. struct */
-
-	const struct ip_vs_pe	*pe;
-	char			*pe_data;
-	__u8			pe_data_len;
+	struct ip_vs_app *app;	/* bound ip_vs_app object */
+	void *app_data;		/* Application private data */
+	struct ip_vs_seq in_seq;	/* incoming seq. struct */
+	struct ip_vs_seq out_seq;	/* outgoing seq. struct */
 
-	struct rcu_head		rcu_head;
+	/* syn-proxy related members
+	 */
+	struct ip_vs_seq syn_proxy_seq;	/* seq. used in syn proxy */
+	struct sk_buff_head ack_skb;	/* ack skb, save in step2 */
+	struct sk_buff *syn_skb;	/* saved rs syn packet */
+	atomic_t syn_retry_max;	/* syn retransmition max count */
+
+	/* add for stopping ack storm */
+	__u32 last_seq;		/* seq of the last ack packet */
+	__u32 last_ack_seq;	/* ack seq of the last ack packet */
+	atomic_t dup_ack_cnt;	/* count of repeated ack packets */
+
+	/* for RST */
+	__u32 rs_end_seq;	/* end seq(seq+datalen) of the last ack packet from rs */
+	__u32 rs_ack_seq;	/* ack seq of the last ack packet from rs */
+
+	/* L2 direct response xmit */
+	struct net_device	*indev;
+	unsigned char		src_hwaddr[MAX_ADDR_LEN];
+	unsigned char		dst_hwaddr[MAX_ADDR_LEN];
 };
 
 /*
- *  To save some memory in conn table when name space is disabled.
- */
-static inline struct net *ip_vs_conn_net(const struct ip_vs_conn *cp)
-{
-#ifdef CONFIG_NET_NS
-	return cp->net;
-#else
-	return &init_net;
-#endif
-}
-static inline void ip_vs_conn_net_set(struct ip_vs_conn *cp, struct net *net)
-{
-#ifdef CONFIG_NET_NS
-	cp->net = net;
-#endif
-}
-
-static inline int ip_vs_conn_net_eq(const struct ip_vs_conn *cp,
-				    struct net *net)
-{
-#ifdef CONFIG_NET_NS
-	return cp->net == net;
-#else
-	return 1;
-#endif
-}
-
-/*
  *	Extended internal versions of struct ip_vs_service_user and
  *	ip_vs_dest_user for IPv6 support.
  *
@@ -645,508 +438,320 @@ static inline int ip_vs_conn_net_eq(const struct ip_vs_conn *cp,
  */
 struct ip_vs_service_user_kern {
 	/* virtual service addresses */
-	u16			af;
-	u16			protocol;
-	union nf_inet_addr	addr;		/* virtual ip address */
-	__be16			port;
-	u32			fwmark;		/* firwall mark of service */
+	u16 af;
+	u16 protocol;
+	union nf_inet_addr addr;	/* virtual ip address */
+	u16 port;
+	u32 fwmark;		/* firwall mark of service */
 
 	/* virtual service options */
-	char			*sched_name;
-	char			*pe_name;
-	unsigned int		flags;		/* virtual service flags */
-	unsigned int		timeout;	/* persistent timeout in sec */
-	__be32			netmask;	/* persistent netmask or plen */
+	char *sched_name;
+	unsigned flags;		/* virtual service flags */
+	unsigned timeout;	/* persistent timeout in sec */
+	u32 netmask;		/* persistent netmask */
 };
 
-
 struct ip_vs_dest_user_kern {
 	/* destination server address */
-	union nf_inet_addr	addr;
-	__be16			port;
+	union nf_inet_addr addr;
+	u16 port;
 
 	/* real server options */
-	unsigned int		conn_flags;	/* connection flags */
-	int			weight;		/* destination weight */
+	unsigned conn_flags;	/* connection flags */
+	int weight;		/* destination weight */
 
 	/* thresholds for active connections */
-	u32			u_threshold;	/* upper threshold */
-	u32			l_threshold;	/* lower threshold */
+	u32 u_threshold;	/* upper threshold */
+	u32 l_threshold;	/* lower threshold */
 };
 
+struct ip_vs_laddr_user_kern {
+	union nf_inet_addr addr;	/* ip address */
+};
 
 /*
  *	The information about the virtual service offered to the net
  *	and the forwarding entries
  */
 struct ip_vs_service {
-	struct hlist_node	s_list;   /* for normal service table */
-	struct hlist_node	f_list;   /* for fwmark-based service table */
-	atomic_t		refcnt;   /* reference counter */
-
-	u16			af;       /* address family */
-	__u16			protocol; /* which protocol (TCP/UDP) */
-	union nf_inet_addr	addr;	  /* IP address for virtual service */
-	__be16			port;	  /* port number for the service */
-	__u32                   fwmark;   /* firewall mark of the service */
-	unsigned int		flags;	  /* service status flags */
-	unsigned int		timeout;  /* persistent timeout in ticks */
-	__be32			netmask;  /* grouping granularity, mask/plen */
-	struct net		*net;
-
-	struct list_head	destinations;  /* real server d-linked list */
-	__u32			num_dests;     /* number of servers */
-	struct ip_vs_stats      stats;         /* statistics for the service */
+	struct list_head s_list;	/* for normal service table */
+	struct list_head f_list;	/* for fwmark-based service table */
+	atomic_t refcnt;	/* reference counter */
+	atomic_t usecnt;	/* use counter */
+
+	u16 af;			/* address family */
+	__u16 protocol;		/* which protocol (TCP/UDP) */
+	union nf_inet_addr addr;	/* IP address for virtual service */
+	__be16 port;		/* port number for the service */
+	__u32 fwmark;		/* firewall mark of the service */
+	unsigned flags;		/* service status flags */
+	unsigned timeout;	/* persistent timeout in ticks */
+	__be32 netmask;		/* grouping granularity */
+
+	/* for realservers list */
+	struct list_head destinations;	/* real server d-linked list */
+	__u32 num_dests;	/* number of servers */
+
+	/* for local ip address list, now only used in FULL NAT model */
+	struct list_head laddr_list;	/* local ip address list */
+	rwlock_t laddr_lock;	/* lock for protect curr_laddr */
+	__u32 num_laddrs;	/* number of local ip address */
+	struct list_head *curr_laddr;	/* laddr data list head */
+
+	struct ip_vs_stats *stats;	/* Use per-cpu statistics for the service */
+	struct ip_vs_app *inc;	/* bind conns to this app inc */
 
 	/* for scheduling */
-	struct ip_vs_scheduler __rcu *scheduler; /* bound scheduler object */
-	spinlock_t		sched_lock;    /* lock sched_data */
-	void			*sched_data;   /* scheduler application data */
-
-	/* alternate persistence engine */
-	struct ip_vs_pe __rcu	*pe;
-
-	struct rcu_head		rcu_head;
+	struct ip_vs_scheduler *scheduler;	/* bound scheduler object */
+	rwlock_t sched_lock;	/* lock sched_data */
+	void *sched_data;	/* scheduler application data */
 };
 
-/* Information for cached dst */
-struct ip_vs_dest_dst {
-	struct dst_entry	*dst_cache;	/* destination cache entry */
-	u32			dst_cookie;
-	union nf_inet_addr	dst_saddr;
-	struct rcu_head		rcu_head;
-};
-
-/* In grace period after removing */
-#define IP_VS_DEST_STATE_REMOVING	0x01
 /*
  *	The real server destination forwarding entry
  *	with ip address, port number, and so on.
  */
 struct ip_vs_dest {
-	struct list_head	n_list;   /* for the dests in the service */
-	struct hlist_node	d_list;   /* for table with all the dests */
+	struct list_head n_list;	/* for the dests in the service */
+	struct list_head d_list;	/* for table with all the dests */
 
-	u16			af;		/* address family */
-	__be16			port;		/* port number of the server */
-	union nf_inet_addr	addr;		/* IP address of the server */
-	volatile unsigned int	flags;		/* dest status flags */
-	atomic_t		conn_flags;	/* flags to copy to conn */
-	atomic_t		weight;		/* server weight */
+	u16 af;			/* address family */
+	union nf_inet_addr addr;	/* IP address of the server */
+	__be16 port;		/* port number of the server */
+	volatile unsigned flags;	/* dest status flags */
+	atomic_t conn_flags;	/* flags to copy to conn */
+	atomic_t weight;	/* server weight */
 
-	atomic_t		refcnt;		/* reference counter */
-	struct ip_vs_stats      stats;          /* statistics */
-	unsigned long		state;		/* state flags */
+	atomic_t refcnt;	/* reference counter */
+	struct ip_vs_stats *stats;	/* Use per-cpu statistics for destination server */
 
 	/* connection counters and thresholds */
-	atomic_t		activeconns;	/* active connections */
-	atomic_t		inactconns;	/* inactive connections */
-	atomic_t		persistconns;	/* persistent connections */
-	__u32			u_threshold;	/* upper threshold */
-	__u32			l_threshold;	/* lower threshold */
+	atomic_t activeconns;	/* active connections */
+	atomic_t inactconns;	/* inactive connections */
+	atomic_t persistconns;	/* persistent connections */
+	__u32 u_threshold;	/* upper threshold */
+	__u32 l_threshold;	/* lower threshold */
 
 	/* for destination cache */
-	spinlock_t		dst_lock;	/* lock of dst_cache */
-	struct ip_vs_dest_dst __rcu *dest_dst;	/* cached dst info */
+	spinlock_t dst_lock;	/* lock of dst_cache */
+	struct dst_entry *dst_cache;	/* destination cache entry */
+	u32 dst_rtos;		/* RT_TOS(tos) for dst */
 
 	/* for virtual service */
-	struct ip_vs_service	*svc;		/* service it belongs to */
-	__u16			protocol;	/* which protocol (TCP/UDP) */
-	__be16			vport;		/* virtual port number */
-	union nf_inet_addr	vaddr;		/* virtual IP address */
-	__u32			vfwmark;	/* firewall mark of service */
-
-	struct list_head	t_list;		/* in dest_trash */
-	struct rcu_head		rcu_head;
-	unsigned int		in_rs_table:1;	/* we are in rs_table */
+	struct ip_vs_service *svc;	/* service it belongs to */
+	__u16 protocol;		/* which protocol (TCP/UDP) */
+	union nf_inet_addr vaddr;	/* virtual IP address */
+	__be16 vport;		/* virtual port number */
+	__u32 vfwmark;		/* firewall mark of service */
 };
 
+/*
+ *	Local ip address object, now only used in FULL NAT model
+ */
+struct ip_vs_laddr {
+	struct list_head n_list;	/* for the local address in the service */
+	u16 af;			/* address family */
+	union nf_inet_addr addr;	/* ip address */
+	atomic64_t port;	/* port counts */
+	atomic_t refcnt;	/* reference count */
+
+	atomic64_t port_conflict;	/* conflict counts */
+	atomic_t conn_counts;	/* connects counts */
+};
 
 /*
  *	The scheduler object
  */
 struct ip_vs_scheduler {
-	struct list_head	n_list;		/* d-linked list head */
-	char			*name;		/* scheduler name */
-	atomic_t		refcnt;		/* reference counter */
-	struct module		*module;	/* THIS_MODULE/NULL */
+	struct list_head n_list;	/* d-linked list head */
+	char *name;		/* scheduler name */
+	atomic_t refcnt;	/* reference counter */
+	struct module *module;	/* THIS_MODULE/NULL */
 
 	/* scheduler initializing service */
-	int (*init_service)(struct ip_vs_service *svc);
+	int (*init_service) (struct ip_vs_service * svc);
 	/* scheduling service finish */
-	void (*done_service)(struct ip_vs_service *svc);
-	/* dest is linked */
-	int (*add_dest)(struct ip_vs_service *svc, struct ip_vs_dest *dest);
-	/* dest is unlinked */
-	int (*del_dest)(struct ip_vs_service *svc, struct ip_vs_dest *dest);
-	/* dest is updated */
-	int (*upd_dest)(struct ip_vs_service *svc, struct ip_vs_dest *dest);
+	int (*done_service) (struct ip_vs_service * svc);
+	/* scheduler updating service */
+	int (*update_service) (struct ip_vs_service * svc);
 
 	/* selecting a server from the given service */
-	struct ip_vs_dest* (*schedule)(struct ip_vs_service *svc,
-				       const struct sk_buff *skb);
-};
-
-/* The persistence engine object */
-struct ip_vs_pe {
-	struct list_head	n_list;		/* d-linked list head */
-	char			*name;		/* scheduler name */
-	atomic_t		refcnt;		/* reference counter */
-	struct module		*module;	/* THIS_MODULE/NULL */
-
-	/* get the connection template, if any */
-	int (*fill_param)(struct ip_vs_conn_param *p, struct sk_buff *skb);
-	bool (*ct_match)(const struct ip_vs_conn_param *p,
-			 struct ip_vs_conn *ct);
-	u32 (*hashkey_raw)(const struct ip_vs_conn_param *p, u32 initval,
-			   bool inverse);
-	int (*show_pe_data)(const struct ip_vs_conn *cp, char *buf);
+	struct ip_vs_dest *(*schedule) (struct ip_vs_service * svc,
+					const struct sk_buff * skb);
 };
 
 /*
  *	The application module object (a.k.a. app incarnation)
  */
 struct ip_vs_app {
-	struct list_head	a_list;		/* member in app list */
-	int			type;		/* IP_VS_APP_TYPE_xxx */
-	char			*name;		/* application module name */
-	__u16			protocol;
-	struct module		*module;	/* THIS_MODULE/NULL */
-	struct list_head	incs_list;	/* list of incarnations */
+	struct list_head a_list;	/* member in app list */
+	int type;		/* IP_VS_APP_TYPE_xxx */
+	char *name;		/* application module name */
+	__u16 protocol;
+	struct module *module;	/* THIS_MODULE/NULL */
+	struct list_head incs_list;	/* list of incarnations */
 
 	/* members for application incarnations */
-	struct list_head	p_list;		/* member in proto app list */
-	struct ip_vs_app	*app;		/* its real application */
-	__be16			port;		/* port number in net order */
-	atomic_t		usecnt;		/* usage counter */
-	struct rcu_head		rcu_head;
-
-	/*
-	 * output hook: Process packet in inout direction, diff set for TCP.
-	 * Return: 0=Error, 1=Payload Not Mangled/Mangled but checksum is ok,
-	 *	   2=Mangled but checksum was not updated
-	 */
-	int (*pkt_out)(struct ip_vs_app *, struct ip_vs_conn *,
-		       struct sk_buff *, int *diff);
+	struct list_head p_list;	/* member in proto app list */
+	struct ip_vs_app *app;	/* its real application */
+	__be16 port;		/* port number in net order */
+	atomic_t usecnt;	/* usage counter */
 
-	/*
-	 * input hook: Process packet in outin direction, diff set for TCP.
-	 * Return: 0=Error, 1=Payload Not Mangled/Mangled but checksum is ok,
-	 *	   2=Mangled but checksum was not updated
-	 */
-	int (*pkt_in)(struct ip_vs_app *, struct ip_vs_conn *,
-		      struct sk_buff *, int *diff);
+	/* output hook: return false if can't linearize. diff set for TCP.  */
+	int (*pkt_out) (struct ip_vs_app *, struct ip_vs_conn *,
+			struct sk_buff *, int *diff);
+
+	/* input hook: return false if can't linearize. diff set for TCP. */
+	int (*pkt_in) (struct ip_vs_app *, struct ip_vs_conn *,
+		       struct sk_buff *, int *diff);
 
 	/* ip_vs_app initializer */
-	int (*init_conn)(struct ip_vs_app *, struct ip_vs_conn *);
+	int (*init_conn) (struct ip_vs_app *, struct ip_vs_conn *);
 
 	/* ip_vs_app finish */
-	int (*done_conn)(struct ip_vs_app *, struct ip_vs_conn *);
-
+	int (*done_conn) (struct ip_vs_app *, struct ip_vs_conn *);
 
 	/* not used now */
-	int (*bind_conn)(struct ip_vs_app *, struct ip_vs_conn *,
-			 struct ip_vs_protocol *);
+	int (*bind_conn) (struct ip_vs_app *, struct ip_vs_conn *,
+			  struct ip_vs_protocol *);
 
-	void (*unbind_conn)(struct ip_vs_app *, struct ip_vs_conn *);
+	void (*unbind_conn) (struct ip_vs_app *, struct ip_vs_conn *);
 
-	int *			timeout_table;
-	int *			timeouts;
-	int			timeouts_size;
+	int *timeout_table;
+	int *timeouts;
+	int timeouts_size;
 
-	int (*conn_schedule)(struct sk_buff *skb, struct ip_vs_app *app,
-			     int *verdict, struct ip_vs_conn **cpp);
+	int (*conn_schedule) (struct sk_buff * skb, struct ip_vs_app * app,
+			      int *verdict, struct ip_vs_conn ** cpp);
 
 	struct ip_vs_conn *
-	(*conn_in_get)(const struct sk_buff *skb, struct ip_vs_app *app,
-		       const struct iphdr *iph, int inverse);
+	    (*conn_in_get) (const struct sk_buff * skb, struct ip_vs_app * app,
+			    const struct iphdr * iph, unsigned int proto_off,
+			    int inverse);
 
 	struct ip_vs_conn *
-	(*conn_out_get)(const struct sk_buff *skb, struct ip_vs_app *app,
-			const struct iphdr *iph, int inverse);
+	    (*conn_out_get) (const struct sk_buff * skb, struct ip_vs_app * app,
+			     const struct iphdr * iph, unsigned int proto_off,
+			     int inverse);
 
-	int (*state_transition)(struct ip_vs_conn *cp, int direction,
-				const struct sk_buff *skb,
-				struct ip_vs_app *app);
+	int (*state_transition) (struct ip_vs_conn * cp, int direction,
+				 const struct sk_buff * skb,
+				 struct ip_vs_app * app);
 
-	void (*timeout_change)(struct ip_vs_app *app, int flags);
+	void (*timeout_change) (struct ip_vs_app * app, int flags);
 };
 
-struct ipvs_master_sync_state {
-	struct list_head	sync_queue;
-	struct ip_vs_sync_buff	*sync_buff;
-	int			sync_queue_len;
-	unsigned int		sync_queue_delay;
-	struct task_struct	*master_thread;
-	struct delayed_work	master_wakeup_work;
-	struct netns_ipvs	*ipvs;
+#define TCPOPT_ADDR  254
+#define TCPOLEN_ADDR 8		/* |opcode|size|ip+port| = 1 + 1 + 6 */
+
+/*
+ * insert client ip in tcp option, now only support IPV4,
+ * must be 4 bytes alignment.
+ */
+struct ip_vs_tcpo_addr {
+	__u8 opcode;
+	__u8 opsize;
+	__u16 port;
+	__u32 addr;
 };
 
-/* How much time to keep dests in trash */
-#define IP_VS_DEST_TRASH_PERIOD		(120 * HZ)
+#ifdef CONFIG_IP_VS_IPV6
+#define TCPOPT_ADDR_V6	253
+#define TCPOLEN_ADDR_V6	20	/* |opcode|size|port|ipv6| = 1 + 1 + 2 + 16 */
 
-/* IPVS in network namespace */
-struct netns_ipvs {
-	int			gen;		/* Generation */
-	int			enable;		/* enable like nf_hooks do */
-	/*
-	 *	Hash table: for real service lookups
-	 */
-	#define IP_VS_RTAB_BITS 4
-	#define IP_VS_RTAB_SIZE (1 << IP_VS_RTAB_BITS)
-	#define IP_VS_RTAB_MASK (IP_VS_RTAB_SIZE - 1)
-
-	struct hlist_head	rs_table[IP_VS_RTAB_SIZE];
-	/* ip_vs_app */
-	struct list_head	app_list;
-	/* ip_vs_proto */
-	#define IP_VS_PROTO_TAB_SIZE	32	/* must be power of 2 */
-	struct ip_vs_proto_data *proto_data_table[IP_VS_PROTO_TAB_SIZE];
-	/* ip_vs_proto_tcp */
-#ifdef CONFIG_IP_VS_PROTO_TCP
-	#define	TCP_APP_TAB_BITS	4
-	#define	TCP_APP_TAB_SIZE	(1 << TCP_APP_TAB_BITS)
-	#define	TCP_APP_TAB_MASK	(TCP_APP_TAB_SIZE - 1)
-	struct list_head	tcp_apps[TCP_APP_TAB_SIZE];
-#endif
-	/* ip_vs_proto_udp */
-#ifdef CONFIG_IP_VS_PROTO_UDP
-	#define	UDP_APP_TAB_BITS	4
-	#define	UDP_APP_TAB_SIZE	(1 << UDP_APP_TAB_BITS)
-	#define	UDP_APP_TAB_MASK	(UDP_APP_TAB_SIZE - 1)
-	struct list_head	udp_apps[UDP_APP_TAB_SIZE];
-#endif
-	/* ip_vs_proto_sctp */
-#ifdef CONFIG_IP_VS_PROTO_SCTP
-	#define SCTP_APP_TAB_BITS	4
-	#define SCTP_APP_TAB_SIZE	(1 << SCTP_APP_TAB_BITS)
-	#define SCTP_APP_TAB_MASK	(SCTP_APP_TAB_SIZE - 1)
-	/* Hash table for SCTP application incarnations	 */
-	struct list_head	sctp_apps[SCTP_APP_TAB_SIZE];
-#endif
-	/* ip_vs_conn */
-	atomic_t		conn_count;      /*  connection counter */
-
-	/* ip_vs_ctl */
-	struct ip_vs_stats		tot_stats;  /* Statistics & est. */
-
-	int			num_services;    /* no of virtual services */
-
-	/* Trash for destinations */
-	struct list_head	dest_trash;
-	spinlock_t		dest_trash_lock;
-	struct timer_list	dest_trash_timer; /* expiration timer */
-	/* Service counters */
-	atomic_t		ftpsvc_counter;
-	atomic_t		nullsvc_counter;
-
-#ifdef CONFIG_SYSCTL
-	/* 1/rate drop and drop-entry variables */
-	struct delayed_work	defense_work;   /* Work handler */
-	int			drop_rate;
-	int			drop_counter;
-	atomic_t		dropentry;
-	/* locks in ctl.c */
-	spinlock_t		dropentry_lock;  /* drop entry handling */
-	spinlock_t		droppacket_lock; /* drop packet handling */
-	spinlock_t		securetcp_lock;  /* state and timeout tables */
-
-	/* sys-ctl struct */
-	struct ctl_table_header	*sysctl_hdr;
-	struct ctl_table	*sysctl_tbl;
+/*
+ * insert client ip in tcp option, for IPv6
+ * must be 4 bytes alignment.
+ */
+struct ip_vs_tcpo_addr_v6 {
+	__u8	opcode;
+	__u8	opsize;
+	__be16	port;
+	struct in6_addr addr;
+};
 #endif
 
-	/* sysctl variables */
-	int			sysctl_amemthresh;
-	int			sysctl_am_droprate;
-	int			sysctl_drop_entry;
-	int			sysctl_drop_packet;
-	int			sysctl_secure_tcp;
-#ifdef CONFIG_IP_VS_NFCT
-	int			sysctl_conntrack;
-#endif
-	int			sysctl_snat_reroute;
-	int			sysctl_sync_ver;
-	int			sysctl_sync_ports;
-	int			sysctl_sync_qlen_max;
-	int			sysctl_sync_sock_size;
-	int			sysctl_cache_bypass;
-	int			sysctl_expire_nodest_conn;
-	int			sysctl_expire_quiescent_template;
-	int			sysctl_sync_threshold[2];
-	unsigned int		sysctl_sync_refresh_period;
-	int			sysctl_sync_retries;
-	int			sysctl_nat_icmp_send;
-	int			sysctl_pmtu_disc;
-	int			sysctl_backup_only;
-	int			sysctl_conn_reuse_mode;
-
-	/* ip_vs_lblc */
-	int			sysctl_lblc_expiration;
-	struct ctl_table_header	*lblc_ctl_header;
-	struct ctl_table	*lblc_ctl_table;
-	/* ip_vs_lblcr */
-	int			sysctl_lblcr_expiration;
-	struct ctl_table_header	*lblcr_ctl_header;
-	struct ctl_table	*lblcr_ctl_table;
-	/* ip_vs_est */
-	struct list_head	est_list;	/* estimator list */
-	spinlock_t		est_lock;
-	struct timer_list	est_timer;	/* Estimation timer */
-	/* ip_vs_sync */
-	spinlock_t		sync_lock;
-	struct ipvs_master_sync_state *ms;
-	spinlock_t		sync_buff_lock;
-	struct task_struct	**backup_threads;
-	int			threads_mask;
-	int			send_mesg_maxlen;
-	int			recv_mesg_maxlen;
-	volatile int		sync_state;
-	volatile int		master_syncid;
-	volatile int		backup_syncid;
-	struct mutex		sync_mutex;
-	/* multicast interface name */
-	char			master_mcast_ifn[IP_VS_IFNAME_MAXLEN];
-	char			backup_mcast_ifn[IP_VS_IFNAME_MAXLEN];
-	/* net name space ptr */
-	struct net		*net;            /* Needed by timer routines */
+/*
+ * statistics for FULLNAT and SYNPROXY
+ * in /proc/net/ip_vs_ext_stats
+ */
+enum {
+	FULLNAT_ADD_TOA_OK = 1,
+	FULLNAT_ADD_TOA_FAIL_LEN,
+	FULLNAT_ADD_TOA_HEAD_FULL,
+	FULLNAT_ADD_TOA_FAIL_MEM,
+	FULLNAT_ADD_TOA_FAIL_PROTO,
+	FULLNAT_CONN_REUSED,
+	FULLNAT_CONN_REUSED_CLOSE,
+	FULLNAT_CONN_REUSED_TIMEWAIT,
+	FULLNAT_CONN_REUSED_FINWAIT,
+	FULLNAT_CONN_REUSED_CLOSEWAIT,
+	FULLNAT_CONN_REUSED_LASTACK,
+	FULLNAT_CONN_REUSED_ESTAB,
+	SYNPROXY_RS_ERROR,
+	SYNPROXY_NULL_ACK,
+	SYNPROXY_BAD_ACK,
+	SYNPROXY_OK_ACK,
+	SYNPROXY_SYN_CNT,
+	SYNPROXY_ACK_STORM,
+	SYNPROXY_SYNSEND_QLEN,
+	SYNPROXY_CONN_REUSED,
+	SYNPROXY_CONN_REUSED_CLOSE,
+	SYNPROXY_CONN_REUSED_TIMEWAIT,
+	SYNPROXY_CONN_REUSED_FINWAIT,
+	SYNPROXY_CONN_REUSED_CLOSEWAIT,
+	SYNPROXY_CONN_REUSED_LASTACK,
+	DEFENCE_IP_FRAG_DROP,
+	DEFENCE_IP_FRAG_GATHER,
+	DEFENCE_TCP_DROP,
+	DEFENCE_UDP_DROP,
+	FAST_XMIT_REJECT,
+	FAST_XMIT_PASS,
+	FAST_XMIT_SKB_COPY,
+	FAST_XMIT_NO_MAC,
+	FAST_XMIT_SYNPROXY_SAVE,
+	FAST_XMIT_DEV_LOST,
+	RST_IN_SYN_SENT,
+	RST_OUT_SYN_SENT,
+	RST_IN_ESTABLISHED,
+	RST_OUT_ESTABLISHED,
+	GRO_PASS,
+	LRO_REJECT,
+	XMIT_UNEXPECTED_MTU,
+	CONN_SCHED_UNREACH,
+	IP_VS_EXT_STAT_LAST
 };
 
-#define DEFAULT_SYNC_THRESHOLD	3
-#define DEFAULT_SYNC_PERIOD	50
-#define DEFAULT_SYNC_VER	1
-#define DEFAULT_SYNC_REFRESH_PERIOD	(0U * HZ)
-#define DEFAULT_SYNC_RETRIES		0
-#define IPVS_SYNC_WAKEUP_RATE	8
-#define IPVS_SYNC_QLEN_MAX	(IPVS_SYNC_WAKEUP_RATE * 4)
-#define IPVS_SYNC_SEND_DELAY	(HZ / 50)
-#define IPVS_SYNC_CHECK_PERIOD	HZ
-#define IPVS_SYNC_FLUSH_TIME	(HZ * 2)
-#define IPVS_SYNC_PORTS_MAX	(1 << 6)
-
-#ifdef CONFIG_SYSCTL
-
-static inline int sysctl_sync_threshold(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_sync_threshold[0];
-}
-
-static inline int sysctl_sync_period(struct netns_ipvs *ipvs)
-{
-	return ACCESS_ONCE(ipvs->sysctl_sync_threshold[1]);
-}
-
-static inline unsigned int sysctl_sync_refresh_period(struct netns_ipvs *ipvs)
-{
-	return ACCESS_ONCE(ipvs->sysctl_sync_refresh_period);
-}
-
-static inline int sysctl_sync_retries(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_sync_retries;
-}
-
-static inline int sysctl_sync_ver(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_sync_ver;
-}
-
-static inline int sysctl_sync_ports(struct netns_ipvs *ipvs)
-{
-	return ACCESS_ONCE(ipvs->sysctl_sync_ports);
-}
-
-static inline int sysctl_sync_qlen_max(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_sync_qlen_max;
-}
-
-static inline int sysctl_sync_sock_size(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_sync_sock_size;
-}
-
-static inline int sysctl_pmtu_disc(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_pmtu_disc;
-}
-
-static inline int sysctl_backup_only(struct netns_ipvs *ipvs)
-{
-	return ipvs->sync_state & IP_VS_STATE_BACKUP &&
-	       ipvs->sysctl_backup_only;
-}
-
-static inline int sysctl_conn_reuse_mode(struct netns_ipvs *ipvs)
-{
-	return ipvs->sysctl_conn_reuse_mode;
-}
-
-#else
-
-static inline int sysctl_sync_threshold(struct netns_ipvs *ipvs)
-{
-	return DEFAULT_SYNC_THRESHOLD;
-}
-
-static inline int sysctl_sync_period(struct netns_ipvs *ipvs)
-{
-	return DEFAULT_SYNC_PERIOD;
-}
-
-static inline unsigned int sysctl_sync_refresh_period(struct netns_ipvs *ipvs)
-{
-	return DEFAULT_SYNC_REFRESH_PERIOD;
-}
-
-static inline int sysctl_sync_retries(struct netns_ipvs *ipvs)
-{
-	return DEFAULT_SYNC_RETRIES & 3;
-}
-
-static inline int sysctl_sync_ver(struct netns_ipvs *ipvs)
-{
-	return DEFAULT_SYNC_VER;
-}
-
-static inline int sysctl_sync_ports(struct netns_ipvs *ipvs)
-{
-	return 1;
-}
-
-static inline int sysctl_sync_qlen_max(struct netns_ipvs *ipvs)
-{
-	return IPVS_SYNC_QLEN_MAX;
-}
+struct ip_vs_estats_entry {
+	char *name;
+	int entry;
+};
 
-static inline int sysctl_sync_sock_size(struct netns_ipvs *ipvs)
-{
-	return 0;
+#define IP_VS_ESTATS_ITEM(_name, _entry) { \
+        .name = _name,            \
+        .entry = _entry,          \
 }
 
-static inline int sysctl_pmtu_disc(struct netns_ipvs *ipvs)
-{
-	return 1;
+#define IP_VS_ESTATS_LAST {    \
+        NULL,           \
+        0,              \
 }
 
-static inline int sysctl_backup_only(struct netns_ipvs *ipvs)
-{
-	return 0;
-}
+struct ip_vs_estats_mib {
+	unsigned long mibs[IP_VS_EXT_STAT_LAST];
+};
 
-static inline int sysctl_conn_reuse_mode(struct netns_ipvs *ipvs)
-{
-	return 1;
-}
+#define IP_VS_INC_ESTATS(mib, field)         \
+        (per_cpu_ptr(mib, smp_processor_id())->mibs[field]++)
 
-#endif
+extern struct ip_vs_estats_mib *ip_vs_esmib;
 
 /*
  *      IPVS core functions
  *      (from ip_vs_core.c)
  */
-const char *ip_vs_proto_name(unsigned int proto);
-void ip_vs_init_hash_table(struct list_head *table, int rows);
+extern const char *ip_vs_proto_name(unsigned proto);
+extern void ip_vs_init_hash_table(struct list_head *table, int rows);
 #define IP_VS_INIT_HASH_TABLE(t) ip_vs_init_hash_table((t), ARRAY_SIZE((t)))
 
 #define IP_VS_APP_TYPE_FTP	1
@@ -1156,6 +761,17 @@ void ip_vs_init_hash_table(struct list_head *table, int rows);
  *     (from ip_vs_conn.c)
  */
 
+/*
+ *     IPVS connection entry hash table
+ */
+#ifndef CONFIG_IP_VS_TAB_BITS
+#define CONFIG_IP_VS_TAB_BITS   22
+#endif
+
+#define IP_VS_CONN_TAB_BITS	CONFIG_IP_VS_TAB_BITS
+#define IP_VS_CONN_TAB_SIZE     (1 << IP_VS_CONN_TAB_BITS)
+#define IP_VS_CONN_TAB_MASK     (IP_VS_CONN_TAB_SIZE - 1)
+
 enum {
 	IP_VS_DIR_INPUT = 0,
 	IP_VS_DIR_OUTPUT,
@@ -1163,68 +779,41 @@ enum {
 	IP_VS_DIR_LAST,
 };
 
-static inline void ip_vs_conn_fill_param(struct net *net, int af, int protocol,
-					 const union nf_inet_addr *caddr,
-					 __be16 cport,
-					 const union nf_inet_addr *vaddr,
-					 __be16 vport,
-					 struct ip_vs_conn_param *p)
-{
-	p->net = net;
-	p->af = af;
-	p->protocol = protocol;
-	p->caddr = caddr;
-	p->cport = cport;
-	p->vaddr = vaddr;
-	p->vport = vport;
-	p->pe = NULL;
-	p->pe_data = NULL;
-}
-
-struct ip_vs_conn *ip_vs_conn_in_get(const struct ip_vs_conn_param *p);
-struct ip_vs_conn *ip_vs_ct_in_get(const struct ip_vs_conn_param *p);
-
-struct ip_vs_conn * ip_vs_conn_in_get_proto(int af, const struct sk_buff *skb,
-					    const struct ip_vs_iphdr *iph,
-					    int inverse);
-
-struct ip_vs_conn *ip_vs_conn_out_get(const struct ip_vs_conn_param *p);
+extern struct ip_vs_conn *ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir);
 
-struct ip_vs_conn * ip_vs_conn_out_get_proto(int af, const struct sk_buff *skb,
-					     const struct ip_vs_iphdr *iph,
-					     int inverse);
-
-/* Get reference to gain full access to conn.
- * By default, RCU read-side critical sections have access only to
- * conn fields and its PE data, see ip_vs_conn_rcu_free() for reference.
- */
-static inline bool __ip_vs_conn_get(struct ip_vs_conn *cp)
-{
-	return atomic_inc_not_zero(&cp->refcnt);
-}
+extern struct ip_vs_conn *ip_vs_ct_in_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port);
 
 /* put back the conn without restarting its timer */
 static inline void __ip_vs_conn_put(struct ip_vs_conn *cp)
 {
-	smp_mb__before_atomic_dec();
 	atomic_dec(&cp->refcnt);
 }
-void ip_vs_conn_put(struct ip_vs_conn *cp);
-void ip_vs_conn_fill_cport(struct ip_vs_conn *cp, __be16 cport);
+extern void ip_vs_conn_put(struct ip_vs_conn *cp);
+extern void ip_vs_conn_fill_cport(struct ip_vs_conn *cp, __be16 cport);
 
-struct ip_vs_conn *ip_vs_conn_new(const struct ip_vs_conn_param *p,
-				  const union nf_inet_addr *daddr,
-				  __be16 dport, unsigned int flags,
-				  struct ip_vs_dest *dest, __u32 fwmark);
-void ip_vs_conn_expire_now(struct ip_vs_conn *cp);
+extern struct ip_vs_conn *ip_vs_conn_new(int af, int proto,
+					 const union nf_inet_addr *caddr,
+					 __be16 cport,
+					 const union nf_inet_addr *vaddr,
+					 __be16 vport,
+					 const union nf_inet_addr *daddr,
+					 __be16 dport, unsigned flags,
+					 struct ip_vs_dest *dest,
+					 struct sk_buff *skb,
+					 int is_synproxy_on);
+extern void ip_vs_conn_expire_now(struct ip_vs_conn *cp);
 
-const char *ip_vs_state_name(__u16 proto, int state);
+extern const char *ip_vs_state_name(__u16 proto, int state);
 
-void ip_vs_tcp_conn_listen(struct net *net, struct ip_vs_conn *cp);
-int ip_vs_check_template(struct ip_vs_conn *ct);
-void ip_vs_random_dropentry(struct net *net);
-int ip_vs_conn_init(void);
-void ip_vs_conn_cleanup(void);
+extern void ip_vs_tcp_conn_listen(struct ip_vs_conn *cp);
+extern int ip_vs_check_template(struct ip_vs_conn *ct);
+extern void ip_vs_random_dropentry(void);
+extern int ip_vs_conn_init(void);
+extern void ip_vs_conn_cleanup(void);
 
 static inline void ip_vs_control_del(struct ip_vs_conn *cp)
 {
@@ -1287,201 +876,232 @@ ip_vs_control_add(struct ip_vs_conn *cp, struct ip_vs_conn *ctl_cp)
 }
 
 /*
- * IPVS netns init & cleanup functions
- */
-int ip_vs_estimator_net_init(struct net *net);
-int ip_vs_control_net_init(struct net *net);
-int ip_vs_protocol_net_init(struct net *net);
-int ip_vs_app_net_init(struct net *net);
-int ip_vs_conn_net_init(struct net *net);
-int ip_vs_sync_net_init(struct net *net);
-void ip_vs_conn_net_cleanup(struct net *net);
-void ip_vs_app_net_cleanup(struct net *net);
-void ip_vs_protocol_net_cleanup(struct net *net);
-void ip_vs_control_net_cleanup(struct net *net);
-void ip_vs_estimator_net_cleanup(struct net *net);
-void ip_vs_sync_net_cleanup(struct net *net);
-void ip_vs_service_net_cleanup(struct net *net);
-
-/*
  *      IPVS application functions
  *      (from ip_vs_app.c)
  */
 #define IP_VS_APP_MAX_PORTS  8
-struct ip_vs_app *register_ip_vs_app(struct net *net, struct ip_vs_app *app);
-void unregister_ip_vs_app(struct net *net, struct ip_vs_app *app);
-int ip_vs_bind_app(struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
-void ip_vs_unbind_app(struct ip_vs_conn *cp);
-int register_ip_vs_app_inc(struct net *net, struct ip_vs_app *app, __u16 proto,
-			   __u16 port);
-int ip_vs_app_inc_get(struct ip_vs_app *inc);
-void ip_vs_app_inc_put(struct ip_vs_app *inc);
-
-int ip_vs_app_pkt_out(struct ip_vs_conn *, struct sk_buff *skb);
-int ip_vs_app_pkt_in(struct ip_vs_conn *, struct sk_buff *skb);
-
-int register_ip_vs_pe(struct ip_vs_pe *pe);
-int unregister_ip_vs_pe(struct ip_vs_pe *pe);
-struct ip_vs_pe *ip_vs_pe_getbyname(const char *name);
-struct ip_vs_pe *__ip_vs_pe_getbyname(const char *pe_name);
-
-/*
- * Use a #define to avoid all of module.h just for these trivial ops
- */
-#define ip_vs_pe_get(pe)			\
-	if (pe && pe->module)			\
-		__module_get(pe->module);
-
-#define ip_vs_pe_put(pe)			\
-	if (pe && pe->module)			\
-		module_put(pe->module);
+extern int register_ip_vs_app(struct ip_vs_app *app);
+extern void unregister_ip_vs_app(struct ip_vs_app *app);
+extern int ip_vs_bind_app(struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern void ip_vs_unbind_app(struct ip_vs_conn *cp);
+extern int
+register_ip_vs_app_inc(struct ip_vs_app *app, __u16 proto, __u16 port);
+extern int ip_vs_app_inc_get(struct ip_vs_app *inc);
+extern void ip_vs_app_inc_put(struct ip_vs_app *inc);
+
+extern int ip_vs_app_pkt_out(struct ip_vs_conn *, struct sk_buff *skb);
+extern int ip_vs_app_pkt_in(struct ip_vs_conn *, struct sk_buff *skb);
+extern int ip_vs_skb_replace(struct sk_buff *skb, gfp_t pri,
+			     char *o_buf, int o_len, char *n_buf, int n_len);
+extern int ip_vs_app_init(void);
+extern void ip_vs_app_cleanup(void);
 
 /*
  *	IPVS protocol functions (from ip_vs_proto.c)
  */
-int ip_vs_protocol_init(void);
-void ip_vs_protocol_cleanup(void);
-void ip_vs_protocol_timeout_change(struct netns_ipvs *ipvs, int flags);
-int *ip_vs_create_timeout_table(int *table, int size);
-int ip_vs_set_state_timeout(int *table, int num, const char *const *names,
-			    const char *name, int to);
-void ip_vs_tcpudp_debug_packet(int af, struct ip_vs_protocol *pp,
-			       const struct sk_buff *skb, int offset,
-			       const char *msg);
+extern int ip_vs_protocol_init(void);
+extern void ip_vs_protocol_cleanup(void);
+extern void ip_vs_protocol_timeout_change(int flags);
+extern int *ip_vs_create_timeout_table(int *table, int size);
+extern int
+ip_vs_set_state_timeout(int *table, int num, const char *const *names,
+			const char *name, int to);
+extern void
+ip_vs_tcpudp_debug_packet(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+			  int offset, const char *msg);
 
 extern struct ip_vs_protocol ip_vs_protocol_tcp;
 extern struct ip_vs_protocol ip_vs_protocol_udp;
 extern struct ip_vs_protocol ip_vs_protocol_icmp;
 extern struct ip_vs_protocol ip_vs_protocol_esp;
 extern struct ip_vs_protocol ip_vs_protocol_ah;
-extern struct ip_vs_protocol ip_vs_protocol_sctp;
 
 /*
  *      Registering/unregistering scheduler functions
  *      (from ip_vs_sched.c)
  */
-int register_ip_vs_scheduler(struct ip_vs_scheduler *scheduler);
-int unregister_ip_vs_scheduler(struct ip_vs_scheduler *scheduler);
-int ip_vs_bind_scheduler(struct ip_vs_service *svc,
-			 struct ip_vs_scheduler *scheduler);
-void ip_vs_unbind_scheduler(struct ip_vs_service *svc,
-			    struct ip_vs_scheduler *sched);
-struct ip_vs_scheduler *ip_vs_scheduler_get(const char *sched_name);
-void ip_vs_scheduler_put(struct ip_vs_scheduler *scheduler);
-struct ip_vs_conn *
-ip_vs_schedule(struct ip_vs_service *svc, struct sk_buff *skb,
-	       struct ip_vs_proto_data *pd, int *ignored,
-	       struct ip_vs_iphdr *iph);
-int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
-		struct ip_vs_proto_data *pd, struct ip_vs_iphdr *iph);
-
-void ip_vs_scheduler_err(struct ip_vs_service *svc, const char *msg);
-
+extern int register_ip_vs_scheduler(struct ip_vs_scheduler *scheduler);
+extern int unregister_ip_vs_scheduler(struct ip_vs_scheduler *scheduler);
+extern int ip_vs_bind_scheduler(struct ip_vs_service *svc,
+				struct ip_vs_scheduler *scheduler);
+extern int ip_vs_unbind_scheduler(struct ip_vs_service *svc);
+extern struct ip_vs_scheduler *ip_vs_scheduler_get(const char *sched_name);
+extern void ip_vs_scheduler_put(struct ip_vs_scheduler *scheduler);
+extern struct ip_vs_conn *ip_vs_schedule(struct ip_vs_service *svc,
+					 struct sk_buff *skb,
+					 int is_synproxy_on);
+extern int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
+		       struct ip_vs_protocol *pp);
 
 /*
  *      IPVS control data and functions (from ip_vs_ctl.c)
  */
-extern struct ip_vs_stats ip_vs_stats;
-extern int sysctl_ip_vs_sync_ver;
-
-struct ip_vs_service *
-ip_vs_service_find(struct net *net, int af, __u32 fwmark, __u16 protocol,
-		  const union nf_inet_addr *vaddr, __be16 vport);
-
-bool ip_vs_has_real_service(struct net *net, int af, __u16 protocol,
-			    const union nf_inet_addr *daddr, __be16 dport);
-
-int ip_vs_use_count_inc(void);
-void ip_vs_use_count_dec(void);
-int ip_vs_register_nl_ioctl(void);
-void ip_vs_unregister_nl_ioctl(void);
-int ip_vs_control_init(void);
-void ip_vs_control_cleanup(void);
-struct ip_vs_dest *
-ip_vs_find_dest(struct net *net, int af, const union nf_inet_addr *daddr,
-		__be16 dport, const union nf_inet_addr *vaddr, __be16 vport,
-		__u16 protocol, __u32 fwmark, __u32 flags);
-void ip_vs_try_bind_dest(struct ip_vs_conn *cp);
-
-static inline void ip_vs_dest_hold(struct ip_vs_dest *dest)
+extern int sysctl_ip_vs_cache_bypass;
+extern int sysctl_ip_vs_expire_nodest_conn;
+extern int sysctl_ip_vs_expire_quiescent_template;
+extern int sysctl_ip_vs_sync_threshold[2];
+extern int sysctl_ip_vs_nat_icmp_send;
+extern struct ip_vs_stats *ip_vs_stats;
+extern const struct ctl_path net_vs_ctl_path[];
+extern int sysctl_ip_vs_timestamp_remove_entry;
+extern int sysctl_ip_vs_mss_adjust_entry;
+extern int sysctl_ip_vs_conn_reused_entry;
+extern int sysctl_ip_vs_toa_entry;
+extern int sysctl_ip_vs_lport_max;
+extern int sysctl_ip_vs_lport_min;
+extern int sysctl_ip_vs_lport_tries;
+extern int sysctl_ip_vs_frag_drop_entry;
+extern int sysctl_ip_vs_tcp_drop_entry;
+extern int sysctl_ip_vs_udp_drop_entry;
+extern int sysctl_ip_vs_conn_expire_tcp_rst;
+extern int sysctl_ip_vs_fast_xmit;
+
+extern struct ip_vs_service *ip_vs_service_get(int af, __u32 fwmark,
+					       __u16 protocol,
+					       const union nf_inet_addr *vaddr,
+					       __be16 vport);
+extern struct ip_vs_service *ip_vs_lookup_vip(int af, __u16 protocol,
+					      const union nf_inet_addr *vaddr);
+
+static inline void ip_vs_service_put(struct ip_vs_service *svc)
 {
-	atomic_inc(&dest->refcnt);
+	atomic_dec(&svc->usecnt);
 }
 
-static inline void ip_vs_dest_put(struct ip_vs_dest *dest)
-{
-	smp_mb__before_atomic_dec();
-	atomic_dec(&dest->refcnt);
-}
+extern struct ip_vs_dest *ip_vs_lookup_real_service(int af, __u16 protocol,
+						    const union nf_inet_addr
+						    *daddr, __be16 dport);
+
+extern int ip_vs_use_count_inc(void);
+extern void ip_vs_use_count_dec(void);
+extern int ip_vs_control_init(void);
+extern void ip_vs_control_cleanup(void);
+extern struct ip_vs_dest *ip_vs_find_dest(int af,
+					  const union nf_inet_addr *daddr,
+					  __be16 dport,
+					  const union nf_inet_addr *vaddr,
+					  __be16 vport, __u16 protocol);
+extern struct ip_vs_dest *ip_vs_try_bind_dest(struct ip_vs_conn *cp);
+
+extern void ip_vs_laddr_hold(struct ip_vs_laddr *addr);
+extern void ip_vs_laddr_put(struct ip_vs_laddr *addr);
 
 /*
  *      IPVS sync daemon data and function prototypes
  *      (from ip_vs_sync.c)
  */
-int start_sync_thread(struct net *net, int state, char *mcast_ifn, __u8 syncid);
-int stop_sync_thread(struct net *net, int state);
-void ip_vs_sync_conn(struct net *net, struct ip_vs_conn *cp, int pkts);
+extern volatile int ip_vs_sync_state;
+extern volatile int ip_vs_master_syncid;
+extern volatile int ip_vs_backup_syncid;
+extern char ip_vs_master_mcast_ifn[IP_VS_IFNAME_MAXLEN];
+extern char ip_vs_backup_mcast_ifn[IP_VS_IFNAME_MAXLEN];
+extern int start_sync_thread(int state, char *mcast_ifn, __u8 syncid);
+extern int stop_sync_thread(int state);
+extern void ip_vs_sync_conn(struct ip_vs_conn *cp);
+
+/*
+ *      IPVS statistic prototypes (from ip_vs_stats.c)
+ */
+#define ip_vs_stats_cpu(stats,cpu)  \
+	(*per_cpu_ptr((stats), (cpu)))
+
+#define ip_vs_stats_this_cpu(stats) \
+	(*this_cpu_ptr((stats)))
+
+extern int ip_vs_new_stats(struct ip_vs_stats** p);
+extern void ip_vs_del_stats(struct ip_vs_stats* p);
+extern void ip_vs_zero_stats(struct ip_vs_stats* stats);
+extern void ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb);
+extern void ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb);
+extern void ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc);
 
 /*
- *      IPVS rate estimator prototypes (from ip_vs_est.c)
+ *	Lookup route table
  */
-void ip_vs_start_estimator(struct net *net, struct ip_vs_stats *stats);
-void ip_vs_stop_estimator(struct net *net, struct ip_vs_stats *stats);
-void ip_vs_zero_estimator(struct ip_vs_stats *stats);
-void ip_vs_read_estimator(struct ip_vs_stats_user *dst,
-			  struct ip_vs_stats *stats);
+extern struct rtable *ip_vs_get_rt(union nf_inet_addr *addr, u32 rtos);
+
+#ifdef CONFIG_IP_VS_IPV6
+extern struct rt6_info *ip_vs_get_rt_v6(union nf_inet_addr *addr);
+#endif
 
 /*
  *	Various IPVS packet transmitters (from ip_vs_xmit.c)
  */
-int ip_vs_null_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		    struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_bypass_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		      struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_nat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		   struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		      struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_dr_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		  struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_icmp_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		    struct ip_vs_protocol *pp, int offset,
-		    unsigned int hooknum, struct ip_vs_iphdr *iph);
-void ip_vs_dest_dst_rcu_free(struct rcu_head *head);
+extern int ip_vs_null_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_bypass_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_nat_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_fnat_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_tunnel_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_dr_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_icmp_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
+     int offset);
+extern void ip_vs_dst_reset(struct ip_vs_dest *dest);
+extern int ip_vs_normal_response_xmit
+    (struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
+     int ihl);
+extern int ip_vs_fnat_response_xmit(struct sk_buff *skb,
+				    struct ip_vs_protocol *pp,
+				    struct ip_vs_conn *cp, int ihl);
+extern int ip_vs_normal_response_icmp_xmit(struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   struct ip_vs_conn *cp, int offset);
+extern int ip_vs_fnat_response_icmp_xmit(struct sk_buff *skb,
+					 struct ip_vs_protocol *pp,
+					 struct ip_vs_conn *cp, int offset);
 
 #ifdef CONFIG_IP_VS_IPV6
-int ip_vs_bypass_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-			 struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_nat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		      struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_tunnel_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-			 struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_dr_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		     struct ip_vs_protocol *pp, struct ip_vs_iphdr *iph);
-int ip_vs_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		       struct ip_vs_protocol *pp, int offset,
-		       unsigned int hooknum, struct ip_vs_iphdr *iph);
+extern int ip_vs_bypass_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_nat_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_fnat_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_tunnel_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_dr_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_icmp_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
+     int offset);
+extern int ip_vs_normal_response_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
+     int ihl);
+extern int ip_vs_fnat_response_xmit_v6(struct sk_buff *skb,
+				       struct ip_vs_protocol *pp,
+				       struct ip_vs_conn *cp, int ihl);
+extern int ip_vs_normal_response_icmp_xmit_v6(struct sk_buff *skb,
+					      struct ip_vs_protocol *pp,
+					      struct ip_vs_conn *cp,
+					      int offset);
+extern int ip_vs_fnat_response_icmp_xmit_v6(struct sk_buff *skb,
+					    struct ip_vs_protocol *pp,
+					    struct ip_vs_conn *cp, int offset);
 #endif
 
-#ifdef CONFIG_SYSCTL
 /*
  *	This is a simple mechanism to ignore packets when
  *	we are loaded. Just set ip_vs_drop_rate to 'n' and
  *	we start to drop 1/rate of the packets
  */
+extern int ip_vs_drop_rate;
+extern int ip_vs_drop_counter;
 
-static inline int ip_vs_todrop(struct netns_ipvs *ipvs)
+static __inline__ int ip_vs_todrop(void)
 {
-	if (!ipvs->drop_rate)
+	if (!ip_vs_drop_rate)
 		return 0;
-	if (--ipvs->drop_counter > 0)
+	if (--ip_vs_drop_counter > 0)
 		return 0;
-	ipvs->drop_counter = ipvs->drop_rate;
+	ip_vs_drop_counter = ip_vs_drop_rate;
 	return 1;
 }
-#else
-static inline int ip_vs_todrop(struct netns_ipvs *ipvs) { return 0; }
-#endif
 
 /*
  *      ip_vs_fwd_tag returns the forwarding tag of the connection
@@ -1494,30 +1114,31 @@ static inline char ip_vs_fwd_tag(struct ip_vs_conn *cp)
 
 	switch (IP_VS_FWD_METHOD(cp)) {
 	case IP_VS_CONN_F_MASQ:
-		fwd = 'M'; break;
+		fwd = 'M';
+		break;
 	case IP_VS_CONN_F_LOCALNODE:
-		fwd = 'L'; break;
+		fwd = 'L';
+		break;
 	case IP_VS_CONN_F_TUNNEL:
-		fwd = 'T'; break;
+		fwd = 'T';
+		break;
 	case IP_VS_CONN_F_DROUTE:
-		fwd = 'R'; break;
+		fwd = 'R';
+		break;
 	case IP_VS_CONN_F_BYPASS:
-		fwd = 'B'; break;
+		fwd = 'B';
+		break;
+	case IP_VS_CONN_F_FULLNAT:
+		fwd = 'F';
+		break;
 	default:
-		fwd = '?'; break;
+		fwd = '?';
+		break;
 	}
 	return fwd;
 }
 
-void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int dir);
-
-#ifdef CONFIG_IP_VS_IPV6
-void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		       struct ip_vs_conn *cp, int dir);
-#endif
-
-__sum16 ip_vs_checksum_complete(struct sk_buff *skb, int offset);
+extern __sum16 ip_vs_checksum_complete(struct sk_buff *skb, int offset);
 
 static inline __wsum ip_vs_check_diff4(__be32 old, __be32 new, __wsum oldsum)
 {
@@ -1527,11 +1148,12 @@ static inline __wsum ip_vs_check_diff4(__be32 old, __be32 new, __wsum oldsum)
 }
 
 #ifdef CONFIG_IP_VS_IPV6
-static inline __wsum ip_vs_check_diff16(const __be32 *old, const __be32 *new,
+static inline __wsum ip_vs_check_diff16(const __be32 * old, const __be32 * new,
 					__wsum oldsum)
 {
 	__be32 diff[8] = { ~old[3], ~old[2], ~old[1], ~old[0],
-			    new[3],  new[2],  new[1],  new[0] };
+		new[3], new[2], new[1], new[0]
+	};
 
 	return csum_partial(diff, sizeof(diff), oldsum);
 }
@@ -1544,81 +1166,6 @@ static inline __wsum ip_vs_check_diff2(__be16 old, __be16 new, __wsum oldsum)
 	return csum_partial(diff, sizeof(diff), oldsum);
 }
 
-/*
- * Forget current conntrack (unconfirmed) and attach notrack entry
- */
-static inline void ip_vs_notrack(struct sk_buff *skb)
-{
-#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
-	enum ip_conntrack_info ctinfo;
-	struct nf_conn *ct = nf_ct_get(skb, &ctinfo);
-
-	if (!ct || !nf_ct_is_untracked(ct)) {
-		nf_conntrack_put(skb->nfct);
-		skb->nfct = &nf_ct_untracked_get()->ct_general;
-		skb->nfctinfo = IP_CT_NEW;
-		nf_conntrack_get(skb->nfct);
-	}
-#endif
-}
-
-#ifdef CONFIG_IP_VS_NFCT
-/*
- *      Netfilter connection tracking
- *      (from ip_vs_nfct.c)
- */
-static inline int ip_vs_conntrack_enabled(struct netns_ipvs *ipvs)
-{
-#ifdef CONFIG_SYSCTL
-	return ipvs->sysctl_conntrack;
-#else
-	return 0;
-#endif
-}
-
-void ip_vs_update_conntrack(struct sk_buff *skb, struct ip_vs_conn *cp,
-			    int outin);
-int ip_vs_confirm_conntrack(struct sk_buff *skb);
-void ip_vs_nfct_expect_related(struct sk_buff *skb, struct nf_conn *ct,
-			       struct ip_vs_conn *cp, u_int8_t proto,
-			       const __be16 port, int from_rs);
-void ip_vs_conn_drop_conntrack(struct ip_vs_conn *cp);
-
-#else
-
-static inline int ip_vs_conntrack_enabled(struct netns_ipvs *ipvs)
-{
-	return 0;
-}
-
-static inline void ip_vs_update_conntrack(struct sk_buff *skb,
-					  struct ip_vs_conn *cp, int outin)
-{
-}
-
-static inline int ip_vs_confirm_conntrack(struct sk_buff *skb)
-{
-	return NF_ACCEPT;
-}
-
-static inline void ip_vs_conn_drop_conntrack(struct ip_vs_conn *cp)
-{
-}
-/* CONFIG_IP_VS_NFCT */
-#endif
-
-static inline unsigned int
-ip_vs_dest_conn_overhead(struct ip_vs_dest *dest)
-{
-	/*
-	 * We think the overhead of processing active connections is 256
-	 * times higher than that of inactive connections in average. (This
-	 * 256 times might not be accurate, we will change it later) We
-	 * use the following formula to estimate the overhead now:
-	 *		  dest->activeconns*256 + dest->inactconns
-	 */
-	return (atomic_read(&dest->activeconns) << 8) +
-		atomic_read(&dest->inactconns);
-}
+#endif				/* __KERNEL__ */
 
-#endif	/* _NET_IP_VS_H */
+#endif				/* _NET_IP_VS_H */
diff --git a/include/net/ip_vs_synproxy.h b/include/net/ip_vs_synproxy.h
new file mode 100644
index 0000000..82b580d
--- /dev/null
+++ b/include/net/ip_vs_synproxy.h
@@ -0,0 +1,135 @@
+/*
+ *     IP Virtual Server Syn-Proxy
+ *     data structure and functionality definitions
+ */
+
+#ifndef _NET_IP_VS_SYNPROXY_H
+#define _NET_IP_VS_SYNPROXY_H
+
+#include <net/ip_vs.h>
+
+/* Add MASKs for TCP OPT in "data" coded in cookie */
+/* |[21][20][19-16][15-0]|
+ * [21]    SACK
+ * [20]    TimeStamp
+ * [19-16] snd_wscale
+ * [15-0]  MSSIND
+ */
+#define IP_VS_SYNPROXY_MSS_BITS 16
+#define IP_VS_SYNPROXY_MSS_MASK (((__u32)1 << IP_VS_SYNPROXY_MSS_BITS) - 1)
+
+#define IP_VS_SYNPROXY_SACKOK_BIT 21
+#define IP_VS_SYNPROXY_SACKOK_MASK ((__u32)1 << IP_VS_SYNPROXY_SACKOK_BIT)
+
+#define IP_VS_SYNPROXY_TSOK_BIT 20
+#define IP_VS_SYNPROXY_TSOK_MASK ((__u32)1 << IP_VS_SYNPROXY_TSOK_BIT)
+
+#define IP_VS_SYNPROXY_SND_WSCALE_BITS 16
+#define IP_VS_SYNPROXY_SND_WSCALE_MASK ((__u32)0xf << IP_VS_SYNPROXY_SND_WSCALE_BITS)
+
+#define IP_VS_SYNPROXY_WSCALE_MAX          14
+
+/* add for supporting tcp options' in syn-proxy */
+struct ip_vs_synproxy_opt {
+	u16 snd_wscale:8,	/* Window scaling received from sender          */
+	 tstamp_ok:1,		/* TIMESTAMP seen on SYN packet                 */
+	 wscale_ok:1,		/* Wscale seen on SYN packet                    */
+	 sack_ok:1;		/* SACK seen on SYN packet                      */
+	u16 mss_clamp;		/* Maximal mss, negotiated at connection setup  */
+};
+
+/* 
+ * For syncookie compute and check 
+ */
+extern __u32 ip_vs_synproxy_cookie_v4_init_sequence(struct sk_buff *skb,
+						    struct ip_vs_synproxy_opt
+						    *opts);
+extern int ip_vs_synproxy_v4_cookie_check(struct sk_buff *skb, __u32 cookie,
+					  struct ip_vs_synproxy_opt *opt);
+
+extern __u32 ip_vs_synproxy_cookie_v6_init_sequence(struct sk_buff *skb,
+						    struct ip_vs_synproxy_opt
+						    *opts);
+extern int ip_vs_synproxy_v6_cookie_check(struct sk_buff *skb, __u32 cookie,
+					  struct ip_vs_synproxy_opt *opt);
+
+/*
+ * Syn-proxy step 1 logic: receive client's Syn.
+ */
+extern int ip_vs_synproxy_syn_rcv(int af, struct sk_buff *skb,
+				  struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Syn-proxy step 2 logic: receive client's Ack.
+ */
+extern int ip_vs_synproxy_ack_rcv(int af, struct sk_buff *skb,
+				  struct tcphdr *th, struct ip_vs_protocol *pp,
+				  struct ip_vs_conn **cpp,
+				  struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Syn-proxy step 3 logic: receive rs's Syn/Ack.
+ */
+extern int ip_vs_synproxy_synack_rcv(struct sk_buff *skb, struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     int ihl, int *verdict);
+/*
+ * Syn-proxy conn reuse logic: receive client's Ack.
+ */
+extern int ip_vs_synproxy_reuse_conn(int af, struct sk_buff *skb,
+				     struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Store or drop client's ack packet, when lvs is waiting for 
+ * rs's Syn/Ack packet.
+ */
+extern int ip_vs_synproxy_filter_ack(struct sk_buff *skb, struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     struct ip_vs_iphdr *iph, int *verdict);
+
+/*
+ * Tranfer ack seq and sack opt for Out-In packet.
+ */
+extern void ip_vs_synproxy_dnat_handler(struct tcphdr *tcph,
+					struct ip_vs_seq *sp_seq);
+/*
+ * Tranfer seq for In-Out packet.
+ */
+extern int ip_vs_synproxy_snat_handler(struct tcphdr *tcph,
+				       struct ip_vs_conn *cp);
+
+/* syn-proxy sysctl variables */
+#define IP_VS_SYNPROXY_INIT_MSS_DEFAULT		1452
+#define IP_VS_SYNPROXY_TTL_DEFAULT		63
+#define IP_VS_SYNPROXY_TTL_MIN			1
+#define IP_VS_SYNPROXY_TTL_MAX			255
+#define IP_VS_SYNPROXY_SACK_DEFAULT		1
+#define IP_VS_SYNPROXY_WSCALE_DEFAULT		0
+#define IP_VS_SYNPROXY_TIMESTAMP_DEFAULT	0
+#define IP_VS_SYNPROXY_DEFER_DEFAULT		0
+#define IP_VS_SYNPROXY_DUP_ACK_DEFAULT		10
+#define IP_VS_SYNPROXY_SKB_STORE_DEFAULT	3
+#define IP_VS_SYNPROXY_CONN_REUSE_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_CL_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_TW_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_FW_DEFAULT	0
+#define	IP_VS_SYNPROXY_CONN_REUSE_CW_DEFAULT	0
+#define	IP_VS_SYNPROXY_CONN_REUSE_LA_DEFAULT	0
+#define	IP_VS_SYNPROXY_SYN_RETRY_DEFAULT	3
+
+extern int sysctl_ip_vs_synproxy_sack;
+extern int sysctl_ip_vs_synproxy_wscale;
+extern int sysctl_ip_vs_synproxy_timestamp;
+extern int sysctl_ip_vs_synproxy_synack_ttl;
+extern int sysctl_ip_vs_synproxy_init_mss;
+extern int sysctl_ip_vs_synproxy_defer;
+extern int sysctl_ip_vs_synproxy_dup_ack_thresh;
+extern int sysctl_ip_vs_synproxy_skb_store_thresh;
+extern int sysctl_ip_vs_synproxy_syn_retry;
+extern int sysctl_ip_vs_synproxy_conn_reuse;
+extern int sysctl_ip_vs_synproxy_conn_reuse_cl;
+extern int sysctl_ip_vs_synproxy_conn_reuse_tw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_fw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_cw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_la;
+
+#endif
diff --git a/include/uapi/linux/ip_vs.h b/include/uapi/linux/ip_vs.h
index a245377..17f51e7 100644
--- a/include/uapi/linux/ip_vs.h
+++ b/include/uapi/linux/ip_vs.h
@@ -17,29 +17,29 @@
 /*
  *      Virtual Service Flags
  */
-#define IP_VS_SVC_F_PERSISTENT	0x0001		/* persistent port */
-#define IP_VS_SVC_F_HASHED	0x0002		/* hashed entry */
-#define IP_VS_SVC_F_ONEPACKET	0x0004		/* one-packet scheduling */
+#define IP_VS_SVC_F_PERSISTENT	0x0001	/* persistent port */
+#define IP_VS_SVC_F_HASHED	0x0002	/* hashed entry */
+#define IP_VS_SVC_F_ONEPACKET	0x0004	/* one-packet scheduling */
 
 /*
  *      Destination Server Flags
  */
-#define IP_VS_DEST_F_AVAILABLE	0x0001		/* server is available */
-#define IP_VS_DEST_F_OVERLOAD	0x0002		/* server is overloaded */
+#define IP_VS_DEST_F_AVAILABLE	0x0001	/* server is available */
+#define IP_VS_DEST_F_OVERLOAD	0x0002	/* server is overloaded */
 
 /*
  *      IPVS sync daemon states
  */
-#define IP_VS_STATE_NONE	0x0000		/* daemon is stopped */
-#define IP_VS_STATE_MASTER	0x0001		/* started as master */
-#define IP_VS_STATE_BACKUP	0x0002		/* started as backup */
+#define IP_VS_STATE_NONE	0x0000	/* daemon is stopped */
+#define IP_VS_STATE_MASTER	0x0001	/* started as master */
+#define IP_VS_STATE_BACKUP	0x0002	/* started as backup */
 
 /*
  *      IPVS socket options
  */
-#define IP_VS_BASE_CTL		(64+1024+64)		/* base */
+#define IP_VS_BASE_CTL		(64+1024+64)	/* base */
 
-#define IP_VS_SO_SET_NONE	IP_VS_BASE_CTL		/* just peek */
+#define IP_VS_SO_SET_NONE	IP_VS_BASE_CTL	/* just peek */
 #define IP_VS_SO_SET_INSERT	(IP_VS_BASE_CTL+1)
 #define IP_VS_SO_SET_ADD	(IP_VS_BASE_CTL+2)
 #define IP_VS_SO_SET_EDIT	(IP_VS_BASE_CTL+3)
@@ -55,7 +55,9 @@
 #define IP_VS_SO_SET_RESTORE    (IP_VS_BASE_CTL+13)
 #define IP_VS_SO_SET_SAVE       (IP_VS_BASE_CTL+14)
 #define IP_VS_SO_SET_ZERO	(IP_VS_BASE_CTL+15)
-#define IP_VS_SO_SET_MAX	IP_VS_SO_SET_ZERO
+#define IP_VS_SO_SET_ADDLADDR	(IP_VS_BASE_CTL+16)
+#define IP_VS_SO_SET_DELLADDR	(IP_VS_BASE_CTL+17)
+#define IP_VS_SO_SET_MAX	IP_VS_SO_SET_DELLADDR
 
 #define IP_VS_SO_GET_VERSION	IP_VS_BASE_CTL
 #define IP_VS_SO_GET_INFO	(IP_VS_BASE_CTL+1)
@@ -65,207 +67,203 @@
 #define IP_VS_SO_GET_DEST	(IP_VS_BASE_CTL+5)	/* not used now */
 #define IP_VS_SO_GET_TIMEOUT	(IP_VS_BASE_CTL+6)
 #define IP_VS_SO_GET_DAEMON	(IP_VS_BASE_CTL+7)
-#define IP_VS_SO_GET_MAX	IP_VS_SO_GET_DAEMON
-
+#define IP_VS_SO_GET_LADDRS	(IP_VS_BASE_CTL+8)
+#define IP_VS_SO_GET_MAX	IP_VS_SO_GET_LADDRS
 
 /*
  *      IPVS Connection Flags
- *      Only flags 0..15 are sent to backup server
  */
-#define IP_VS_CONN_F_FWD_MASK	0x0007		/* mask for the fwd methods */
-#define IP_VS_CONN_F_MASQ	0x0000		/* masquerading/NAT */
-#define IP_VS_CONN_F_LOCALNODE	0x0001		/* local node */
-#define IP_VS_CONN_F_TUNNEL	0x0002		/* tunneling */
-#define IP_VS_CONN_F_DROUTE	0x0003		/* direct routing */
-#define IP_VS_CONN_F_BYPASS	0x0004		/* cache bypass */
-#define IP_VS_CONN_F_SYNC	0x0020		/* entry created by sync */
-#define IP_VS_CONN_F_HASHED	0x0040		/* hashed entry */
-#define IP_VS_CONN_F_NOOUTPUT	0x0080		/* no output packets */
-#define IP_VS_CONN_F_INACTIVE	0x0100		/* not established */
-#define IP_VS_CONN_F_OUT_SEQ	0x0200		/* must do output seq adjust */
-#define IP_VS_CONN_F_IN_SEQ	0x0400		/* must do input seq adjust */
-#define IP_VS_CONN_F_SEQ_MASK	0x0600		/* in/out sequence mask */
-#define IP_VS_CONN_F_NO_CPORT	0x0800		/* no client port set yet */
-#define IP_VS_CONN_F_TEMPLATE	0x1000		/* template, not connection */
-#define IP_VS_CONN_F_ONE_PACKET	0x2000		/* forward only one packet */
-
-/* Initial bits allowed in backup server */
-#define IP_VS_CONN_F_BACKUP_MASK (IP_VS_CONN_F_FWD_MASK | \
-				  IP_VS_CONN_F_NOOUTPUT | \
-				  IP_VS_CONN_F_INACTIVE | \
-				  IP_VS_CONN_F_SEQ_MASK | \
-				  IP_VS_CONN_F_NO_CPORT | \
-				  IP_VS_CONN_F_TEMPLATE \
-				 )
-
-/* Bits allowed to update in backup server */
-#define IP_VS_CONN_F_BACKUP_UPD_MASK (IP_VS_CONN_F_INACTIVE | \
-				      IP_VS_CONN_F_SEQ_MASK)
-
-/* Flags that are not sent to backup server start from bit 16 */
-#define IP_VS_CONN_F_NFCT	(1 << 16)	/* use netfilter conntrack */
-
-/* Connection flags from destination that can be changed by user space */
-#define IP_VS_CONN_F_DEST_MASK (IP_VS_CONN_F_FWD_MASK | \
-				IP_VS_CONN_F_ONE_PACKET | \
-				IP_VS_CONN_F_NFCT | \
-				0)
+#define IP_VS_CONN_F_FWD_MASK	0x0007	/* mask for the fwd methods */
+#define IP_VS_CONN_F_MASQ	0x0000	/* masquerading/NAT */
+#define IP_VS_CONN_F_LOCALNODE	0x0001	/* local node */
+#define IP_VS_CONN_F_TUNNEL	0x0002	/* tunneling */
+#define IP_VS_CONN_F_DROUTE	0x0003	/* direct routing */
+#define IP_VS_CONN_F_BYPASS	0x0004	/* cache bypass */
+#define IP_VS_CONN_F_FULLNAT	0x0005	/* full nat */
+#define IP_VS_CONN_F_SYNC	0x0020	/* entry created by sync */
+#define IP_VS_CONN_F_HASHED	0x0040	/* hashed entry */
+#define IP_VS_CONN_F_NOOUTPUT	0x0080	/* no output packets */
+#define IP_VS_CONN_F_INACTIVE	0x0100	/* not established */
+#define IP_VS_CONN_F_OUT_SEQ	0x0200	/* must do output seq adjust */
+#define IP_VS_CONN_F_IN_SEQ	0x0400	/* must do input seq adjust */
+#define IP_VS_CONN_F_SEQ_MASK	0x0600	/* in/out sequence mask */
+#define IP_VS_CONN_F_NO_CPORT	0x0800	/* no client port set yet */
+#define IP_VS_CONN_F_TEMPLATE	0x1000	/* template, not connection */
+#define IP_VS_CONN_F_ONE_PACKET	0x2000	/* forward only one packet */
+#define IP_VS_CONN_F_SYNPROXY	0x8000	/* syn proxy flag */
 
 #define IP_VS_SCHEDNAME_MAXLEN	16
-#define IP_VS_PENAME_MAXLEN	16
 #define IP_VS_IFNAME_MAXLEN	16
 
-#define IP_VS_PEDATA_MAXLEN     255
-
 /*
  *	The struct ip_vs_service_user and struct ip_vs_dest_user are
  *	used to set IPVS rules through setsockopt.
  */
 struct ip_vs_service_user {
 	/* virtual service addresses */
-	__u16		protocol;
-	__be32			addr;		/* virtual ip address */
-	__be16			port;
-	__u32		fwmark;		/* firwall mark of service */
+	__u16 protocol;
+	__be32 addr;		/* virtual ip address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
 
 	/* virtual service options */
-	char			sched_name[IP_VS_SCHEDNAME_MAXLEN];
-	unsigned int		flags;		/* virtual service flags */
-	unsigned int		timeout;	/* persistent timeout in sec */
-	__be32			netmask;	/* persistent netmask */
+	char sched_name[IP_VS_SCHEDNAME_MAXLEN];
+	unsigned flags;		/* virtual service flags */
+	unsigned timeout;	/* persistent timeout in sec */
+	__be32 netmask;		/* persistent netmask */
 };
 
-
 struct ip_vs_dest_user {
 	/* destination server address */
-	__be32			addr;
-	__be16			port;
+	__be32 addr;
+	__be16 port;
 
 	/* real server options */
-	unsigned int		conn_flags;	/* connection flags */
-	int			weight;		/* destination weight */
+	unsigned conn_flags;	/* connection flags */
+	int weight;		/* destination weight */
 
 	/* thresholds for active connections */
-	__u32		u_threshold;	/* upper threshold */
-	__u32		l_threshold;	/* lower threshold */
+	__u32 u_threshold;	/* upper threshold */
+	__u32 l_threshold;	/* lower threshold */
 };
 
+struct ip_vs_laddr_user {
+	__be32 addr;		/* ipv4 address */
+};
 
 /*
  *	IPVS statistics object (for user space)
  */
 struct ip_vs_stats_user {
-	__u32                   conns;          /* connections scheduled */
-	__u32                   inpkts;         /* incoming packets */
-	__u32                   outpkts;        /* outgoing packets */
-	__u64                   inbytes;        /* incoming bytes */
-	__u64                   outbytes;       /* outgoing bytes */
-
-	__u32			cps;		/* current connection rate */
-	__u32			inpps;		/* current in packet rate */
-	__u32			outpps;		/* current out packet rate */
-	__u32			inbps;		/* current in byte rate */
-	__u32			outbps;		/* current out byte rate */
+	__u64 conns;		/* connections scheduled */
+	__u64 inpkts;		/* incoming packets */
+	__u64 outpkts;		/* outgoing packets */
+	__u64 inbytes;		/* incoming bytes */
+	__u64 outbytes;		/* outgoing bytes */
+
+	__u32 cps;		/* current connection rate */
+	__u32 inpps;		/* current in packet rate */
+	__u32 outpps;		/* current out packet rate */
+	__u32 inbps;		/* current in byte rate */
+	__u32 outbps;		/* current out byte rate */
 };
 
-
 /* The argument to IP_VS_SO_GET_INFO */
 struct ip_vs_getinfo {
 	/* version number */
-	unsigned int		version;
+	unsigned int version;
 
 	/* size of connection hash table */
-	unsigned int		size;
+	unsigned int size;
 
 	/* number of virtual services */
-	unsigned int		num_services;
+	unsigned int num_services;
 };
 
-
 /* The argument to IP_VS_SO_GET_SERVICE */
 struct ip_vs_service_entry {
 	/* which service: user fills in these */
-	__u16		protocol;
-	__be32			addr;		/* virtual address */
-	__be16			port;
-	__u32		fwmark;		/* firwall mark of service */
+	__u16 protocol;
+	__be32 addr;		/* virtual address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
 
 	/* service options */
-	char			sched_name[IP_VS_SCHEDNAME_MAXLEN];
-	unsigned int		flags;          /* virtual service flags */
-	unsigned int		timeout;	/* persistent timeout */
-	__be32			netmask;	/* persistent netmask */
+	char sched_name[IP_VS_SCHEDNAME_MAXLEN];
+	unsigned flags;		/* virtual service flags */
+	unsigned timeout;	/* persistent timeout */
+	__be32 netmask;		/* persistent netmask */
 
 	/* number of real servers */
-	unsigned int		num_dests;
+	unsigned int num_dests;
+
+	/* number of local address */
+	unsigned int num_laddrs;
 
 	/* statistics */
 	struct ip_vs_stats_user stats;
 };
 
-
 struct ip_vs_dest_entry {
-	__be32			addr;		/* destination address */
-	__be16			port;
-	unsigned int		conn_flags;	/* connection flags */
-	int			weight;		/* destination weight */
+	__be32 addr;		/* destination address */
+	__be16 port;
+	unsigned conn_flags;	/* connection flags */
+	int weight;		/* destination weight */
 
-	__u32		u_threshold;	/* upper threshold */
-	__u32		l_threshold;	/* lower threshold */
+	__u32 u_threshold;	/* upper threshold */
+	__u32 l_threshold;	/* lower threshold */
 
-	__u32		activeconns;	/* active connections */
-	__u32		inactconns;	/* inactive connections */
-	__u32		persistconns;	/* persistent connections */
+	__u32 activeconns;	/* active connections */
+	__u32 inactconns;	/* inactive connections */
+	__u32 persistconns;	/* persistent connections */
 
 	/* statistics */
 	struct ip_vs_stats_user stats;
 };
 
+struct ip_vs_laddr_entry {
+	__be32 addr;		/* ipv4 address */
+
+	__u64 port_conflict;	/* conflict counts */
+	__u32 conn_counts;	/* current connects */
+};
 
 /* The argument to IP_VS_SO_GET_DESTS */
 struct ip_vs_get_dests {
 	/* which service: user fills in these */
-	__u16		protocol;
-	__be32			addr;		/* virtual address */
-	__be16			port;
-	__u32		fwmark;		/* firwall mark of service */
+	__u16 protocol;
+	__be32 addr;		/* virtual address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
 
 	/* number of real servers */
-	unsigned int		num_dests;
+	unsigned int num_dests;
 
 	/* the real servers */
-	struct ip_vs_dest_entry	entrytable[0];
+	struct ip_vs_dest_entry entrytable[0];
 };
 
+/* The argument to IP_VS_SO_GET_LADDRS */
+struct ip_vs_get_laddrs {
+	/* which service: user fills in these */
+	__u16 protocol;
+	__be32 addr;		/* virtual address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
+
+	/* number of local address */
+	unsigned int num_laddrs;
+
+	/* the real servers */
+	struct ip_vs_laddr_entry entrytable[0];
+};
 
 /* The argument to IP_VS_SO_GET_SERVICES */
 struct ip_vs_get_services {
 	/* number of virtual services */
-	unsigned int		num_services;
+	unsigned int num_services;
 
 	/* service table */
 	struct ip_vs_service_entry entrytable[0];
 };
 
-
 /* The argument to IP_VS_SO_GET_TIMEOUT */
 struct ip_vs_timeout_user {
-	int			tcp_timeout;
-	int			tcp_fin_timeout;
-	int			udp_timeout;
+	int tcp_timeout;
+	int tcp_fin_timeout;
+	int udp_timeout;
 };
 
-
 /* The argument to IP_VS_SO_GET_DAEMON */
 struct ip_vs_daemon_user {
 	/* sync daemon state (master/backup) */
-	int			state;
+	int state;
 
 	/* multicast interface name */
-	char			mcast_ifn[IP_VS_IFNAME_MAXLEN];
+	char mcast_ifn[IP_VS_IFNAME_MAXLEN];
 
 	/* SyncID we belong to */
-	int			syncid;
+	int syncid;
 };
 
 /*
@@ -280,36 +278,40 @@ struct ip_vs_daemon_user {
 #define IPVS_GENL_VERSION	0x1
 
 struct ip_vs_flags {
-	__u32 flags;
-	__u32 mask;
+	__be32 flags;
+	__be32 mask;
 };
 
 /* Generic Netlink command attributes */
 enum {
 	IPVS_CMD_UNSPEC = 0,
 
-	IPVS_CMD_NEW_SERVICE,		/* add service */
-	IPVS_CMD_SET_SERVICE,		/* modify service */
-	IPVS_CMD_DEL_SERVICE,		/* delete service */
-	IPVS_CMD_GET_SERVICE,		/* get service info */
+	IPVS_CMD_NEW_SERVICE,	/* add service */
+	IPVS_CMD_SET_SERVICE,	/* modify service */
+	IPVS_CMD_DEL_SERVICE,	/* delete service */
+	IPVS_CMD_GET_SERVICE,	/* get service info */
 
-	IPVS_CMD_NEW_DEST,		/* add destination */
-	IPVS_CMD_SET_DEST,		/* modify destination */
-	IPVS_CMD_DEL_DEST,		/* delete destination */
-	IPVS_CMD_GET_DEST,		/* get destination info */
+	IPVS_CMD_NEW_DEST,	/* add destination */
+	IPVS_CMD_SET_DEST,	/* modify destination */
+	IPVS_CMD_DEL_DEST,	/* delete destination */
+	IPVS_CMD_GET_DEST,	/* get destination info */
 
-	IPVS_CMD_NEW_DAEMON,		/* start sync daemon */
-	IPVS_CMD_DEL_DAEMON,		/* stop sync daemon */
-	IPVS_CMD_GET_DAEMON,		/* get sync daemon status */
+	IPVS_CMD_NEW_DAEMON,	/* start sync daemon */
+	IPVS_CMD_DEL_DAEMON,	/* stop sync daemon */
+	IPVS_CMD_GET_DAEMON,	/* get sync daemon status */
 
-	IPVS_CMD_SET_CONFIG,		/* set config settings */
-	IPVS_CMD_GET_CONFIG,		/* get config settings */
+	IPVS_CMD_SET_CONFIG,	/* set config settings */
+	IPVS_CMD_GET_CONFIG,	/* get config settings */
 
-	IPVS_CMD_SET_INFO,		/* only used in GET_INFO reply */
-	IPVS_CMD_GET_INFO,		/* get general IPVS info */
+	IPVS_CMD_SET_INFO,	/* only used in GET_INFO reply */
+	IPVS_CMD_GET_INFO,	/* get general IPVS info */
 
-	IPVS_CMD_ZERO,			/* zero all counters and stats */
-	IPVS_CMD_FLUSH,			/* flush services and dests */
+	IPVS_CMD_ZERO,		/* zero all counters and stats */
+	IPVS_CMD_FLUSH,		/* flush services and dests */
+
+	IPVS_CMD_NEW_LADDR,	/* add local address */
+	IPVS_CMD_DEL_LADDR,	/* del local address */
+	IPVS_CMD_GET_LADDR,	/* dump local address */
 
 	__IPVS_CMD_MAX,
 };
@@ -319,12 +321,13 @@ enum {
 /* Attributes used in the first level of commands */
 enum {
 	IPVS_CMD_ATTR_UNSPEC = 0,
-	IPVS_CMD_ATTR_SERVICE,		/* nested service attribute */
-	IPVS_CMD_ATTR_DEST,		/* nested destination attribute */
-	IPVS_CMD_ATTR_DAEMON,		/* nested sync daemon attribute */
+	IPVS_CMD_ATTR_SERVICE,	/* nested service attribute */
+	IPVS_CMD_ATTR_DEST,	/* nested destination attribute */
+	IPVS_CMD_ATTR_DAEMON,	/* nested sync daemon attribute */
 	IPVS_CMD_ATTR_TIMEOUT_TCP,	/* TCP connection timeout */
 	IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,	/* TCP FIN wait timeout */
 	IPVS_CMD_ATTR_TIMEOUT_UDP,	/* UDP timeout */
+	IPVS_CMD_ATTR_LADDR,	/* nested local address attribute */
 	__IPVS_CMD_ATTR_MAX,
 };
 
@@ -337,21 +340,18 @@ enum {
  */
 enum {
 	IPVS_SVC_ATTR_UNSPEC = 0,
-	IPVS_SVC_ATTR_AF,		/* address family */
-	IPVS_SVC_ATTR_PROTOCOL,		/* virtual service protocol */
-	IPVS_SVC_ATTR_ADDR,		/* virtual service address */
-	IPVS_SVC_ATTR_PORT,		/* virtual service port */
-	IPVS_SVC_ATTR_FWMARK,		/* firewall mark of service */
+	IPVS_SVC_ATTR_AF,	/* address family */
+	IPVS_SVC_ATTR_PROTOCOL,	/* virtual service protocol */
+	IPVS_SVC_ATTR_ADDR,	/* virtual service address */
+	IPVS_SVC_ATTR_PORT,	/* virtual service port */
+	IPVS_SVC_ATTR_FWMARK,	/* firewall mark of service */
 
 	IPVS_SVC_ATTR_SCHED_NAME,	/* name of scheduler */
-	IPVS_SVC_ATTR_FLAGS,		/* virtual service flags */
-	IPVS_SVC_ATTR_TIMEOUT,		/* persistent timeout */
-	IPVS_SVC_ATTR_NETMASK,		/* persistent netmask */
-
-	IPVS_SVC_ATTR_STATS,		/* nested attribute for service stats */
-
-	IPVS_SVC_ATTR_PE_NAME,		/* name of ct retriever */
+	IPVS_SVC_ATTR_FLAGS,	/* virtual service flags */
+	IPVS_SVC_ATTR_TIMEOUT,	/* persistent timeout */
+	IPVS_SVC_ATTR_NETMASK,	/* persistent netmask */
 
+	IPVS_SVC_ATTR_STATS,	/* nested attribute for service stats */
 	__IPVS_SVC_ATTR_MAX,
 };
 
@@ -364,11 +364,11 @@ enum {
  */
 enum {
 	IPVS_DEST_ATTR_UNSPEC = 0,
-	IPVS_DEST_ATTR_ADDR,		/* real server address */
-	IPVS_DEST_ATTR_PORT,		/* real server port */
+	IPVS_DEST_ATTR_ADDR,	/* real server address */
+	IPVS_DEST_ATTR_PORT,	/* real server port */
 
 	IPVS_DEST_ATTR_FWD_METHOD,	/* forwarding method */
-	IPVS_DEST_ATTR_WEIGHT,		/* destination weight */
+	IPVS_DEST_ATTR_WEIGHT,	/* destination weight */
 
 	IPVS_DEST_ATTR_U_THRESH,	/* upper threshold */
 	IPVS_DEST_ATTR_L_THRESH,	/* lower threshold */
@@ -377,20 +377,35 @@ enum {
 	IPVS_DEST_ATTR_INACT_CONNS,	/* inactive connections */
 	IPVS_DEST_ATTR_PERSIST_CONNS,	/* persistent connections */
 
-	IPVS_DEST_ATTR_STATS,		/* nested attribute for dest stats */
+	IPVS_DEST_ATTR_STATS,	/* nested attribute for dest stats */
 	__IPVS_DEST_ATTR_MAX,
 };
 
 #define IPVS_DEST_ATTR_MAX (__IPVS_DEST_ATTR_MAX - 1)
 
 /*
+ *  * Attirbutes used to describe a local address
+ *   *
+ *    */
+
+enum {
+	IPVS_LADDR_ATTR_UNSPEC = 0,
+	IPVS_LADDR_ATTR_ADDR,
+	IPVS_LADDR_ATTR_PORT_CONFLICT,
+	IPVS_LADDR_ATTR_CONN_COUNTS,
+	__IPVS_LADDR_ATTR_MAX,
+};
+
+#define IPVS_LADDR_ATTR_MAX (__IPVS_LADDR_ATTR_MAX - 1)
+
+/*
  * Attributes describing a sync daemon
  *
  * Used inside nested attribute IPVS_CMD_ATTR_DAEMON
  */
 enum {
 	IPVS_DAEMON_ATTR_UNSPEC = 0,
-	IPVS_DAEMON_ATTR_STATE,		/* sync daemon state (master/backup) */
+	IPVS_DAEMON_ATTR_STATE,	/* sync daemon state (master/backup) */
 	IPVS_DAEMON_ATTR_MCAST_IFN,	/* multicast interface name */
 	IPVS_DAEMON_ATTR_SYNC_ID,	/* SyncID we belong to */
 	__IPVS_DAEMON_ATTR_MAX,
@@ -405,17 +420,17 @@ enum {
  */
 enum {
 	IPVS_STATS_ATTR_UNSPEC = 0,
-	IPVS_STATS_ATTR_CONNS,		/* connections scheduled */
-	IPVS_STATS_ATTR_INPKTS,		/* incoming packets */
+	IPVS_STATS_ATTR_CONNS,	/* connections scheduled */
+	IPVS_STATS_ATTR_INPKTS,	/* incoming packets */
 	IPVS_STATS_ATTR_OUTPKTS,	/* outgoing packets */
 	IPVS_STATS_ATTR_INBYTES,	/* incoming bytes */
 	IPVS_STATS_ATTR_OUTBYTES,	/* outgoing bytes */
 
-	IPVS_STATS_ATTR_CPS,		/* current connection rate */
-	IPVS_STATS_ATTR_INPPS,		/* current in packet rate */
-	IPVS_STATS_ATTR_OUTPPS,		/* current out packet rate */
-	IPVS_STATS_ATTR_INBPS,		/* current in byte rate */
-	IPVS_STATS_ATTR_OUTBPS,		/* current out byte rate */
+	IPVS_STATS_ATTR_CPS,	/* current connection rate */
+	IPVS_STATS_ATTR_INPPS,	/* current in packet rate */
+	IPVS_STATS_ATTR_OUTPPS,	/* current out packet rate */
+	IPVS_STATS_ATTR_INBPS,	/* current in byte rate */
+	IPVS_STATS_ATTR_OUTBPS,	/* current out byte rate */
 	__IPVS_STATS_ATTR_MAX,
 };
 
@@ -424,11 +439,11 @@ enum {
 /* Attributes used in response to IPVS_CMD_GET_INFO command */
 enum {
 	IPVS_INFO_ATTR_UNSPEC = 0,
-	IPVS_INFO_ATTR_VERSION,		/* IPVS version number */
+	IPVS_INFO_ATTR_VERSION,	/* IPVS version number */
 	IPVS_INFO_ATTR_CONN_TAB_SIZE,	/* size of connection hash table */
 	__IPVS_INFO_ATTR_MAX,
 };
 
 #define IPVS_INFO_ATTR_MAX (__IPVS_INFO_ATTR_MAX - 1)
 
-#endif	/* _IP_VS_H */
+#endif				/* _IP_VS_H */
diff --git a/net/core/secure_seq.c b/net/core/secure_seq.c
index 15b1263..6cb3185 100644
--- a/net/core/secure_seq.c
+++ b/net/core/secure_seq.c
@@ -126,6 +126,7 @@ __u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,
 
 	return seq_scale(hash[0]);
 }
+EXPORT_SYMBOL(secure_tcp_sequence_number);
 
 u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)
 {
diff --git a/net/ipv4/syncookies.c b/net/ipv4/syncookies.c
index 650ced4..695618e 100644
--- a/net/ipv4/syncookies.c
+++ b/net/ipv4/syncookies.c
@@ -18,6 +18,7 @@
 #include <linux/export.h>
 #include <net/tcp.h>
 #include <net/route.h>
+#include <net/ip_vs_synproxy.h>
 
 extern int sysctl_tcp_syncookies;
 
@@ -401,3 +402,88 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 		inet_sk(ret)->cork.fl.u.ip4 = fl4;
 out:	return ret;
 }
+
+/*
+ * Generate a syncookie for ip_vs module. 
+ * Besides mss, we store additional tcp options in cookie "data".
+ * 
+ * Cookie "data" format: 
+ * |[21][20][19-16][15-0]|
+ * [21] SACKOK
+ * [20] TimeStampOK
+ * [19-16] snd_wscale
+ * [15-0] MSSIND 
+ */
+__u32 ip_vs_synproxy_cookie_v4_init_sequence(struct sk_buff *skb, 
+					     struct ip_vs_synproxy_opt *opts) 
+{
+	const struct iphdr *iph = ip_hdr(skb);
+	const struct tcphdr *th = tcp_hdr(skb);
+	int mssind;
+	const __u16 mss = opts->mss_clamp;
+	__u32 data = 0;
+
+	/* XXX sort msstab[] by probability?  Binary search? */
+	for (mssind = 0; mss > msstab[mssind + 1]; mssind++)
+		;
+	opts->mss_clamp = msstab[mssind] + 1;
+
+	data = mssind & IP_VS_SYNPROXY_MSS_MASK;
+	data |= opts->sack_ok << IP_VS_SYNPROXY_SACKOK_BIT;
+	data |= opts->tstamp_ok << IP_VS_SYNPROXY_TSOK_BIT;
+	data |= ((opts->snd_wscale & 0x0f) << IP_VS_SYNPROXY_SND_WSCALE_BITS);
+
+	return secure_tcp_syn_cookie(iph->saddr, iph->daddr,
+				     th->source, th->dest, ntohl(th->seq),
+				     data);
+}
+EXPORT_SYMBOL(ip_vs_synproxy_cookie_v4_init_sequence);
+
+
+/*
+ * when ip_vs_synproxy_cookie_v4_init_sequence is used, we check
+ * cookie as follow:
+ *  1. mssind check.
+ *  2. get sack/timestamp/wscale options.
+ */
+int ip_vs_synproxy_v4_cookie_check(struct sk_buff * skb, __u32 cookie, 
+			      struct ip_vs_synproxy_opt * opt) 
+{
+	const struct iphdr *iph = ip_hdr(skb);
+	const struct tcphdr *th = tcp_hdr(skb);
+	__u32 seq = ntohl(th->seq) - 1;
+	__u32 mssind;
+	int   ret = 0;
+	__u32 res = check_tcp_syn_cookie(cookie, iph->saddr, iph->daddr,
+					 th->source, th->dest, seq);
+
+	if(res == (__u32)-1) /* count is invalid, jiffies' >> jiffies */
+		goto out;
+
+	mssind = res & IP_VS_SYNPROXY_MSS_MASK;
+
+	memset(opt, 0, sizeof(struct ip_vs_synproxy_opt));
+
+	if (mssind < (ARRAY_SIZE(msstab) - 1)) {
+		opt->mss_clamp = msstab[mssind] + 1;
+		opt->sack_ok = (res & IP_VS_SYNPROXY_SACKOK_MASK) >> 
+					IP_VS_SYNPROXY_SACKOK_BIT;
+		opt->tstamp_ok = (res & IP_VS_SYNPROXY_TSOK_MASK) >> 
+					IP_VS_SYNPROXY_TSOK_BIT;
+		opt->snd_wscale = (res & IP_VS_SYNPROXY_SND_WSCALE_MASK) >> 
+					IP_VS_SYNPROXY_SND_WSCALE_BITS;
+                if (opt->snd_wscale > 0 && 
+		    opt->snd_wscale <= IP_VS_SYNPROXY_WSCALE_MAX)
+                        opt->wscale_ok = 1;
+                else if (opt->snd_wscale == 0)
+                        opt->wscale_ok = 0;
+                else
+                        goto out;
+
+		ret = 1;
+	}
+
+out:	return ret;
+}
+EXPORT_SYMBOL(ip_vs_synproxy_v4_cookie_check);
+
diff --git a/net/ipv6/syncookies.c b/net/ipv6/syncookies.c
index 47e5432..c874df5 100644
--- a/net/ipv6/syncookies.c
+++ b/net/ipv6/syncookies.c
@@ -20,6 +20,7 @@
 #include <linux/kernel.h>
 #include <net/ipv6.h>
 #include <net/tcp.h>
+#include <net/ip_vs_synproxy.h>
 
 #define COOKIEBITS 24	/* Upper bits store count */
 #define COOKIEMASK (((__u32)1 << COOKIEBITS) - 1)
@@ -267,3 +268,84 @@ out_free:
 	return NULL;
 }
 
+/*
+ * Generate a syncookie for ip_vs module. 
+ * Besides mss, we store additional tcp options in cookie "data".
+ * 
+ * Cookie "data" format: 
+ * |[21][20][19-16][15-0]|
+ * [21] SACKOK
+ * [20] TimeStampOK
+ * [19-16] snd_wscale
+ * [15-0] MSSIND 
+ */
+__u32 ip_vs_synproxy_cookie_v6_init_sequence(struct sk_buff *skb, 
+					     struct ip_vs_synproxy_opt *opts) 
+{
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	const struct tcphdr *th = tcp_hdr(skb);
+	int mssind;
+	const __u16 mss = opts->mss_clamp;
+	__u32 data = 0;
+
+	/* XXX sort msstab[] by probability?  Binary search? */
+	for (mssind = 0; mss > msstab[mssind + 1]; mssind++)
+		;
+	opts->mss_clamp = msstab[mssind] + 1;
+
+	data = mssind & IP_VS_SYNPROXY_MSS_MASK;
+	data |= opts->sack_ok << IP_VS_SYNPROXY_SACKOK_BIT;
+	data |= opts->tstamp_ok << IP_VS_SYNPROXY_TSOK_BIT;
+	data |= ((opts->snd_wscale & 0x0f) << IP_VS_SYNPROXY_SND_WSCALE_BITS);
+
+	return secure_tcp_syn_cookie(&iph->saddr, &iph->daddr,
+				     th->source, th->dest, ntohl(th->seq),
+				     data);
+}
+EXPORT_SYMBOL(ip_vs_synproxy_cookie_v6_init_sequence);
+
+/*
+ * when ip_vs_synproxy_cookie_v6_init_sequence is used, we check
+ * cookie as follow:
+ *  1. mssind check.
+ *  2. get sack/timestamp/wscale options.
+ */
+int ip_vs_synproxy_v6_cookie_check(struct sk_buff * skb, __u32 cookie, 
+			      struct ip_vs_synproxy_opt * opt) 
+{
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	const struct tcphdr *th = tcp_hdr(skb);
+	__u32 seq = ntohl(th->seq) - 1;
+	__u32 mssind;
+	int   ret = 0;
+	__u32 res = check_tcp_syn_cookie(cookie, &iph->saddr, &iph->daddr,
+					 th->source, th->dest, seq);
+	if(res == (__u32)-1) /* count is invalid, jiffies' >> jiffies */
+		goto out;
+
+	mssind = res & IP_VS_SYNPROXY_MSS_MASK;
+
+	memset(opt, 0, sizeof(struct ip_vs_synproxy_opt));
+
+	if (mssind < (ARRAY_SIZE(msstab) - 1)) {
+		opt->mss_clamp = msstab[mssind] + 1;
+		opt->sack_ok = (res & IP_VS_SYNPROXY_SACKOK_MASK) >> 
+					IP_VS_SYNPROXY_SACKOK_BIT;
+		opt->tstamp_ok = (res & IP_VS_SYNPROXY_TSOK_MASK) >> 
+					IP_VS_SYNPROXY_TSOK_BIT;
+		opt->snd_wscale = (res & IP_VS_SYNPROXY_SND_WSCALE_MASK) >> 
+					IP_VS_SYNPROXY_SND_WSCALE_BITS;
+                if (opt->snd_wscale > 0 && 
+		    opt->snd_wscale <= IP_VS_SYNPROXY_WSCALE_MAX)
+                        opt->wscale_ok = 1;
+                else if (opt->snd_wscale == 0)
+                        opt->wscale_ok = 0;
+                else
+                        goto out;
+
+		ret = 1;
+	}
+
+out:	return ret;
+}
+EXPORT_SYMBOL(ip_vs_synproxy_v6_cookie_check);
diff --git a/net/netfilter/ipvs/Kconfig b/net/netfilter/ipvs/Kconfig
index 0c3b167..e1e4401 100644
--- a/net/netfilter/ipvs/Kconfig
+++ b/net/netfilter/ipvs/Kconfig
@@ -4,7 +4,6 @@
 menuconfig IP_VS
 	tristate "IP virtual server support"
 	depends on NET && INET && NETFILTER
-	depends on (NF_CONNTRACK || NF_CONNTRACK=n)
 	---help---
 	  IP Virtual Server support will let you build a high-performance
 	  virtual server based on cluster of two or more real servers. This
@@ -27,12 +26,13 @@ if IP_VS
 
 config	IP_VS_IPV6
 	bool "IPv6 support for IPVS"
-	depends on IPV6 = y || IP_VS = IPV6
-	select IP6_NF_IPTABLES
+	depends on EXPERIMENTAL && (IPV6 = y || IP_VS = IPV6)
 	---help---
-	  Add IPv6 support to IPVS.
+	  Add IPv6 support to IPVS. This is incomplete and might be dangerous.
 
-	  Say Y if unsure.
+	  See http://www.mindbasket.com/ipvs for more information.
+
+	  Say N if unsure.
 
 config	IP_VS_DEBUG
 	bool "IP virtual server debugging"
@@ -43,8 +43,8 @@ config	IP_VS_DEBUG
 
 config	IP_VS_TAB_BITS
 	int "IPVS connection table size (the Nth power of 2)"
-	range 8 20
-	default 12
+	range 8 22
+	default 22
 	---help---
 	  The IPVS connection hash table uses the chaining scheme to handle
 	  hash collisions. Using a big IPVS connection hash table will greatly
@@ -68,10 +68,6 @@ config	IP_VS_TAB_BITS
 	  each hash entry uses 8 bytes, so you can estimate how much memory is
 	  needed for your box.
 
-	  You can overwrite this number setting conn_tab_bits module parameter
-	  or by appending ip_vs.conn_tab_bits=? to the kernel command line
-	  if IP VS was compiled built-in.
-
 comment "IPVS transport protocol load balancing support"
 
 config	IP_VS_PROTO_TCP
@@ -87,27 +83,23 @@ config	IP_VS_PROTO_UDP
 	  protocol. Say Y if unsure.
 
 config	IP_VS_PROTO_AH_ESP
-	def_bool IP_VS_PROTO_ESP || IP_VS_PROTO_AH
+	bool
+	depends on UNDEFINED
 
 config	IP_VS_PROTO_ESP
 	bool "ESP load balancing support"
+	select IP_VS_PROTO_AH_ESP
 	---help---
 	  This option enables support for load balancing ESP (Encapsulation
 	  Security Payload) transport protocol. Say Y if unsure.
 
 config	IP_VS_PROTO_AH
 	bool "AH load balancing support"
+	select IP_VS_PROTO_AH_ESP
 	---help---
 	  This option enables support for load balancing AH (Authentication
 	  Header) transport protocol. Say Y if unsure.
 
-config  IP_VS_PROTO_SCTP
-	bool "SCTP load balancing support"
-	select LIBCRC32C
-	---help---
-	  This option enables support for load balancing SCTP transport
-	  protocol. Say Y if unsure.
-
 comment "IPVS scheduler"
 
 config	IP_VS_RR
@@ -120,7 +112,7 @@ config	IP_VS_RR
 	  module, choose M here. If unsure, say N.
  
 config	IP_VS_WRR
-	tristate "weighted round-robin scheduling"
+        tristate "weighted round-robin scheduling" 
 	---help---
 	  The weighted robin-robin scheduling algorithm directs network
 	  connections to different real servers based on server weights
@@ -230,28 +222,11 @@ config	IP_VS_NQ
 	  If you want to compile it in kernel, say Y. To compile it as a
 	  module, choose M here. If unsure, say N.
 
-comment 'IPVS SH scheduler'
-
-config IP_VS_SH_TAB_BITS
-	int "IPVS source hashing table size (the Nth power of 2)"
-	range 4 20
-	default 8
-	---help---
-	  The source hashing scheduler maps source IPs to destinations
-	  stored in a hash table. This table is tiled by each destination
-	  until all slots in the table are filled. When using weights to
-	  allow destinations to receive more connections, the table is
-	  tiled an amount proportional to the weights specified. The table
-	  needs to be large enough to effectively fit all the destinations
-	  multiplied by their respective weights.
-
 comment 'IPVS application helper'
 
 config	IP_VS_FTP
   	tristate "FTP protocol helper"
-	depends on IP_VS_PROTO_TCP && NF_CONNTRACK && NF_NAT && \
-		NF_CONNTRACK_FTP
-	select IP_VS_NFCT
+        depends on IP_VS_PROTO_TCP
 	---help---
 	  FTP is a protocol that transfers IP address and/or port number in
 	  the payload. In the virtual server via Network Address Translation,
@@ -263,19 +238,4 @@ config	IP_VS_FTP
 	  If you want to compile it in kernel, say Y. To compile it as a
 	  module, choose M here. If unsure, say N.
 
-config	IP_VS_NFCT
-	bool "Netfilter connection tracking"
-	depends on NF_CONNTRACK
-	---help---
-	  The Netfilter connection tracking support allows the IPVS
-	  connection state to be exported to the Netfilter framework
-	  for filtering purposes.
-
-config	IP_VS_PE_SIP
-	tristate "SIP persistence engine"
-        depends on IP_VS_PROTO_UDP
-	depends on NF_CONNTRACK_SIP
-	---help---
-	  Allow persistence based on the SIP Call-ID
-
 endif # IP_VS
diff --git a/net/netfilter/ipvs/Makefile b/net/netfilter/ipvs/Makefile
index 34ee602..f7493c5 100644
--- a/net/netfilter/ipvs/Makefile
+++ b/net/netfilter/ipvs/Makefile
@@ -7,15 +7,12 @@ ip_vs_proto-objs-y :=
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_TCP) += ip_vs_proto_tcp.o
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_UDP) += ip_vs_proto_udp.o
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_AH_ESP) += ip_vs_proto_ah_esp.o
-ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_SCTP) += ip_vs_proto_sctp.o
 
-ip_vs-extra_objs-y :=
-ip_vs-extra_objs-$(CONFIG_IP_VS_NFCT) += ip_vs_nfct.o
-
-ip_vs-objs :=	ip_vs_conn.o ip_vs_core.o ip_vs_ctl.o ip_vs_sched.o	   \
-		ip_vs_xmit.o ip_vs_app.o ip_vs_sync.o	   		   \
-		ip_vs_est.o ip_vs_proto.o ip_vs_pe.o			   \
-		$(ip_vs_proto-objs-y) $(ip_vs-extra_objs-y)
+ip_vs-objs :=	ip_vs_conn.o ip_vs_core.o ip_vs_ctl.o ip_vs_sched.o   \
+		ip_vs_xmit.o ip_vs_app.o ip_vs_sync.o                 \
+		ip_vs_proto.o                                         \
+		ip_vs_synproxy.o ip_vs_stats.o                        \
+		$(ip_vs_proto-objs-y)
 
 
 # IPVS core
@@ -35,6 +32,3 @@ obj-$(CONFIG_IP_VS_NQ) += ip_vs_nq.o
 
 # IPVS application helpers
 obj-$(CONFIG_IP_VS_FTP) += ip_vs_ftp.o
-
-# IPVS connection template retrievers
-obj-$(CONFIG_IP_VS_PE_SIP) += ip_vs_pe_sip.o
diff --git a/net/netfilter/ipvs/ip_vs_app.c b/net/netfilter/ipvs/ip_vs_app.c
index dfd7b65..d2e2d9b 100644
--- a/net/netfilter/ipvs/ip_vs_app.c
+++ b/net/netfilter/ipvs/ip_vs_app.c
@@ -27,7 +27,6 @@
 #include <linux/in.h>
 #include <linux/ip.h>
 #include <linux/netfilter.h>
-#include <linux/slab.h>
 #include <net/net_namespace.h>
 #include <net/protocol.h>
 #include <net/tcp.h>
@@ -42,6 +41,8 @@ EXPORT_SYMBOL(register_ip_vs_app);
 EXPORT_SYMBOL(unregister_ip_vs_app);
 EXPORT_SYMBOL(register_ip_vs_app_inc);
 
+/* ipvs application list head */
+static LIST_HEAD(ip_vs_app_list);
 static DEFINE_MUTEX(__ip_vs_app_mutex);
 
 /*
@@ -52,31 +53,15 @@ static inline int ip_vs_app_get(struct ip_vs_app *app)
 	return try_module_get(app->module);
 }
 
-
 static inline void ip_vs_app_put(struct ip_vs_app *app)
 {
 	module_put(app->module);
 }
 
-static void ip_vs_app_inc_destroy(struct ip_vs_app *inc)
-{
-	kfree(inc->timeout_table);
-	kfree(inc);
-}
-
-static void ip_vs_app_inc_rcu_free(struct rcu_head *head)
-{
-	struct ip_vs_app *inc = container_of(head, struct ip_vs_app, rcu_head);
-
-	ip_vs_app_inc_destroy(inc);
-}
-
 /*
  *	Allocate/initialize app incarnation and register it in proto apps.
  */
-static int
-ip_vs_app_inc_new(struct net *net, struct ip_vs_app *app, __u16 proto,
-		  __u16 port)
+static int ip_vs_app_inc_new(struct ip_vs_app *app, __u16 proto, __u16 port)
 {
 	struct ip_vs_protocol *pp;
 	struct ip_vs_app *inc;
@@ -99,35 +84,34 @@ ip_vs_app_inc_new(struct net *net, struct ip_vs_app *app, __u16 proto,
 
 	if (app->timeouts) {
 		inc->timeout_table =
-			ip_vs_create_timeout_table(app->timeouts,
-						   app->timeouts_size);
+		    ip_vs_create_timeout_table(app->timeouts,
+					       app->timeouts_size);
 		if (!inc->timeout_table) {
 			ret = -ENOMEM;
 			goto out;
 		}
 	}
 
-	ret = pp->register_app(net, inc);
+	ret = pp->register_app(inc);
 	if (ret)
 		goto out;
 
 	list_add(&inc->a_list, &app->incs_list);
-	IP_VS_DBG(9, "%s App %s:%u registered\n",
-		  pp->name, inc->name, ntohs(inc->port));
+	IP_VS_DBG(9, "%s application %s:%u registered\n",
+		  pp->name, inc->name, inc->port);
 
 	return 0;
 
-  out:
-	ip_vs_app_inc_destroy(inc);
+      out:
+	kfree(inc->timeout_table);
+	kfree(inc);
 	return ret;
 }
 
-
 /*
  *	Release app incarnation
  */
-static void
-ip_vs_app_inc_release(struct net *net, struct ip_vs_app *inc)
+static void ip_vs_app_inc_release(struct ip_vs_app *inc)
 {
 	struct ip_vs_protocol *pp;
 
@@ -135,17 +119,17 @@ ip_vs_app_inc_release(struct net *net, struct ip_vs_app *inc)
 		return;
 
 	if (pp->unregister_app)
-		pp->unregister_app(net, inc);
+		pp->unregister_app(inc);
 
 	IP_VS_DBG(9, "%s App %s:%u unregistered\n",
-		  pp->name, inc->name, ntohs(inc->port));
+		  pp->name, inc->name, inc->port);
 
 	list_del(&inc->a_list);
 
-	call_rcu(&inc->rcu_head, ip_vs_app_inc_rcu_free);
+	kfree(inc->timeout_table);
+	kfree(inc);
 }
 
-
 /*
  *	Get reference to app inc (only called from softirq)
  *
@@ -154,120 +138,84 @@ int ip_vs_app_inc_get(struct ip_vs_app *inc)
 {
 	int result;
 
-	result = ip_vs_app_get(inc->app);
-	if (result)
-		atomic_inc(&inc->usecnt);
+	atomic_inc(&inc->usecnt);
+	if (unlikely((result = ip_vs_app_get(inc->app)) != 1))
+		atomic_dec(&inc->usecnt);
 	return result;
 }
 
-
 /*
  *	Put the app inc (only called from timer or net softirq)
  */
 void ip_vs_app_inc_put(struct ip_vs_app *inc)
 {
-	atomic_dec(&inc->usecnt);
 	ip_vs_app_put(inc->app);
+	atomic_dec(&inc->usecnt);
 }
 
-
 /*
  *	Register an application incarnation in protocol applications
  */
-int
-register_ip_vs_app_inc(struct net *net, struct ip_vs_app *app, __u16 proto,
-		       __u16 port)
+int register_ip_vs_app_inc(struct ip_vs_app *app, __u16 proto, __u16 port)
 {
 	int result;
 
 	mutex_lock(&__ip_vs_app_mutex);
 
-	result = ip_vs_app_inc_new(net, app, proto, port);
+	result = ip_vs_app_inc_new(app, proto, port);
 
 	mutex_unlock(&__ip_vs_app_mutex);
 
 	return result;
 }
 
-
-/* Register application for netns */
-struct ip_vs_app *register_ip_vs_app(struct net *net, struct ip_vs_app *app)
+/*
+ *	ip_vs_app registration routine
+ */
+int register_ip_vs_app(struct ip_vs_app *app)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_app *a;
-	int err = 0;
-
-	if (!ipvs)
-		return ERR_PTR(-ENOENT);
+	/* increase the module use count */
+	ip_vs_use_count_inc();
 
 	mutex_lock(&__ip_vs_app_mutex);
 
-	list_for_each_entry(a, &ipvs->app_list, a_list) {
-		if (!strcmp(app->name, a->name)) {
-			err = -EEXIST;
-			goto out_unlock;
-		}
-	}
-	a = kmemdup(app, sizeof(*app), GFP_KERNEL);
-	if (!a) {
-		err = -ENOMEM;
-		goto out_unlock;
-	}
-	INIT_LIST_HEAD(&a->incs_list);
-	list_add(&a->a_list, &ipvs->app_list);
-	/* increase the module use count */
-	ip_vs_use_count_inc();
+	list_add(&app->a_list, &ip_vs_app_list);
 
-out_unlock:
 	mutex_unlock(&__ip_vs_app_mutex);
 
-	return err ? ERR_PTR(err) : a;
+	return 0;
 }
 
-
 /*
  *	ip_vs_app unregistration routine
  *	We are sure there are no app incarnations attached to services
- *	Caller should use synchronize_rcu() or rcu_barrier()
  */
-void unregister_ip_vs_app(struct net *net, struct ip_vs_app *app)
+void unregister_ip_vs_app(struct ip_vs_app *app)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_app *a, *anxt, *inc, *nxt;
-
-	if (!ipvs)
-		return;
+	struct ip_vs_app *inc, *nxt;
 
 	mutex_lock(&__ip_vs_app_mutex);
 
-	list_for_each_entry_safe(a, anxt, &ipvs->app_list, a_list) {
-		if (app && strcmp(app->name, a->name))
-			continue;
-		list_for_each_entry_safe(inc, nxt, &a->incs_list, a_list) {
-			ip_vs_app_inc_release(net, inc);
-		}
-
-		list_del(&a->a_list);
-		kfree(a);
-
-		/* decrease the module use count */
-		ip_vs_use_count_dec();
+	list_for_each_entry_safe(inc, nxt, &app->incs_list, a_list) {
+		ip_vs_app_inc_release(inc);
 	}
 
+	list_del(&app->a_list);
+
 	mutex_unlock(&__ip_vs_app_mutex);
-}
 
+	/* decrease the module use count */
+	ip_vs_use_count_dec();
+}
 
 /*
  *	Bind ip_vs_conn to its ip_vs_app (called by cp constructor)
  */
-int ip_vs_bind_app(struct ip_vs_conn *cp,
-		   struct ip_vs_protocol *pp)
+int ip_vs_bind_app(struct ip_vs_conn *cp, struct ip_vs_protocol *pp)
 {
 	return pp->app_conn_bind(cp);
 }
 
-
 /*
  *	Unbind cp from application incarnation (called by cp destructor)
  */
@@ -286,7 +234,6 @@ void ip_vs_unbind_app(struct ip_vs_conn *cp)
 	cp->app = NULL;
 }
 
-
 /*
  *	Fixes th->seq based on ip_vs_seq info.
  */
@@ -295,12 +242,12 @@ static inline void vs_fix_seq(const struct ip_vs_seq *vseq, struct tcphdr *th)
 	__u32 seq = ntohl(th->seq);
 
 	/*
-	 *	Adjust seq with delta-offset for all packets after
-	 *	the most recent resized pkt seq and with previous_delta offset
-	 *	for all packets	before most recent resized pkt seq.
+	 *      Adjust seq with delta-offset for all packets after
+	 *      the most recent resized pkt seq and with previous_delta offset
+	 *      for all packets before most recent resized pkt seq.
 	 */
 	if (vseq->delta || vseq->previous_delta) {
-		if(after(seq, vseq->init_seq)) {
+		if (after(seq, vseq->init_seq)) {
 			th->seq = htonl(seq + vseq->delta);
 			IP_VS_DBG(9, "%s(): added delta (%d) to seq\n",
 				  __func__, vseq->delta);
@@ -312,7 +259,6 @@ static inline void vs_fix_seq(const struct ip_vs_seq *vseq, struct tcphdr *th)
 	}
 }
 
-
 /*
  *	Fixes th->ack_seq based on ip_vs_seq info.
  */
@@ -329,7 +275,7 @@ vs_fix_ack_seq(const struct ip_vs_seq *vseq, struct tcphdr *th)
 	if (vseq->delta || vseq->previous_delta) {
 		/* since ack_seq is the number of octet that is expected
 		   to receive next, so compare it with init_seq+delta */
-		if(after(ack_seq, vseq->init_seq+vseq->delta)) {
+		if (after(ack_seq, vseq->init_seq + vseq->delta)) {
 			th->ack_seq = htonl(ack_seq - vseq->delta);
 			IP_VS_DBG(9, "%s(): subtracted delta "
 				  "(%d) from ack_seq\n", __func__, vseq->delta);
@@ -343,23 +289,22 @@ vs_fix_ack_seq(const struct ip_vs_seq *vseq, struct tcphdr *th)
 	}
 }
 
-
 /*
  *	Updates ip_vs_seq if pkt has been resized
  *	Assumes already checked proto==IPPROTO_TCP and diff!=0.
  */
 static inline void vs_seq_update(struct ip_vs_conn *cp, struct ip_vs_seq *vseq,
-				 unsigned int flag, __u32 seq, int diff)
+				 unsigned flag, __u32 seq, int diff)
 {
 	/* spinlock is to keep updating cp->flags atomic */
-	spin_lock_bh(&cp->lock);
+	spin_lock(&cp->lock);
 	if (!(cp->flags & flag) || after(seq, vseq->init_seq)) {
 		vseq->previous_delta = vseq->delta;
 		vseq->delta += diff;
 		vseq->init_seq = seq;
 		cp->flags |= flag;
 	}
-	spin_unlock_bh(&cp->lock);
+	spin_unlock(&cp->lock);
 }
 
 static inline int app_tcp_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb,
@@ -376,12 +321,12 @@ static inline int app_tcp_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb,
 	th = (struct tcphdr *)(skb_network_header(skb) + tcp_offset);
 
 	/*
-	 *	Remember seq number in case this pkt gets resized
+	 *      Remember seq number in case this pkt gets resized
 	 */
 	seq = ntohl(th->seq);
 
 	/*
-	 *	Fix seq stuff if flagged as so.
+	 *      Fix seq stuff if flagged as so.
 	 */
 	if (cp->flags & IP_VS_CONN_F_OUT_SEQ)
 		vs_fix_seq(&cp->out_seq, th);
@@ -389,7 +334,7 @@ static inline int app_tcp_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb,
 		vs_fix_ack_seq(&cp->in_seq, th);
 
 	/*
-	 *	Call private output hook function
+	 *      Call private output hook function
 	 */
 	if (app->pkt_out == NULL)
 		return 1;
@@ -398,7 +343,7 @@ static inline int app_tcp_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb,
 		return 0;
 
 	/*
-	 *	Update ip_vs seq stuff if len has changed.
+	 *      Update ip_vs seq stuff if len has changed.
 	 */
 	if (diff != 0)
 		vs_seq_update(cp, &cp->out_seq,
@@ -417,8 +362,8 @@ int ip_vs_app_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb)
 	struct ip_vs_app *app;
 
 	/*
-	 *	check if application module is bound to
-	 *	this ip_vs_conn.
+	 *      check if application module is bound to
+	 *      this ip_vs_conn.
 	 */
 	if ((app = cp->app) == NULL)
 		return 1;
@@ -428,7 +373,7 @@ int ip_vs_app_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb)
 		return app_tcp_pkt_out(cp, skb, app);
 
 	/*
-	 *	Call private output hook function
+	 *      Call private output hook function
 	 */
 	if (app->pkt_out == NULL)
 		return 1;
@@ -436,7 +381,6 @@ int ip_vs_app_pkt_out(struct ip_vs_conn *cp, struct sk_buff *skb)
 	return app->pkt_out(app, cp, skb, NULL);
 }
 
-
 static inline int app_tcp_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb,
 				 struct ip_vs_app *app)
 {
@@ -451,12 +395,12 @@ static inline int app_tcp_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb,
 	th = (struct tcphdr *)(skb_network_header(skb) + tcp_offset);
 
 	/*
-	 *	Remember seq number in case this pkt gets resized
+	 *      Remember seq number in case this pkt gets resized
 	 */
 	seq = ntohl(th->seq);
 
 	/*
-	 *	Fix seq stuff if flagged as so.
+	 *      Fix seq stuff if flagged as so.
 	 */
 	if (cp->flags & IP_VS_CONN_F_IN_SEQ)
 		vs_fix_seq(&cp->in_seq, th);
@@ -464,7 +408,7 @@ static inline int app_tcp_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb,
 		vs_fix_ack_seq(&cp->out_seq, th);
 
 	/*
-	 *	Call private input hook function
+	 *      Call private input hook function
 	 */
 	if (app->pkt_in == NULL)
 		return 1;
@@ -473,11 +417,10 @@ static inline int app_tcp_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb,
 		return 0;
 
 	/*
-	 *	Update ip_vs seq stuff if len has changed.
+	 *      Update ip_vs seq stuff if len has changed.
 	 */
 	if (diff != 0)
-		vs_seq_update(cp, &cp->in_seq,
-			      IP_VS_CONN_F_IN_SEQ, seq, diff);
+		vs_seq_update(cp, &cp->in_seq, IP_VS_CONN_F_IN_SEQ, seq, diff);
 
 	return 1;
 }
@@ -492,8 +435,8 @@ int ip_vs_app_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb)
 	struct ip_vs_app *app;
 
 	/*
-	 *	check if application module is bound to
-	 *	this ip_vs_conn.
+	 *      check if application module is bound to
+	 *      this ip_vs_conn.
 	 */
 	if ((app = cp->app) == NULL)
 		return 1;
@@ -503,7 +446,7 @@ int ip_vs_app_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb)
 		return app_tcp_pkt_in(cp, skb, app);
 
 	/*
-	 *	Call private input hook function
+	 *      Call private input hook function
 	 */
 	if (app->pkt_in == NULL)
 		return 1;
@@ -511,17 +454,16 @@ int ip_vs_app_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb)
 	return app->pkt_in(app, cp, skb, NULL);
 }
 
-
 #ifdef CONFIG_PROC_FS
 /*
  *	/proc/net/ip_vs_app entry function
  */
 
-static struct ip_vs_app *ip_vs_app_idx(struct netns_ipvs *ipvs, loff_t pos)
+static struct ip_vs_app *ip_vs_app_idx(loff_t pos)
 {
 	struct ip_vs_app *app, *inc;
 
-	list_for_each_entry(app, &ipvs->app_list, a_list) {
+	list_for_each_entry(app, &ip_vs_app_list, a_list) {
 		list_for_each_entry(inc, &app->incs_list, a_list) {
 			if (pos-- == 0)
 				return inc;
@@ -531,26 +473,21 @@ static struct ip_vs_app *ip_vs_app_idx(struct netns_ipvs *ipvs, loff_t pos)
 
 }
 
-static void *ip_vs_app_seq_start(struct seq_file *seq, loff_t *pos)
+static void *ip_vs_app_seq_start(struct seq_file *seq, loff_t * pos)
 {
-	struct net *net = seq_file_net(seq);
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
 	mutex_lock(&__ip_vs_app_mutex);
 
-	return *pos ? ip_vs_app_idx(ipvs, *pos - 1) : SEQ_START_TOKEN;
+	return *pos ? ip_vs_app_idx(*pos - 1) : SEQ_START_TOKEN;
 }
 
-static void *ip_vs_app_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+static void *ip_vs_app_seq_next(struct seq_file *seq, void *v, loff_t * pos)
 {
 	struct ip_vs_app *inc, *app;
 	struct list_head *e;
-	struct net *net = seq_file_net(seq);
-	struct netns_ipvs *ipvs = net_ipvs(net);
 
 	++*pos;
 	if (v == SEQ_START_TOKEN)
-		return ip_vs_app_idx(ipvs, 0);
+		return ip_vs_app_idx(0);
 
 	inc = v;
 	app = inc->app;
@@ -559,7 +496,7 @@ static void *ip_vs_app_seq_next(struct seq_file *seq, void *v, loff_t *pos)
 		return list_entry(e, struct ip_vs_app, a_list);
 
 	/* go on to next application */
-	for (e = app->a_list.next; e != &ipvs->app_list; e = e->next) {
+	for (e = app->a_list.next; e != &ip_vs_app_list; e = e->next) {
 		app = list_entry(e, struct ip_vs_app, a_list);
 		list_for_each_entry(inc, &app->incs_list, a_list) {
 			return inc;
@@ -583,45 +520,81 @@ static int ip_vs_app_seq_show(struct seq_file *seq, void *v)
 		seq_printf(seq, "%-3s  %-7u %-6d %-17s\n",
 			   ip_vs_proto_name(inc->protocol),
 			   ntohs(inc->port),
-			   atomic_read(&inc->usecnt),
-			   inc->name);
+			   atomic_read(&inc->usecnt), inc->name);
 	}
 	return 0;
 }
 
 static const struct seq_operations ip_vs_app_seq_ops = {
 	.start = ip_vs_app_seq_start,
-	.next  = ip_vs_app_seq_next,
-	.stop  = ip_vs_app_seq_stop,
-	.show  = ip_vs_app_seq_show,
+	.next = ip_vs_app_seq_next,
+	.stop = ip_vs_app_seq_stop,
+	.show = ip_vs_app_seq_show,
 };
 
 static int ip_vs_app_open(struct inode *inode, struct file *file)
 {
-	return seq_open_net(inode, file, &ip_vs_app_seq_ops,
-			    sizeof(struct seq_net_private));
+	return seq_open(file, &ip_vs_app_seq_ops);
 }
 
 static const struct file_operations ip_vs_app_fops = {
-	.owner	 = THIS_MODULE,
-	.open	 = ip_vs_app_open,
-	.read	 = seq_read,
-	.llseek  = seq_lseek,
-	.release = seq_release_net,
+	.owner = THIS_MODULE,
+	.open = ip_vs_app_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
 };
 #endif
 
-int __net_init ip_vs_app_net_init(struct net *net)
+/*
+ *	Replace a segment of data with a new segment
+ */
+int ip_vs_skb_replace(struct sk_buff *skb, gfp_t pri,
+		      char *o_buf, int o_len, char *n_buf, int n_len)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
+	int diff;
+	int o_offset;
+	int o_left;
+
+	EnterFunction(9);
+
+	diff = n_len - o_len;
+	o_offset = o_buf - (char *)skb->data;
+	/* The length of left data after o_buf+o_len in the skb data */
+	o_left = skb->len - (o_offset + o_len);
+
+	if (diff <= 0) {
+		memmove(o_buf + n_len, o_buf + o_len, o_left);
+		memcpy(o_buf, n_buf, n_len);
+		skb_trim(skb, skb->len + diff);
+	} else if (diff <= skb_tailroom(skb)) {
+		skb_put(skb, diff);
+		memmove(o_buf + n_len, o_buf + o_len, o_left);
+		memcpy(o_buf, n_buf, n_len);
+	} else {
+		if (pskb_expand_head(skb, skb_headroom(skb), diff, pri))
+			return -ENOMEM;
+		skb_put(skb, diff);
+		memmove(skb->data + o_offset + n_len,
+			skb->data + o_offset + o_len, o_left);
+		skb_copy_to_linear_data_offset(skb, o_offset, n_buf, n_len);
+	}
 
-	INIT_LIST_HEAD(&ipvs->app_list);
-	proc_create("ip_vs_app", 0, net->proc_net, &ip_vs_app_fops);
+	/* must update the iph total length here */
+	ip_hdr(skb)->tot_len = htons(skb->len);
+
+	LeaveFunction(9);
+	return 0;
+}
+
+int __init ip_vs_app_init(void)
+{
+	/* we will replace it with proc_net_ipvs_create() soon */
+	proc_create("ip_vs_app", 0,init_net.proc_net, &ip_vs_app_fops);
 	return 0;
 }
 
-void __net_exit ip_vs_app_net_cleanup(struct net *net)
+void ip_vs_app_cleanup(void)
 {
-	unregister_ip_vs_app(net, NULL /* all */);
-	remove_proc_entry("ip_vs_app", net->proc_net);
+	remove_proc_entry("ip_vs_app", init_net.proc_net);
 }
diff --git a/net/netfilter/ipvs/ip_vs_conn.c b/net/netfilter/ipvs/ip_vs_conn.c
index e884d17..73787a8 100644
--- a/net/netfilter/ipvs/ip_vs_conn.c
+++ b/net/netfilter/ipvs/ip_vs_conn.c
@@ -31,8 +31,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/vmalloc.h>
-#include <linux/proc_fs.h>		/* for proc_net_* */
-#include <linux/slab.h>
+#include <linux/proc_fs.h>	/* for proc_net_* */
 #include <linux/seq_file.h>
 #include <linux/jhash.h>
 #include <linux/random.h>
@@ -40,140 +39,162 @@
 #include <net/net_namespace.h>
 #include <net/ip_vs.h>
 
-
-#ifndef CONFIG_IP_VS_TAB_BITS
-#define CONFIG_IP_VS_TAB_BITS	12
-#endif
-
-/*
- * Connection hash size. Default is what was selected at compile time.
-*/
-static int ip_vs_conn_tab_bits = CONFIG_IP_VS_TAB_BITS;
-module_param_named(conn_tab_bits, ip_vs_conn_tab_bits, int, 0444);
-MODULE_PARM_DESC(conn_tab_bits, "Set connections' hash size");
-
-/* size and mask values */
-int ip_vs_conn_tab_size __read_mostly;
-static int ip_vs_conn_tab_mask __read_mostly;
-
 /*
  *  Connection hash table: for input and output packets lookups of IPVS
  */
-static struct hlist_head *ip_vs_conn_tab __read_mostly;
+static struct list_head *ip_vs_conn_tab;
 
 /*  SLAB cache for IPVS connections */
 static struct kmem_cache *ip_vs_conn_cachep __read_mostly;
 
+/*  counter for current IPVS connections */
+static atomic_t ip_vs_conn_count = ATOMIC_INIT(0);
+
 /*  counter for no client port connections */
 static atomic_t ip_vs_conn_no_cport_cnt = ATOMIC_INIT(0);
 
 /* random value for IPVS connection hash */
-static unsigned int ip_vs_conn_rnd __read_mostly;
+static unsigned int ip_vs_conn_rnd;
 
 /*
  *  Fine locking granularity for big connection hash table
  */
-#define CT_LOCKARRAY_BITS  5
+#define CT_LOCKARRAY_BITS  8
 #define CT_LOCKARRAY_SIZE  (1<<CT_LOCKARRAY_BITS)
 #define CT_LOCKARRAY_MASK  (CT_LOCKARRAY_SIZE-1)
 
-struct ip_vs_aligned_lock
-{
-	spinlock_t	l;
-} __attribute__((__aligned__(SMP_CACHE_BYTES)));
+struct ip_vs_aligned_lock {
+	rwlock_t l;
+} __attribute__ ((__aligned__(SMP_CACHE_BYTES)));
 
 /* lock array for conn table */
 static struct ip_vs_aligned_lock
-__ip_vs_conntbl_lock_array[CT_LOCKARRAY_SIZE] __cacheline_aligned;
+    __ip_vs_conntbl_lock_array[CT_LOCKARRAY_SIZE] __cacheline_aligned;
+
+static inline void ct_read_lock(unsigned key)
+{
+	read_lock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
+}
+
+static inline void ct_read_unlock(unsigned key)
+{
+	read_unlock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
+}
+
+static inline void ct_write_lock(unsigned key)
+{
+	write_lock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
+}
+
+static inline void ct_write_unlock(unsigned key)
+{
+	write_unlock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
+}
 
-static inline void ct_write_lock_bh(unsigned int key)
+static inline void ct_read_lock_bh(unsigned key)
 {
-	spin_lock_bh(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	read_lock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
-static inline void ct_write_unlock_bh(unsigned int key)
+static inline void ct_read_unlock_bh(unsigned key)
 {
-	spin_unlock_bh(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	read_unlock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
+static inline void ct_write_lock_bh(unsigned key)
+{
+	write_lock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
+}
+
+static inline void ct_write_unlock_bh(unsigned key)
+{
+	write_unlock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
+}
 
 /*
  *	Returns hash value for IPVS connection entry
  */
-static unsigned int ip_vs_conn_hashkey(struct net *net, int af, unsigned int proto,
-				       const union nf_inet_addr *addr,
-				       __be16 port)
+static unsigned int ip_vs_conn_hashkey(int af, const union nf_inet_addr *s_addr,
+				       __be16 s_port,
+				       const union nf_inet_addr *d_addr,
+				       __be16 d_port)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		return (jhash_3words(jhash(addr, 16, ip_vs_conn_rnd),
-				    (__force u32)port, proto, ip_vs_conn_rnd) ^
-			((size_t)net>>8)) & ip_vs_conn_tab_mask;
+		return jhash_3words(jhash(s_addr, 16, ip_vs_conn_rnd),
+				    jhash(d_addr, 16, ip_vs_conn_rnd),
+				    ((__force u32) s_port) << 16 | (__force u32)
+				    d_port, ip_vs_conn_rnd)
+		    & IP_VS_CONN_TAB_MASK;
 #endif
-	return (jhash_3words((__force u32)addr->ip, (__force u32)port, proto,
-			    ip_vs_conn_rnd) ^
-		((size_t)net>>8)) & ip_vs_conn_tab_mask;
+	return jhash_3words((__force u32) s_addr->ip, (__force u32) d_addr->ip,
+			    ((__force u32) s_port) << 16 | (__force u32) d_port,
+			    ip_vs_conn_rnd)
+	    & IP_VS_CONN_TAB_MASK;
 }
 
-static unsigned int ip_vs_conn_hashkey_param(const struct ip_vs_conn_param *p,
-					     bool inverse)
+/*
+ * Lock two buckets of ip_vs_conn_tab
+ */
+static inline void ip_vs_conn_lock2(unsigned ihash, unsigned ohash)
 {
-	const union nf_inet_addr *addr;
-	__be16 port;
-
-	if (p->pe_data && p->pe->hashkey_raw)
-		return p->pe->hashkey_raw(p, ip_vs_conn_rnd, inverse) &
-			ip_vs_conn_tab_mask;
-
-	if (likely(!inverse)) {
-		addr = p->caddr;
-		port = p->cport;
+	unsigned ilock, olock;
+
+	ilock = ihash & CT_LOCKARRAY_MASK;
+	olock = ohash & CT_LOCKARRAY_MASK;
+
+	/* lock the conntab bucket */
+	if (ilock < olock) {
+		ct_write_lock(ihash);
+		ct_write_lock(ohash);
+	} else if (ilock > olock) {
+		ct_write_lock(ohash);
+		ct_write_lock(ihash);
 	} else {
-		addr = p->vaddr;
-		port = p->vport;
+		ct_write_lock(ihash);
 	}
-
-	return ip_vs_conn_hashkey(p->net, p->af, p->protocol, addr, port);
 }
 
-static unsigned int ip_vs_conn_hashkey_conn(const struct ip_vs_conn *cp)
+/*
+ * Unlock two buckets of ip_vs_conn_tab
+ */
+static inline void ip_vs_conn_unlock2(unsigned ihash, unsigned ohash)
 {
-	struct ip_vs_conn_param p;
-
-	ip_vs_conn_fill_param(ip_vs_conn_net(cp), cp->af, cp->protocol,
-			      &cp->caddr, cp->cport, NULL, 0, &p);
-
-	if (cp->pe) {
-		p.pe = cp->pe;
-		p.pe_data = cp->pe_data;
-		p.pe_data_len = cp->pe_data_len;
+	unsigned ilock, olock;
+
+	ilock = ihash & CT_LOCKARRAY_MASK;
+	olock = ohash & CT_LOCKARRAY_MASK;
+
+	/* lock the conntab bucket */
+	if (ilock < olock) {
+		ct_write_unlock(ohash);
+		ct_write_unlock(ihash);
+	} else if (ilock > olock) {
+		ct_write_unlock(ihash);
+		ct_write_unlock(ohash);
+	} else {
+		ct_write_unlock(ihash);
 	}
-
-	return ip_vs_conn_hashkey_param(&p, false);
 }
 
 /*
- *	Hashes ip_vs_conn in ip_vs_conn_tab by netns,proto,addr,port.
+ *      Hashed ip_vs_conn into ip_vs_conn_tab
  *	returns bool success.
  */
-static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
+
+static inline int __ip_vs_conn_hash(struct ip_vs_conn *cp, unsigned ihash,
+				    unsigned ohash)
 {
-	unsigned int hash;
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
 	int ret;
 
-	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
-		return 0;
-
-	/* Hash by protocol, client address and port */
-	hash = ip_vs_conn_hashkey_conn(cp);
-
-	ct_write_lock_bh(hash);
-	spin_lock(&cp->lock);
-
 	if (!(cp->flags & IP_VS_CONN_F_HASHED)) {
+		ci_idx = cp->in_idx;
+		co_idx = cp->out_idx;
+		list_add(&ci_idx->c_list, &ip_vs_conn_tab[ihash]);
+		list_add(&co_idx->c_list, &ip_vs_conn_tab[ohash]);
 		cp->flags |= IP_VS_CONN_F_HASHED;
 		atomic_inc(&cp->refcnt);
-		hlist_add_head_rcu(&cp->c_list, &ip_vs_conn_tab[hash]);
 		ret = 1;
 	} else {
 		pr_err("%s(): request for already hashed, called from %pF\n",
@@ -181,301 +202,221 @@ static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
 		ret = 0;
 	}
 
-	spin_unlock(&cp->lock);
-	ct_write_unlock_bh(hash);
-
 	return ret;
 }
 
-
 /*
- *	UNhashes ip_vs_conn from ip_vs_conn_tab.
- *	returns bool success. Caller should hold conn reference.
+ *	Hashed ip_vs_conn in two buckets of ip_vs_conn_tab
+ *	by caddr/cport/vaddr/vport and raddr/rport/laddr/lport,
+ *	returns bool success.
  */
-static inline int ip_vs_conn_unhash(struct ip_vs_conn *cp)
+static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
 {
-	unsigned int hash;
+	unsigned ihash, ohash;
 	int ret;
 
-	/* unhash it and decrease its reference counter */
-	hash = ip_vs_conn_hashkey_conn(cp);
+	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
+		return 0;
 
-	ct_write_lock_bh(hash);
-	spin_lock(&cp->lock);
+	/*OUTside2INside: hashed by client address and port, virtual address and port */
+	ihash =
+	    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport, &cp->vaddr,
+			       cp->vport);
+	/*INside2OUTside: hashed by destination address and port, local address and port */
+	ohash =
+	    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport, &cp->laddr,
+			       cp->lport);
 
-	if (cp->flags & IP_VS_CONN_F_HASHED) {
-		hlist_del_rcu(&cp->c_list);
-		cp->flags &= ~IP_VS_CONN_F_HASHED;
-		atomic_dec(&cp->refcnt);
-		ret = 1;
-	} else
-		ret = 0;
+	/* locked */
+	ip_vs_conn_lock2(ihash, ohash);
 
-	spin_unlock(&cp->lock);
-	ct_write_unlock_bh(hash);
+	/* hashed */
+	ret = __ip_vs_conn_hash(cp, ihash, ohash);
+
+	/* unlocked */
+	ip_vs_conn_unlock2(ihash, ohash);
 
 	return ret;
 }
 
-/* Try to unlink ip_vs_conn from ip_vs_conn_tab.
- * returns bool success.
+/*
+ *	UNhashes ip_vs_conn from ip_vs_conn_tab.
+ *	cp->refcnt must be equal 2,
+ *	returns bool success.
  */
-static inline bool ip_vs_conn_unlink(struct ip_vs_conn *cp)
+static inline int ip_vs_conn_unhash(struct ip_vs_conn *cp)
 {
-	unsigned int hash;
-	bool ret;
-
-	hash = ip_vs_conn_hashkey_conn(cp);
-
-	ct_write_lock_bh(hash);
-	spin_lock(&cp->lock);
+	unsigned ihash, ohash;
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
+	int ret;
 
-	if (cp->flags & IP_VS_CONN_F_HASHED) {
-		ret = false;
-		/* Decrease refcnt and unlink conn only if we are last user */
-		if (atomic_cmpxchg(&cp->refcnt, 1, 0) == 1) {
-			hlist_del_rcu(&cp->c_list);
-			cp->flags &= ~IP_VS_CONN_F_HASHED;
-			ret = true;
-		}
-	} else
-		ret = atomic_read(&cp->refcnt) ? false : true;
+	/* OUTside2INside: unhash it and decrease its reference counter */
+	ihash =
+	    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport, &cp->vaddr,
+			       cp->vport);
+	/* INside2OUTside: unhash it and decrease its reference counter */
+	ohash =
+	    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport, &cp->laddr,
+			       cp->lport);
+
+	/* locked */
+	ip_vs_conn_lock2(ihash, ohash);
+
+	/* unhashed */
+	if ((cp->flags & IP_VS_CONN_F_HASHED)
+	    && (atomic_read(&cp->refcnt) == 2)) {
+		ci_idx = cp->in_idx;
+		co_idx = cp->out_idx;
+		list_del(&ci_idx->c_list);
+		list_del(&co_idx->c_list);
+		cp->flags &= ~IP_VS_CONN_F_HASHED;
+		atomic_dec(&cp->refcnt);
+		ret = 1;
+	} else {
+		ret = 0;
+	}
 
-	spin_unlock(&cp->lock);
-	ct_write_unlock_bh(hash);
+	/* unlocked */
+	ip_vs_conn_unlock2(ihash, ohash);
 
 	return ret;
 }
 
-
 /*
  *  Gets ip_vs_conn associated with supplied parameters in the ip_vs_conn_tab.
- *  Called for pkts coming from OUTside-to-INside.
- *	p->caddr, p->cport: pkt source address (foreign host)
- *	p->vaddr, p->vport: pkt dest address (load balancer)
+ *  Return director: OUTside-to-INside or INside-to-OUTside in res_dir.
+ *	s_addr, s_port: pkt source address (foreign host/realserver)
+ *	d_addr, d_port: pkt dest address (virtual address/local address)
  */
-static inline struct ip_vs_conn *
-__ip_vs_conn_in_get(const struct ip_vs_conn_param *p)
-{
-	unsigned int hash;
+static inline struct ip_vs_conn *__ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir) {
+	unsigned hash;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
-	hash = ip_vs_conn_hashkey_param(p, false);
+	hash = ip_vs_conn_hashkey(af, s_addr, s_port, d_addr, d_port);
 
-	rcu_read_lock();
+	ct_read_lock(hash);
 
-	hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (p->cport == cp->cport && p->vport == cp->vport &&
-		    cp->af == p->af &&
-		    ip_vs_addr_equal(p->af, p->caddr, &cp->caddr) &&
-		    ip_vs_addr_equal(p->af, p->vaddr, &cp->vaddr) &&
-		    ((!p->cport) ^ (!(cp->flags & IP_VS_CONN_F_NO_CPORT))) &&
-		    p->protocol == cp->protocol &&
-		    ip_vs_conn_net_eq(cp, p->net)) {
-			if (!__ip_vs_conn_get(cp))
-				continue;
+	list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+		cp = cidx->cp;
+		if (cidx->af == af &&
+		    ip_vs_addr_equal(af, s_addr, &cidx->s_addr) &&
+		    ip_vs_addr_equal(af, d_addr, &cidx->d_addr) &&
+		    s_port == cidx->s_port && d_port == cidx->d_port &&
+		    ((!s_port) ^ (!(cp->flags & IP_VS_CONN_F_NO_CPORT))) &&
+		    protocol == cidx->protocol) {
 			/* HIT */
-			rcu_read_unlock();
+			atomic_inc(&cp->refcnt);
+			*res_dir = cidx->flags & IP_VS_CIDX_F_DIR_MASK;
+			ct_read_unlock(hash);
 			return cp;
 		}
 	}
 
-	rcu_read_unlock();
+	ct_read_unlock(hash);
 
 	return NULL;
 }
 
-struct ip_vs_conn *ip_vs_conn_in_get(const struct ip_vs_conn_param *p)
-{
+struct ip_vs_conn *ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir) {
 	struct ip_vs_conn *cp;
 
-	cp = __ip_vs_conn_in_get(p);
-	if (!cp && atomic_read(&ip_vs_conn_no_cport_cnt)) {
-		struct ip_vs_conn_param cport_zero_p = *p;
-		cport_zero_p.cport = 0;
-		cp = __ip_vs_conn_in_get(&cport_zero_p);
-	}
+	cp = __ip_vs_conn_get(af, protocol, s_addr, s_port, d_addr, d_port,
+			      res_dir);
+	if (!cp && atomic_read(&ip_vs_conn_no_cport_cnt))
+		cp = __ip_vs_conn_get(af, protocol, s_addr, 0, d_addr, d_port,
+				      res_dir);
 
-	IP_VS_DBG_BUF(9, "lookup/in %s %s:%d->%s:%d %s\n",
-		      ip_vs_proto_name(p->protocol),
-		      IP_VS_DBG_ADDR(p->af, p->caddr), ntohs(p->cport),
-		      IP_VS_DBG_ADDR(p->af, p->vaddr), ntohs(p->vport),
+	IP_VS_DBG_BUF(9, "lookup %s %s:%d->%s:%d %s\n",
+		      ip_vs_proto_name(protocol),
+		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
+		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
 		      cp ? "hit" : "not hit");
 
 	return cp;
 }
 
-static int
-ip_vs_conn_fill_param_proto(int af, const struct sk_buff *skb,
-			    const struct ip_vs_iphdr *iph,
-			    int inverse, struct ip_vs_conn_param *p)
-{
-	__be16 _ports[2], *pptr;
-	struct net *net = skb_net(skb);
-
-	pptr = frag_safe_skb_hp(skb, iph->len, sizeof(_ports), _ports, iph);
-	if (pptr == NULL)
-		return 1;
-
-	if (likely(!inverse))
-		ip_vs_conn_fill_param(net, af, iph->protocol, &iph->saddr,
-				      pptr[0], &iph->daddr, pptr[1], p);
-	else
-		ip_vs_conn_fill_param(net, af, iph->protocol, &iph->daddr,
-				      pptr[1], &iph->saddr, pptr[0], p);
-	return 0;
-}
-
-struct ip_vs_conn *
-ip_vs_conn_in_get_proto(int af, const struct sk_buff *skb,
-			const struct ip_vs_iphdr *iph, int inverse)
-{
-	struct ip_vs_conn_param p;
-
-	if (ip_vs_conn_fill_param_proto(af, skb, iph, inverse, &p))
-		return NULL;
-
-	return ip_vs_conn_in_get(&p);
-}
-EXPORT_SYMBOL_GPL(ip_vs_conn_in_get_proto);
-
 /* Get reference to connection template */
-struct ip_vs_conn *ip_vs_ct_in_get(const struct ip_vs_conn_param *p)
-{
-	unsigned int hash;
+struct ip_vs_conn *ip_vs_ct_in_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port) {
+	unsigned hash;
+	struct ip_vs_conn_idx *cidx;
 	struct ip_vs_conn *cp;
 
-	hash = ip_vs_conn_hashkey_param(p, false);
-
-	rcu_read_lock();
+	hash = ip_vs_conn_hashkey(af, s_addr, s_port, d_addr, d_port);
 
-	hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (unlikely(p->pe_data && p->pe->ct_match)) {
-			if (!ip_vs_conn_net_eq(cp, p->net))
-				continue;
-			if (p->pe == cp->pe && p->pe->ct_match(p, cp)) {
-				if (__ip_vs_conn_get(cp))
-					goto out;
-			}
-			continue;
-		}
+	ct_read_lock(hash);
 
-		if (cp->af == p->af &&
-		    ip_vs_addr_equal(p->af, p->caddr, &cp->caddr) &&
+	list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+		cp = cidx->cp;
+		if (cidx->af == af &&
+		    ip_vs_addr_equal(af, s_addr, &cidx->s_addr) &&
 		    /* protocol should only be IPPROTO_IP if
-		     * p->vaddr is a fwmark */
-		    ip_vs_addr_equal(p->protocol == IPPROTO_IP ? AF_UNSPEC :
-				     p->af, p->vaddr, &cp->vaddr) &&
-		    p->vport == cp->vport && p->cport == cp->cport &&
+		     * d_addr is a fwmark */
+		    ip_vs_addr_equal(protocol == IPPROTO_IP ? AF_UNSPEC : af,
+				     d_addr, &cidx->d_addr) &&
+		    s_port == cidx->s_port && d_port == cidx->d_port &&
 		    cp->flags & IP_VS_CONN_F_TEMPLATE &&
-		    p->protocol == cp->protocol &&
-		    ip_vs_conn_net_eq(cp, p->net)) {
-			if (__ip_vs_conn_get(cp))
-				goto out;
+		    protocol == cidx->protocol) {
+			/* HIT */
+			atomic_inc(&cp->refcnt);
+			goto out;
 		}
 	}
 	cp = NULL;
 
-  out:
-	rcu_read_unlock();
+      out:
+	ct_read_unlock(hash);
 
-	IP_VS_DBG_BUF(9, "template lookup/in %s %s:%d->%s:%d %s\n",
-		      ip_vs_proto_name(p->protocol),
-		      IP_VS_DBG_ADDR(p->af, p->caddr), ntohs(p->cport),
-		      IP_VS_DBG_ADDR(p->af, p->vaddr), ntohs(p->vport),
+	IP_VS_DBG_BUF(9, "template lookup %s %s:%d->%s:%d %s\n",
+		      ip_vs_proto_name(protocol),
+		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
+		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
 		      cp ? "hit" : "not hit");
 
 	return cp;
 }
 
-/* Gets ip_vs_conn associated with supplied parameters in the ip_vs_conn_tab.
- * Called for pkts coming from inside-to-OUTside.
- *	p->caddr, p->cport: pkt source address (inside host)
- *	p->vaddr, p->vport: pkt dest address (foreign host) */
-struct ip_vs_conn *ip_vs_conn_out_get(const struct ip_vs_conn_param *p)
-{
-	unsigned int hash;
-	struct ip_vs_conn *cp, *ret=NULL;
-
-	/*
-	 *	Check for "full" addressed entries
-	 */
-	hash = ip_vs_conn_hashkey_param(p, true);
-
-	rcu_read_lock();
-
-	hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (p->vport == cp->cport && p->cport == cp->dport &&
-		    cp->af == p->af &&
-		    ip_vs_addr_equal(p->af, p->vaddr, &cp->caddr) &&
-		    ip_vs_addr_equal(p->af, p->caddr, &cp->daddr) &&
-		    p->protocol == cp->protocol &&
-		    ip_vs_conn_net_eq(cp, p->net)) {
-			if (!__ip_vs_conn_get(cp))
-				continue;
-			/* HIT */
-			ret = cp;
-			break;
-		}
-	}
-
-	rcu_read_unlock();
-
-	IP_VS_DBG_BUF(9, "lookup/out %s %s:%d->%s:%d %s\n",
-		      ip_vs_proto_name(p->protocol),
-		      IP_VS_DBG_ADDR(p->af, p->caddr), ntohs(p->cport),
-		      IP_VS_DBG_ADDR(p->af, p->vaddr), ntohs(p->vport),
-		      ret ? "hit" : "not hit");
-
-	return ret;
-}
-
-struct ip_vs_conn *
-ip_vs_conn_out_get_proto(int af, const struct sk_buff *skb,
-			 const struct ip_vs_iphdr *iph, int inverse)
-{
-	struct ip_vs_conn_param p;
-
-	if (ip_vs_conn_fill_param_proto(af, skb, iph, inverse, &p))
-		return NULL;
-
-	return ip_vs_conn_out_get(&p);
-}
-EXPORT_SYMBOL_GPL(ip_vs_conn_out_get_proto);
-
 /*
  *      Put back the conn and restart its timer with its timeout
  */
 void ip_vs_conn_put(struct ip_vs_conn *cp)
 {
-	unsigned long t = (cp->flags & IP_VS_CONN_F_ONE_PACKET) ?
-		0 : cp->timeout;
-	mod_timer(&cp->timer, jiffies+t);
+	unsigned long timeout = cp->timeout;
+
+	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
+		timeout = 0;
+
+	/* reset it expire in its timeout */
+	mod_timer(&cp->timer, jiffies + timeout);
 
 	__ip_vs_conn_put(cp);
 }
 
-
 /*
  *	Fill a no_client_port connection with a client port number
  */
 void ip_vs_conn_fill_cport(struct ip_vs_conn *cp, __be16 cport)
 {
 	if (ip_vs_conn_unhash(cp)) {
-		spin_lock_bh(&cp->lock);
+		spin_lock(&cp->lock);
 		if (cp->flags & IP_VS_CONN_F_NO_CPORT) {
 			atomic_dec(&ip_vs_conn_no_cport_cnt);
 			cp->flags &= ~IP_VS_CONN_F_NO_CPORT;
 			cp->cport = cport;
 		}
-		spin_unlock_bh(&cp->lock);
+		spin_unlock(&cp->lock);
 
 		/* hash on new dport */
 		ip_vs_conn_hash(cp);
 	}
 }
 
-
 /*
  *	Bind a connection entry with the corresponding packet_xmit.
  *	Called by ip_vs_conn_new.
@@ -487,6 +428,10 @@ static inline void ip_vs_bind_xmit(struct ip_vs_conn *cp)
 		cp->packet_xmit = ip_vs_nat_xmit;
 		break;
 
+	case IP_VS_CONN_F_FULLNAT:
+		cp->packet_xmit = ip_vs_fnat_xmit;
+		break;
+
 	case IP_VS_CONN_F_TUNNEL:
 		cp->packet_xmit = ip_vs_tunnel_xmit;
 		break;
@@ -513,6 +458,10 @@ static inline void ip_vs_bind_xmit_v6(struct ip_vs_conn *cp)
 		cp->packet_xmit = ip_vs_nat_xmit_v6;
 		break;
 
+	case IP_VS_CONN_F_FULLNAT:
+		cp->packet_xmit = ip_vs_fnat_xmit_v6;
+		break;
+
 	case IP_VS_CONN_F_TUNNEL:
 		cp->packet_xmit = ip_vs_tunnel_xmit_v6;
 		break;
@@ -532,11 +481,10 @@ static inline void ip_vs_bind_xmit_v6(struct ip_vs_conn *cp)
 }
 #endif
 
-
 static inline int ip_vs_dest_totalconns(struct ip_vs_dest *dest)
 {
 	return atomic_read(&dest->activeconns)
-		+ atomic_read(&dest->inactconns);
+	    + atomic_read(&dest->inactconns);
 }
 
 /*
@@ -546,32 +494,23 @@ static inline int ip_vs_dest_totalconns(struct ip_vs_dest *dest)
 static inline void
 ip_vs_bind_dest(struct ip_vs_conn *cp, struct ip_vs_dest *dest)
 {
-	unsigned int conn_flags;
-	__u32 flags;
-
 	/* if dest is NULL, then return directly */
 	if (!dest)
 		return;
 
 	/* Increase the refcnt counter of the dest */
-	ip_vs_dest_hold(dest);
+	atomic_inc(&dest->refcnt);
 
-	conn_flags = atomic_read(&dest->conn_flags);
-	if (cp->protocol != IPPROTO_UDP)
-		conn_flags &= ~IP_VS_CONN_F_ONE_PACKET;
-	flags = cp->flags;
 	/* Bind with the destination and its corresponding transmitter */
-	if (flags & IP_VS_CONN_F_SYNC) {
+	if ((cp->flags & IP_VS_CONN_F_SYNC) &&
+	    (!(cp->flags & IP_VS_CONN_F_TEMPLATE)))
 		/* if the connection is not template and is created
 		 * by sync, preserve the activity flag.
 		 */
-		if (!(flags & IP_VS_CONN_F_TEMPLATE))
-			conn_flags &= ~IP_VS_CONN_F_INACTIVE;
-		/* connections inherit forwarding method from dest */
-		flags &= ~(IP_VS_CONN_F_FWD_MASK | IP_VS_CONN_F_NOOUTPUT);
-	}
-	flags |= conn_flags;
-	cp->flags = flags;
+		cp->flags |= atomic_read(&dest->conn_flags) &
+		    (~IP_VS_CONN_F_INACTIVE);
+	else
+		cp->flags |= atomic_read(&dest->conn_flags);
 	cp->dest = dest;
 
 	IP_VS_DBG_BUF(7, "Bind-dest %s c:%s:%d v:%s:%d "
@@ -586,18 +525,18 @@ ip_vs_bind_dest(struct ip_vs_conn *cp, struct ip_vs_dest *dest)
 		      atomic_read(&dest->refcnt));
 
 	/* Update the connection counters */
-	if (!(flags & IP_VS_CONN_F_TEMPLATE)) {
-		/* It is a normal connection, so modify the counters
-		 * according to the flags, later the protocol can
-		 * update them on state change
-		 */
-		if (!(flags & IP_VS_CONN_F_INACTIVE))
+	if (!(cp->flags & IP_VS_CONN_F_TEMPLATE)) {
+		/* It is a normal connection, so increase the inactive
+		   connection counter because it is in TCP SYNRECV
+		   state (inactive) or other protocol inacive state */
+		if ((cp->flags & IP_VS_CONN_F_SYNC) &&
+		    (!(cp->flags & IP_VS_CONN_F_INACTIVE)))
 			atomic_inc(&dest->activeconns);
 		else
 			atomic_inc(&dest->inactconns);
 	} else {
 		/* It is a persistent connection/template, so increase
-		   the persistent connection counter */
+		   the peristent connection counter */
 		atomic_inc(&dest->persistconns);
 	}
 
@@ -606,54 +545,23 @@ ip_vs_bind_dest(struct ip_vs_conn *cp, struct ip_vs_dest *dest)
 		dest->flags |= IP_VS_DEST_F_OVERLOAD;
 }
 
-
 /*
  * Check if there is a destination for the connection, if so
  * bind the connection to the destination.
  */
-void ip_vs_try_bind_dest(struct ip_vs_conn *cp)
+struct ip_vs_dest *ip_vs_try_bind_dest(struct ip_vs_conn *cp)
 {
 	struct ip_vs_dest *dest;
 
-	rcu_read_lock();
-	dest = ip_vs_find_dest(ip_vs_conn_net(cp), cp->af, &cp->daddr,
-			       cp->dport, &cp->vaddr, cp->vport,
-			       cp->protocol, cp->fwmark, cp->flags);
-	if (dest) {
-		struct ip_vs_proto_data *pd;
-
-		spin_lock_bh(&cp->lock);
-		if (cp->dest) {
-			spin_unlock_bh(&cp->lock);
-			rcu_read_unlock();
-			return;
-		}
-
-		/* Applications work depending on the forwarding method
-		 * but better to reassign them always when binding dest */
-		if (cp->app)
-			ip_vs_unbind_app(cp);
-
+	if ((cp) && (!cp->dest)) {
+		dest = ip_vs_find_dest(cp->af, &cp->daddr, cp->dport,
+				       &cp->vaddr, cp->vport, cp->protocol);
 		ip_vs_bind_dest(cp, dest);
-		spin_unlock_bh(&cp->lock);
-
-		/* Update its packet transmitter */
-		cp->packet_xmit = NULL;
-#ifdef CONFIG_IP_VS_IPV6
-		if (cp->af == AF_INET6)
-			ip_vs_bind_xmit_v6(cp);
-		else
-#endif
-			ip_vs_bind_xmit(cp);
-
-		pd = ip_vs_proto_data_get(ip_vs_conn_net(cp), cp->protocol);
-		if (pd && atomic_read(&pd->appcnt))
-			ip_vs_bind_app(cp, pd->pp);
-	}
-	rcu_read_unlock();
+		return dest;
+	} else
+		return NULL;
 }
 
-
 /*
  *	Unbind a connection entry with its VS destination
  *	Called by the ip_vs_conn_expire function.
@@ -687,7 +595,7 @@ static inline void ip_vs_unbind_dest(struct ip_vs_conn *cp)
 		}
 	} else {
 		/* It is a persistent connection/template, so decrease
-		   the persistent connection counter */
+		   the peristent connection counter */
 		atomic_dec(&dest->persistconns);
 	}
 
@@ -702,18 +610,189 @@ static inline void ip_vs_unbind_dest(struct ip_vs_conn *cp)
 			dest->flags &= ~IP_VS_DEST_F_OVERLOAD;
 	}
 
-	ip_vs_dest_put(dest);
+	/*
+	 * Simply decrease the refcnt of the dest, because the
+	 * dest will be either in service's destination list
+	 * or in the trash.
+	 */
+	atomic_dec(&dest->refcnt);
 }
 
-static int expire_quiescent_template(struct netns_ipvs *ipvs,
-				     struct ip_vs_dest *dest)
+/*
+ * get a local address from given virtual service
+ */
+static struct ip_vs_laddr *ip_vs_get_laddr(struct ip_vs_service *svc)
 {
-#ifdef CONFIG_SYSCTL
-	return ipvs->sysctl_expire_quiescent_template &&
-		(atomic_read(&dest->weight) == 0);
-#else
-	return 0;
-#endif
+	struct ip_vs_laddr *local;
+	struct list_head *p, *q;
+
+	write_lock(&svc->laddr_lock);
+	p = svc->curr_laddr;
+	p = p->next;
+	q = p;
+	do {
+		/* skip list head */
+		if (q == &svc->laddr_list) {
+			q = q->next;
+			continue;
+		}
+		local = list_entry(q, struct ip_vs_laddr, n_list);
+		goto out;
+	} while (q != p);
+	write_unlock(&svc->laddr_lock);
+	return NULL;
+
+      out:
+	svc->curr_laddr = q;
+	write_unlock(&svc->laddr_lock);
+	return local;
+}
+
+/*
+ *	Bind a connection entry with a local address
+ *	and hashed it in connection table.
+ *	Called just after a new connection entry is created and destination has binded.
+ *	returns bool success.
+ */
+static inline int ip_vs_hbind_laddr(struct ip_vs_conn *cp)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	struct ip_vs_service *svc = dest->svc;
+	struct ip_vs_laddr *local;
+	int ret = 0;
+	int remaining, i, tport, hit = 0;
+	unsigned ihash, ohash;
+	struct ip_vs_conn_idx *cidx;
+
+	/* fwd methods: not IP_VS_CONN_F_FULLNAT */
+	switch (IP_VS_FWD_METHOD(cp)) {
+	case IP_VS_CONN_F_MASQ:
+	case IP_VS_CONN_F_TUNNEL:
+	case IP_VS_CONN_F_DROUTE:
+	case IP_VS_CONN_F_LOCALNODE:
+	case IP_VS_CONN_F_BYPASS:
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &cp->caddr);
+		cp->out_idx->d_port = cp->cport;
+		ip_vs_addr_copy(cp->af, &cp->laddr, &cp->caddr);
+		cp->lport = cp->cport;
+		cp->local = NULL;
+		ip_vs_conn_hash(cp);
+		ret = 1;
+		goto out;
+	}
+
+	if (cp->flags & IP_VS_CONN_F_TEMPLATE) {
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &cp->caddr);
+		cp->out_idx->d_port = cp->cport;
+		ip_vs_addr_copy(cp->af, &cp->laddr, &cp->caddr);
+		cp->lport = cp->cport;
+		cp->local = NULL;
+		ip_vs_conn_hash(cp);
+		ret = 1;
+		goto out;
+	}
+	/*
+	 * fwd methods: IP_VS_CONN_F_FULLNAT
+	 */
+	/* choose a local address by round-robin */
+	local = ip_vs_get_laddr(svc);
+	if (local != NULL) {
+		/*OUTside2INside: hashed by client address and port, virtual address and port */
+		ihash =
+		    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport,
+				       &cp->vaddr, cp->vport);
+
+		/* increase the refcnt counter of the local address */
+		ip_vs_laddr_hold(local);
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &local->addr);
+		ip_vs_addr_copy(cp->af, &cp->laddr, &local->addr);
+		remaining = sysctl_ip_vs_lport_max - sysctl_ip_vs_lport_min + 1;
+		for (i = 0; i < sysctl_ip_vs_lport_tries; i++) {
+			/* choose a port */
+			tport =
+			    sysctl_ip_vs_lport_min +
+			    atomic64_inc_return(&local->port) % remaining;
+			cp->out_idx->d_port = cp->lport = htons(tport);
+
+			/* init hit everytime before lookup the tuple */
+			hit = 0;
+
+			/*INside2OUTside: hashed by destination address and port, local address and port */
+			ohash =
+			    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport,
+					       &cp->laddr, cp->lport);
+			/* lock the conntab bucket */
+			ip_vs_conn_lock2(ihash, ohash);
+			/*
+			 * check local address and port is valid by lookup connection table
+			 */
+			list_for_each_entry(cidx, &ip_vs_conn_tab[ohash],
+					    c_list) {
+				if (cidx->af == cp->af
+				    && ip_vs_addr_equal(cp->af, &cp->daddr,
+							&cidx->s_addr)
+				    && ip_vs_addr_equal(cp->af, &cp->laddr,
+							&cidx->d_addr)
+				    && cp->dport == cidx->s_port
+				    && cp->lport == cidx->d_port
+				    && cp->protocol == cidx->protocol) {
+					/* HIT */
+					atomic64_inc(&local->port_conflict);
+					hit = 1;
+					break;
+				}
+			}
+			if (hit == 0) {
+				cp->local = local;
+				/* hashed */
+				__ip_vs_conn_hash(cp, ihash, ohash);
+				ip_vs_conn_unlock2(ihash, ohash);
+				atomic_inc(&local->conn_counts);
+				ret = 1;
+				goto out;
+			}
+			ip_vs_conn_unlock2(ihash, ohash);
+		}
+		if (ret == 0) {
+			ip_vs_laddr_put(local);
+		}
+	}
+	ret = 0;
+
+      out:
+	return ret;
+}
+
+/*
+ *	Unbind a connection entry with its local address
+ *	Called by the ip_vs_conn_expire function.
+ */
+static inline void ip_vs_unbind_laddr(struct ip_vs_conn *cp)
+{
+	struct ip_vs_laddr *local = cp->local;
+
+	if (!local)
+		return;
+
+	IP_VS_DBG_BUF(7, "Unbind-laddr %s c:%s:%d v:%s:%d l:%s:%d "
+		      "d:%s:%d fwd:%c s:%u conn->flags:%X conn->refcnt:%d "
+		      "local->refcnt:%d\n",
+		      ip_vs_proto_name(cp->protocol),
+		      IP_VS_DBG_ADDR(cp->af, &cp->caddr), ntohs(cp->cport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->vaddr), ntohs(cp->vport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->laddr), ntohs(cp->lport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->daddr), ntohs(cp->dport),
+		      ip_vs_fwd_tag(cp), cp->state,
+		      cp->flags, atomic_read(&cp->refcnt),
+		      atomic_read(&local->refcnt));
+
+	/* Update the connection counters */
+	atomic_dec(&local->conn_counts);
+
+	/*
+	 * Simply decrease the refcnt of the local address;
+	 */
+	ip_vs_laddr_put(local);
 }
 
 /*
@@ -724,22 +803,24 @@ static int expire_quiescent_template(struct netns_ipvs *ipvs,
 int ip_vs_check_template(struct ip_vs_conn *ct)
 {
 	struct ip_vs_dest *dest = ct->dest;
-	struct netns_ipvs *ipvs = net_ipvs(ip_vs_conn_net(ct));
 
 	/*
 	 * Checking the dest server status.
 	 */
 	if ((dest == NULL) ||
 	    !(dest->flags & IP_VS_DEST_F_AVAILABLE) ||
-	    expire_quiescent_template(ipvs, dest)) {
+	    (sysctl_ip_vs_expire_quiescent_template &&
+	     (atomic_read(&dest->weight) == 0))) {
 		IP_VS_DBG_BUF(9, "check_template: dest not available for "
 			      "protocol %s s:%s:%d v:%s:%d "
-			      "-> d:%s:%d\n",
+			      "-> l:%s:%d d:%s:%d\n",
 			      ip_vs_proto_name(ct->protocol),
 			      IP_VS_DBG_ADDR(ct->af, &ct->caddr),
 			      ntohs(ct->cport),
 			      IP_VS_DBG_ADDR(ct->af, &ct->vaddr),
 			      ntohs(ct->vport),
+			      IP_VS_DBG_ADDR(ct->af, &ct->laddr),
+			      ntohs(ct->lport),
 			      IP_VS_DBG_ADDR(ct->af, &ct->daddr),
 			      ntohs(ct->dport));
 
@@ -750,6 +831,7 @@ int ip_vs_check_template(struct ip_vs_conn *ct)
 			if (ip_vs_conn_unhash(ct)) {
 				ct->dport = htons(0xffff);
 				ct->vport = htons(0xffff);
+				ct->lport = 0;
 				ct->cport = 0;
 				ip_vs_conn_hash(ct);
 			}
@@ -759,137 +841,212 @@ int ip_vs_check_template(struct ip_vs_conn *ct)
 		 * Simply decrease the refcnt of the template,
 		 * don't restart its timer.
 		 */
-		__ip_vs_conn_put(ct);
+		atomic_dec(&ct->refcnt);
 		return 0;
 	}
 	return 1;
 }
 
-static void ip_vs_conn_rcu_free(struct rcu_head *head)
+/* Warning: only be allowed call in ip_vs_conn_new */
+static void ip_vs_conn_del(struct ip_vs_conn *cp)
 {
-	struct ip_vs_conn *cp = container_of(head, struct ip_vs_conn,
-					     rcu_head);
+	if (cp == NULL)
+		return;
+
+	/* delete the timer if it is activated by other users */
+	if (timer_pending(&cp->timer))
+		del_timer(&cp->timer);
+
+	/* does anybody control me? */
+	if (cp->control)
+		ip_vs_control_del(cp);
+
+	if (unlikely(cp->app != NULL))
+		ip_vs_unbind_app(cp);
+	ip_vs_unbind_dest(cp);
+	ip_vs_unbind_laddr(cp);
+	if (cp->flags & IP_VS_CONN_F_NO_CPORT)
+		atomic_dec(&ip_vs_conn_no_cport_cnt);
+	atomic_dec(&ip_vs_conn_count);
 
-	ip_vs_pe_put(cp->pe);
-	kfree(cp->pe_data);
 	kmem_cache_free(ip_vs_conn_cachep, cp);
+	cp = NULL;
 }
 
 static void ip_vs_conn_expire(unsigned long data)
 {
 	struct ip_vs_conn *cp = (struct ip_vs_conn *)data;
-	struct net *net = ip_vs_conn_net(cp);
-	struct netns_ipvs *ipvs = net_ipvs(net);
+	struct sk_buff *tmp_skb = NULL;
+	struct ip_vs_protocol *pp = ip_vs_proto_get(cp->protocol);
+
+	/*
+	 * Set proper timeout.
+	 */
+	if ((pp != NULL) && (pp->timeout_table != NULL)) {
+		cp->timeout = pp->timeout_table[cp->state];
+	} else {
+		cp->timeout = 60 * HZ;
+	}
+
+	/*
+	 *      hey, I'm using it
+	 */
+	atomic_inc(&cp->refcnt);
+
+	/*
+	 * Retransmit syn packet to rs.
+	 * We just check syn_skb is not NULL, as syn_skb 
+	 * is stored only if syn-proxy is enabled.
+	 */
+	spin_lock(&cp->lock);
+	if (cp->syn_skb != NULL && atomic_read(&cp->syn_retry_max) > 0) {
+		atomic_dec(&cp->syn_retry_max);
+		if (cp->packet_xmit) {
+			tmp_skb = skb_copy(cp->syn_skb, GFP_ATOMIC);
+			cp->packet_xmit(tmp_skb, cp, pp);
+		}
+		/* statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_RS_ERROR);
+		spin_unlock(&cp->lock);
+		goto expire_later;
+	}
+	spin_unlock(&cp->lock);
 
 	/*
-	 *	do I control anybody?
+	 *      do I control anybody?
 	 */
 	if (atomic_read(&cp->n_control))
 		goto expire_later;
 
-	/* Unlink conn if not referenced anymore */
-	if (likely(ip_vs_conn_unlink(cp))) {
+	/*
+	 *      unhash it if it is hashed in the conn table
+	 */
+	if (!ip_vs_conn_unhash(cp) && !(cp->flags & IP_VS_CONN_F_ONE_PACKET))
+		goto expire_later;
+
+	/*
+	 *      refcnt==1 implies I'm the only one referrer
+	 */
+	if (likely(atomic_read(&cp->refcnt) == 1)) {
 		/* delete the timer if it is activated by other users */
-		del_timer(&cp->timer);
+		if (timer_pending(&cp->timer))
+			del_timer(&cp->timer);
 
 		/* does anybody control me? */
 		if (cp->control)
 			ip_vs_control_del(cp);
 
-		if (cp->flags & IP_VS_CONN_F_NFCT) {
-			ip_vs_conn_drop_conntrack(cp);
-			/* Do not access conntracks during subsys cleanup
-			 * because nf_conntrack_find_get can not be used after
-			 * conntrack cleanup for the net.
-			 */
-			smp_rmb();
-			if (ipvs->enable)
-				ip_vs_conn_drop_conntrack(cp);
-		}
+		if (pp->conn_expire_handler)
+			pp->conn_expire_handler(pp, cp);
 
 		if (unlikely(cp->app != NULL))
 			ip_vs_unbind_app(cp);
 		ip_vs_unbind_dest(cp);
+		ip_vs_unbind_laddr(cp);
 		if (cp->flags & IP_VS_CONN_F_NO_CPORT)
 			atomic_dec(&ip_vs_conn_no_cport_cnt);
-		call_rcu(&cp->rcu_head, ip_vs_conn_rcu_free);
-		atomic_dec(&ipvs->conn_count);
+		atomic_dec(&ip_vs_conn_count);
+
+		/* free stored ack packet */
+		while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		/* free stored syn skb */
+		if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		if (cp->indev != NULL)
+			dev_put(cp->indev);
+
+		kmem_cache_free(ip_vs_conn_cachep, cp);
 		return;
 	}
 
-  expire_later:
-	IP_VS_DBG(7, "delayed: conn->refcnt=%d conn->n_control=%d\n",
-		  atomic_read(&cp->refcnt),
-		  atomic_read(&cp->n_control));
-
-	atomic_inc(&cp->refcnt);
-	cp->timeout = 60*HZ;
+	/* hash it back to the table */
+	ip_vs_conn_hash(cp);
 
-	if (ipvs->sync_state & IP_VS_STATE_MASTER)
-		ip_vs_sync_conn(net, cp, sysctl_sync_threshold(ipvs));
+      expire_later:
+	IP_VS_DBG(7, "delayed: conn->refcnt-1=%d conn->n_control=%d\n",
+		  atomic_read(&cp->refcnt) - 1, atomic_read(&cp->n_control));
 
 	ip_vs_conn_put(cp);
 }
 
-/* Modify timer, so that it expires as soon as possible.
- * Can be called without reference only if under RCU lock.
- */
 void ip_vs_conn_expire_now(struct ip_vs_conn *cp)
 {
-	/* Using mod_timer_pending will ensure the timer is not
-	 * modified after the final del_timer in ip_vs_conn_expire.
-	 */
-	if (timer_pending(&cp->timer) &&
-	    time_after(cp->timer.expires, jiffies))
-		mod_timer_pending(&cp->timer, jiffies);
+	if (del_timer(&cp->timer))
+		mod_timer(&cp->timer, jiffies);
 }
 
-
 /*
  *	Create a new connection entry and hash it into the ip_vs_conn_tab
  */
-struct ip_vs_conn *
-ip_vs_conn_new(const struct ip_vs_conn_param *p,
-	       const union nf_inet_addr *daddr, __be16 dport, unsigned int flags,
-	       struct ip_vs_dest *dest, __u32 fwmark)
+struct ip_vs_conn *ip_vs_conn_new(int af, int proto,
+				  const union nf_inet_addr *caddr, __be16 cport,
+				  const union nf_inet_addr *vaddr, __be16 vport,
+				  const union nf_inet_addr *daddr, __be16 dport,
+				  unsigned flags, struct ip_vs_dest *dest,
+				  struct sk_buff *skb, int is_synproxy_on)
 {
 	struct ip_vs_conn *cp;
-	struct netns_ipvs *ipvs = net_ipvs(p->net);
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(p->net,
-							   p->protocol);
+	struct ip_vs_protocol *pp = ip_vs_proto_get(proto);
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
+	struct tcphdr _tcph, *th;
 
-	cp = kmem_cache_alloc(ip_vs_conn_cachep, GFP_ATOMIC);
+	cp = kmem_cache_zalloc(ip_vs_conn_cachep, GFP_ATOMIC);
 	if (cp == NULL) {
 		IP_VS_ERR_RL("%s(): no memory\n", __func__);
 		return NULL;
 	}
 
-	INIT_HLIST_NODE(&cp->c_list);
+	/* init connection index of OUTside2INside */
+	ci_idx =
+	    (struct ip_vs_conn_idx *)(((__u8 *) cp) +
+				      sizeof(struct ip_vs_conn));
+	INIT_LIST_HEAD(&ci_idx->c_list);
+	ci_idx->af = af;
+	ci_idx->protocol = proto;
+	ip_vs_addr_copy(af, &ci_idx->s_addr, caddr);
+	ci_idx->s_port = cport;
+	ip_vs_addr_copy(af, &ci_idx->d_addr, vaddr);
+	ci_idx->d_port = vport;
+	ci_idx->flags |= IP_VS_CIDX_F_OUT2IN;
+	ci_idx->cp = cp;
+
+	/* init connection index of INside2OUTside */
+	co_idx =
+	    (struct ip_vs_conn_idx *)(((__u8 *) cp) +
+				      sizeof(struct ip_vs_conn) +
+				      sizeof(struct ip_vs_conn_idx));
+	INIT_LIST_HEAD(&co_idx->c_list);
+	co_idx->af = af;
+	co_idx->protocol = proto;
+	ip_vs_addr_copy(proto == IPPROTO_IP ? AF_UNSPEC : af,
+			&co_idx->s_addr, daddr);
+	co_idx->s_port = dport;
+	co_idx->flags |= IP_VS_CIDX_F_IN2OUT;
+	co_idx->cp = cp;
+
+	/* now init connection */
 	setup_timer(&cp->timer, ip_vs_conn_expire, (unsigned long)cp);
-	ip_vs_conn_net_set(cp, p->net);
-	cp->af		   = p->af;
-	cp->protocol	   = p->protocol;
-	ip_vs_addr_set(p->af, &cp->caddr, p->caddr);
-	cp->cport	   = p->cport;
-	/* proto should only be IPPROTO_IP if p->vaddr is a fwmark */
-	ip_vs_addr_set(p->protocol == IPPROTO_IP ? AF_UNSPEC : p->af,
-		       &cp->vaddr, p->vaddr);
-	cp->vport	   = p->vport;
-	ip_vs_addr_set(p->af, &cp->daddr, daddr);
-	cp->dport          = dport;
-	cp->flags	   = flags;
-	cp->fwmark         = fwmark;
-	if (flags & IP_VS_CONN_F_TEMPLATE && p->pe) {
-		ip_vs_pe_get(p->pe);
-		cp->pe = p->pe;
-		cp->pe_data = p->pe_data;
-		cp->pe_data_len = p->pe_data_len;
-	} else {
-		cp->pe = NULL;
-		cp->pe_data = NULL;
-		cp->pe_data_len = 0;
-	}
+	cp->af = af;
+	cp->protocol = proto;
+	ip_vs_addr_copy(af, &cp->caddr, caddr);
+	cp->cport = cport;
+	ip_vs_addr_copy(af, &cp->vaddr, vaddr);
+	cp->vport = vport;
+	/* proto should only be IPPROTO_IP if d_addr is a fwmark */
+	ip_vs_addr_copy(proto == IPPROTO_IP ? AF_UNSPEC : af,
+			&cp->daddr, daddr);
+	cp->dport = dport;
+	cp->flags = flags;
 	spin_lock_init(&cp->lock);
+	cp->in_idx = ci_idx;
+	cp->out_idx = co_idx;
 
 	/*
 	 * Set the entry is referenced by the current thread before hashing
@@ -898,54 +1055,79 @@ ip_vs_conn_new(const struct ip_vs_conn_param *p,
 	 */
 	atomic_set(&cp->refcnt, 1);
 
-	cp->control = NULL;
 	atomic_set(&cp->n_control, 0);
 	atomic_set(&cp->in_pkts, 0);
 
-	cp->packet_xmit = NULL;
-	cp->app = NULL;
-	cp->app_data = NULL;
-	/* reset struct ip_vs_seq */
-	cp->in_seq.delta = 0;
-	cp->out_seq.delta = 0;
-
-	atomic_inc(&ipvs->conn_count);
+	atomic_inc(&ip_vs_conn_count);
 	if (flags & IP_VS_CONN_F_NO_CPORT)
 		atomic_inc(&ip_vs_conn_no_cport_cnt);
 
 	/* Bind the connection with a destination server */
-	cp->dest = NULL;
 	ip_vs_bind_dest(cp, dest);
 
 	/* Set its state and timeout */
 	cp->state = 0;
-	cp->old_state = 0;
-	cp->timeout = 3*HZ;
-	cp->sync_endtime = jiffies & ~3UL;
+	cp->timeout = 3 * HZ;
 
 	/* Bind its packet transmitter */
 #ifdef CONFIG_IP_VS_IPV6
-	if (p->af == AF_INET6)
+	if (af == AF_INET6)
 		ip_vs_bind_xmit_v6(cp);
 	else
 #endif
 		ip_vs_bind_xmit(cp);
 
-	if (unlikely(pd && atomic_read(&pd->appcnt)))
-		ip_vs_bind_app(cp, pd->pp);
+	if (unlikely(pp && atomic_read(&pp->appcnt)))
+		ip_vs_bind_app(cp, pp);
 
-	/*
-	 * Allow conntrack to be preserved. By default, conntrack
-	 * is created and destroyed for every packet.
-	 * Sometimes keeping conntrack can be useful for
-	 * IP_VS_CONN_F_ONE_PACKET too.
+	/* Set syn-proxy members 
+	 * Set cp->flag manually to avoid svn->flags change when 
+	 * ack_skb is on the way
 	 */
+	skb_queue_head_init(&cp->ack_skb);
+	atomic_set(&cp->syn_retry_max, 0);
+	if (is_synproxy_on == 1 && skb != NULL) {
+		unsigned int tcphoff;
 
-	if (ip_vs_conntrack_enabled(ipvs))
-		cp->flags |= IP_VS_CONN_F_NFCT;
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdr(skb)->ihl * 4;
+		th = skb_header_pointer(skb, tcphoff, sizeof(_tcph), &_tcph);
+		if (th == NULL) {
+			IP_VS_ERR_RL("%s(): get tcphdr failed\n", __func__);
+			ip_vs_conn_del(cp);
+			return NULL;
+		}
+		/* Set syn-proxy flag */
+		cp->flags |= IP_VS_CONN_F_SYNPROXY;
+
+		/* Save ack packet */
+		skb_queue_tail(&cp->ack_skb, skb);
+		/* Save ack_seq - 1 */
+		cp->syn_proxy_seq.init_seq =
+		    htonl((__u32) ((htonl(th->ack_seq) - 1)));
+		/* Save ack_seq */
+		cp->fnat_seq.fdata_seq = htonl(th->ack_seq);
+		/* Use IP_VS_TCP_S_SYN_SENT for syn */
+		cp->timeout = pp->timeout_table[cp->state =
+						IP_VS_TCP_S_SYN_SENT];
+	} else {
+		/* Unset syn-proxy flag */
+		cp->flags &= ~IP_VS_CONN_F_SYNPROXY;
+	}
 
-	/* Hash it in the ip_vs_conn_tab finally */
-	ip_vs_conn_hash(cp);
+	/*
+	 * bind the connection with a local address
+	 * and hash it in the ip_vs_conn_tab finally.
+	 */
+	if (unlikely(ip_vs_hbind_laddr(cp) == 0)) {
+		IP_VS_ERR_RL("bind local address: no port available\n");
+		ip_vs_conn_del(cp);
+		return NULL;
+	}
 
 	return cp;
 }
@@ -954,78 +1136,75 @@ ip_vs_conn_new(const struct ip_vs_conn_param *p,
  *	/proc/net/ip_vs_conn entries
  */
 #ifdef CONFIG_PROC_FS
-struct ip_vs_iter_state {
-	struct seq_net_private	p;
-	struct hlist_head	*l;
-};
 
 static void *ip_vs_conn_array(struct seq_file *seq, loff_t pos)
 {
 	int idx;
-	struct ip_vs_conn *cp;
-	struct ip_vs_iter_state *iter = seq->private;
-
-	for (idx = 0; idx < ip_vs_conn_tab_size; idx++) {
-		hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[idx], c_list) {
-			/* __ip_vs_conn_get() is not needed by
-			 * ip_vs_conn_seq_show and ip_vs_conn_sync_seq_show
-			 */
-			if (pos-- == 0) {
-				iter->l = &ip_vs_conn_tab[idx];
-				return cp;
+	struct ip_vs_conn_idx *cidx;
+
+	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
+		ct_read_lock_bh(idx);
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
+			if ((cidx->flags & IP_VS_CIDX_F_OUT2IN) && (pos-- == 0)) {
+				seq->private = &ip_vs_conn_tab[idx];
+				return cidx->cp;
 			}
 		}
-		rcu_read_unlock();
-		rcu_read_lock();
+		ct_read_unlock_bh(idx);
 	}
 
 	return NULL;
 }
 
-static void *ip_vs_conn_seq_start(struct seq_file *seq, loff_t *pos)
-	__acquires(RCU)
+static void *ip_vs_conn_seq_start(struct seq_file *seq, loff_t * pos)
 {
-	struct ip_vs_iter_state *iter = seq->private;
-
-	iter->l = NULL;
-	rcu_read_lock();
-	return *pos ? ip_vs_conn_array(seq, *pos - 1) :SEQ_START_TOKEN;
+	seq->private = NULL;
+	return *pos ? ip_vs_conn_array(seq, *pos - 1) : SEQ_START_TOKEN;
 }
 
-static void *ip_vs_conn_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+static void *ip_vs_conn_seq_next(struct seq_file *seq, void *v, loff_t * pos)
 {
 	struct ip_vs_conn *cp = v;
-	struct ip_vs_iter_state *iter = seq->private;
-	struct hlist_node *e;
-	struct hlist_head *l = iter->l;
+	struct list_head *e, *l = seq->private;
+	struct ip_vs_conn_idx *cidx;
 	int idx;
 
 	++*pos;
 	if (v == SEQ_START_TOKEN)
 		return ip_vs_conn_array(seq, 0);
 
+	cidx = cp->in_idx;
 	/* more on same hash chain? */
-	e = rcu_dereference(hlist_next_rcu(&cp->c_list));
-	if (e)
-		return hlist_entry(e, struct ip_vs_conn, c_list);
+	while ((e = cidx->c_list.next) != l) {
+		cidx = list_entry(e, struct ip_vs_conn_idx, c_list);
+		if (cidx->flags & IP_VS_CIDX_F_OUT2IN) {
+			return cidx->cp;
+		}
+	}
 
 	idx = l - ip_vs_conn_tab;
-	while (++idx < ip_vs_conn_tab_size) {
-		hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[idx], c_list) {
-			iter->l = &ip_vs_conn_tab[idx];
-			return cp;
+	ct_read_unlock_bh(idx);
+
+	while (++idx < IP_VS_CONN_TAB_SIZE) {
+		ct_read_lock_bh(idx);
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
+			if (cidx->flags & IP_VS_CIDX_F_OUT2IN) {
+				seq->private = &ip_vs_conn_tab[idx];
+				return cidx->cp;
+			}
 		}
-		rcu_read_unlock();
-		rcu_read_lock();
+		ct_read_unlock_bh(idx);
 	}
-	iter->l = NULL;
+	seq->private = NULL;
 	return NULL;
 }
 
 static void ip_vs_conn_seq_stop(struct seq_file *seq, void *v)
-	__releases(RCU)
 {
-	rcu_read_unlock();
+	struct list_head *l = seq->private;
+
+	if (l)
+		ct_read_unlock_bh(l - ip_vs_conn_tab);
 }
 
 static int ip_vs_conn_seq_show(struct seq_file *seq, void *v)
@@ -1033,72 +1212,58 @@ static int ip_vs_conn_seq_show(struct seq_file *seq, void *v)
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq,
-   "Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Expires PEName PEData\n");
+			 "Pro FromIP   FPrt ToIP     TPrt LocalIP  LPrt DestIP   DPrt State       Expires\n");
 	else {
 		const struct ip_vs_conn *cp = v;
-		struct net *net = seq_file_net(seq);
-		char pe_data[IP_VS_PENAME_MAXLEN + IP_VS_PEDATA_MAXLEN + 3];
-		size_t len = 0;
-
-		if (!ip_vs_conn_net_eq(cp, net))
-			return 0;
-		if (cp->pe_data) {
-			pe_data[0] = ' ';
-			len = strlen(cp->pe->name);
-			memcpy(pe_data + 1, cp->pe->name, len);
-			pe_data[len + 1] = ' ';
-			len += 2;
-			len += cp->pe->show_pe_data(cp, pe_data + len);
-		}
-		pe_data[len] = '\0';
 
 #ifdef CONFIG_IP_VS_IPV6
 		if (cp->af == AF_INET6)
-			seq_printf(seq, "%-3s %pI6 %04X %pI6 %04X "
-				"%pI6 %04X %-11s %7lu%s\n",
-				ip_vs_proto_name(cp->protocol),
-				&cp->caddr.in6, ntohs(cp->cport),
-				&cp->vaddr.in6, ntohs(cp->vport),
-				&cp->daddr.in6, ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				(cp->timer.expires-jiffies)/HZ, pe_data);
+			seq_printf(seq,
+				   "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %pI6 %04X %-11s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   &cp->caddr.in6, ntohs(cp->cport),
+				   &cp->vaddr.in6, ntohs(cp->vport),
+				   &cp->laddr.in6, ntohs(cp->lport),
+				   &cp->daddr.in6, ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   (cp->timer.expires - jiffies) / HZ);
 		else
 #endif
 			seq_printf(seq,
-				"%-3s %08X %04X %08X %04X"
-				" %08X %04X %-11s %7lu%s\n",
-				ip_vs_proto_name(cp->protocol),
-				ntohl(cp->caddr.ip), ntohs(cp->cport),
-				ntohl(cp->vaddr.ip), ntohs(cp->vport),
-				ntohl(cp->daddr.ip), ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				(cp->timer.expires-jiffies)/HZ, pe_data);
+				   "%-3s %08X %04X %08X %04X"
+				   " %08X %04X %08X %04X %-11s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   ntohl(cp->caddr.ip), ntohs(cp->cport),
+				   ntohl(cp->vaddr.ip), ntohs(cp->vport),
+				   ntohl(cp->laddr.ip), ntohs(cp->lport),
+				   ntohl(cp->daddr.ip), ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   (cp->timer.expires - jiffies) / HZ);
 	}
 	return 0;
 }
 
 static const struct seq_operations ip_vs_conn_seq_ops = {
 	.start = ip_vs_conn_seq_start,
-	.next  = ip_vs_conn_seq_next,
-	.stop  = ip_vs_conn_seq_stop,
-	.show  = ip_vs_conn_seq_show,
+	.next = ip_vs_conn_seq_next,
+	.stop = ip_vs_conn_seq_stop,
+	.show = ip_vs_conn_seq_show,
 };
 
 static int ip_vs_conn_open(struct inode *inode, struct file *file)
 {
-	return seq_open_net(inode, file, &ip_vs_conn_seq_ops,
-			    sizeof(struct ip_vs_iter_state));
+	return seq_open(file, &ip_vs_conn_seq_ops);
 }
 
 static const struct file_operations ip_vs_conn_fops = {
-	.owner	 = THIS_MODULE,
-	.open    = ip_vs_conn_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
-	.release = seq_release_net,
+	.owner = THIS_MODULE,
+	.open = ip_vs_conn_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
 };
 
-static const char *ip_vs_origin_name(unsigned int flags)
+static const char *ip_vs_origin_name(unsigned flags)
 {
 	if (flags & IP_VS_CONN_F_SYNC)
 		return "SYNC";
@@ -1111,64 +1276,61 @@ static int ip_vs_conn_sync_seq_show(struct seq_file *seq, void *v)
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq,
-   "Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Origin Expires\n");
+			 "Pro FromIP   FPrt ToIP     TPrt LocalIP  LPrt DestIP   DPrt State       Origin Expires\n");
 	else {
 		const struct ip_vs_conn *cp = v;
-		struct net *net = seq_file_net(seq);
-
-		if (!ip_vs_conn_net_eq(cp, net))
-			return 0;
 
 #ifdef CONFIG_IP_VS_IPV6
 		if (cp->af == AF_INET6)
-			seq_printf(seq, "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %-11s %-6s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				&cp->caddr.in6, ntohs(cp->cport),
-				&cp->vaddr.in6, ntohs(cp->vport),
-				&cp->daddr.in6, ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				ip_vs_origin_name(cp->flags),
-				(cp->timer.expires-jiffies)/HZ);
+			seq_printf(seq,
+				   "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %pI6 %04X %-11s %-6s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   &cp->caddr.in6, ntohs(cp->cport),
+				   &cp->vaddr.in6, ntohs(cp->vport),
+				   &cp->laddr.in6, ntohs(cp->lport),
+				   &cp->daddr.in6, ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   ip_vs_origin_name(cp->flags),
+				   (cp->timer.expires - jiffies) / HZ);
 		else
 #endif
 			seq_printf(seq,
-				"%-3s %08X %04X %08X %04X "
-				"%08X %04X %-11s %-6s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				ntohl(cp->caddr.ip), ntohs(cp->cport),
-				ntohl(cp->vaddr.ip), ntohs(cp->vport),
-				ntohl(cp->daddr.ip), ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				ip_vs_origin_name(cp->flags),
-				(cp->timer.expires-jiffies)/HZ);
+				   "%-3s %08X %04X %08X %04X "
+				   "%08X %04X %08X %04X %-11s %-6s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   ntohl(cp->caddr.ip), ntohs(cp->cport),
+				   ntohl(cp->vaddr.ip), ntohs(cp->vport),
+				   ntohl(cp->laddr.ip), ntohs(cp->lport),
+				   ntohl(cp->daddr.ip), ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   ip_vs_origin_name(cp->flags),
+				   (cp->timer.expires - jiffies) / HZ);
 	}
 	return 0;
 }
 
 static const struct seq_operations ip_vs_conn_sync_seq_ops = {
 	.start = ip_vs_conn_seq_start,
-	.next  = ip_vs_conn_seq_next,
-	.stop  = ip_vs_conn_seq_stop,
-	.show  = ip_vs_conn_sync_seq_show,
+	.next = ip_vs_conn_seq_next,
+	.stop = ip_vs_conn_seq_stop,
+	.show = ip_vs_conn_sync_seq_show,
 };
 
 static int ip_vs_conn_sync_open(struct inode *inode, struct file *file)
 {
-	return seq_open_net(inode, file, &ip_vs_conn_sync_seq_ops,
-			    sizeof(struct ip_vs_iter_state));
+	return seq_open(file, &ip_vs_conn_sync_seq_ops);
 }
 
 static const struct file_operations ip_vs_conn_sync_fops = {
-	.owner	 = THIS_MODULE,
-	.open    = ip_vs_conn_sync_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
-	.release = seq_release_net,
+	.owner = THIS_MODULE,
+	.open = ip_vs_conn_sync_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
 };
 
 #endif
 
-
 /*
  *      Randomly drop connection entries before running out of memory
  */
@@ -1178,53 +1340,57 @@ static inline int todrop_entry(struct ip_vs_conn *cp)
 	 * The drop rate array needs tuning for real environments.
 	 * Called from timer bh only => no locking
 	 */
-	static const char todrop_rate[9] = {0, 1, 2, 3, 4, 5, 6, 7, 8};
-	static char todrop_counter[9] = {0};
+	static const char todrop_rate[9] = { 0, 1, 2, 3, 4, 5, 6, 7, 8 };
+	static char todrop_counter[9] = { 0 };
 	int i;
 
 	/* if the conn entry hasn't lasted for 60 seconds, don't drop it.
 	   This will leave enough time for normal connection to get
 	   through. */
-	if (time_before(cp->timeout + jiffies, cp->timer.expires + 60*HZ))
+	if (time_before(cp->timeout + jiffies, cp->timer.expires + 60 * HZ))
 		return 0;
 
 	/* Don't drop the entry if its number of incoming packets is not
 	   located in [0, 8] */
 	i = atomic_read(&cp->in_pkts);
-	if (i > 8 || i < 0) return 0;
+	if (i > 8 || i < 0)
+		return 0;
 
-	if (!todrop_rate[i]) return 0;
-	if (--todrop_counter[i] > 0) return 0;
+	if (!todrop_rate[i])
+		return 0;
+	if (--todrop_counter[i] > 0)
+		return 0;
 
 	todrop_counter[i] = todrop_rate[i];
 	return 1;
 }
 
 /* Called from keventd and must protect itself from softirqs */
-void ip_vs_random_dropentry(struct net *net)
+void ip_vs_random_dropentry(void)
 {
 	int idx;
-	struct ip_vs_conn *cp, *cp_c;
+	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
 	/*
 	 * Randomly scan 1/32 of the whole table every second
 	 */
-	for (idx = 0; idx < (ip_vs_conn_tab_size>>5); idx++) {
-		unsigned int hash = net_random() & ip_vs_conn_tab_mask;
+	for (idx = 0; idx < (IP_VS_CONN_TAB_SIZE >> 5); idx++) {
+		unsigned hash = net_random() & IP_VS_CONN_TAB_MASK;
 
 		/*
 		 *  Lock is actually needed in this loop.
 		 */
-		rcu_read_lock();
+		ct_write_lock_bh(hash);
 
-		hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[hash], c_list) {
+		list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+			cp = cidx->cp;
 			if (cp->flags & IP_VS_CONN_F_TEMPLATE)
 				/* connection template */
 				continue;
-			if (!ip_vs_conn_net_eq(cp, net))
-				continue;
+
 			if (cp->protocol == IPPROTO_TCP) {
-				switch(cp->state) {
+				switch (cp->state) {
 				case IP_VS_TCP_S_SYN_RECV:
 				case IP_VS_TCP_S_SYNACK:
 					break;
@@ -1244,99 +1410,68 @@ void ip_vs_random_dropentry(struct net *net)
 
 			IP_VS_DBG(4, "del connection\n");
 			ip_vs_conn_expire_now(cp);
-			cp_c = cp->control;
-			/* cp->control is valid only with reference to cp */
-			if (cp_c && __ip_vs_conn_get(cp)) {
+			if (cp->control) {
 				IP_VS_DBG(4, "del conn template\n");
-				ip_vs_conn_expire_now(cp_c);
-				__ip_vs_conn_put(cp);
+				ip_vs_conn_expire_now(cp->control);
 			}
 		}
-		rcu_read_unlock();
+		ct_write_unlock_bh(hash);
 	}
 }
 
-
 /*
  *      Flush all the connection entries in the ip_vs_conn_tab
  */
-static void ip_vs_conn_flush(struct net *net)
+static void ip_vs_conn_flush(void)
 {
 	int idx;
-	struct ip_vs_conn *cp, *cp_c;
-	struct netns_ipvs *ipvs = net_ipvs(net);
+	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
-flush_again:
-	for (idx = 0; idx < ip_vs_conn_tab_size; idx++) {
+      flush_again:
+	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
 		/*
 		 *  Lock is actually needed in this loop.
 		 */
-		rcu_read_lock();
+		ct_write_lock_bh(idx);
 
-		hlist_for_each_entry_rcu(cp, &ip_vs_conn_tab[idx], c_list) {
-			if (!ip_vs_conn_net_eq(cp, net))
-				continue;
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
 			IP_VS_DBG(4, "del connection\n");
+			cp = cidx->cp;
 			ip_vs_conn_expire_now(cp);
-			cp_c = cp->control;
-			/* cp->control is valid only with reference to cp */
-			if (cp_c && __ip_vs_conn_get(cp)) {
+			if (cp->control) {
 				IP_VS_DBG(4, "del conn template\n");
-				ip_vs_conn_expire_now(cp_c);
-				__ip_vs_conn_put(cp);
+				ip_vs_conn_expire_now(cp->control);
 			}
 		}
-		rcu_read_unlock();
+		ct_write_unlock_bh(idx);
 	}
 
 	/* the counter may be not NULL, because maybe some conn entries
 	   are run by slow timer handler or unhashed but still referred */
-	if (atomic_read(&ipvs->conn_count) != 0) {
+	if (atomic_read(&ip_vs_conn_count) != 0) {
 		schedule();
 		goto flush_again;
 	}
 }
-/*
- * per netns init and exit
- */
-int __net_init ip_vs_conn_net_init(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	atomic_set(&ipvs->conn_count, 0);
-
-	proc_create("ip_vs_conn", 0, net->proc_net, &ip_vs_conn_fops);
-	proc_create("ip_vs_conn_sync", 0, net->proc_net, &ip_vs_conn_sync_fops);
-	return 0;
-}
-
-void __net_exit ip_vs_conn_net_cleanup(struct net *net)
-{
-	/* flush all the connection entries first */
-	ip_vs_conn_flush(net);
-	remove_proc_entry("ip_vs_conn", net->proc_net);
-	remove_proc_entry("ip_vs_conn_sync", net->proc_net);
-}
 
 int __init ip_vs_conn_init(void)
 {
 	int idx;
 
-	/* Compute size and mask */
-	ip_vs_conn_tab_size = 1 << ip_vs_conn_tab_bits;
-	ip_vs_conn_tab_mask = ip_vs_conn_tab_size - 1;
-
 	/*
 	 * Allocate the connection hash table and initialize its list heads
 	 */
-	ip_vs_conn_tab = vmalloc(ip_vs_conn_tab_size * sizeof(*ip_vs_conn_tab));
+	ip_vs_conn_tab =
+	    vmalloc(IP_VS_CONN_TAB_SIZE * (sizeof(struct list_head)));
 	if (!ip_vs_conn_tab)
 		return -ENOMEM;
 
 	/* Allocate ip_vs_conn slab cache */
 	ip_vs_conn_cachep = kmem_cache_create("ip_vs_conn",
-					      sizeof(struct ip_vs_conn), 0,
-					      SLAB_HWCACHE_ALIGN, NULL);
+					      sizeof(struct ip_vs_conn) +
+					      2 * sizeof(struct ip_vs_conn_idx),
+					      0, SLAB_HWCACHE_ALIGN, NULL);
 	if (!ip_vs_conn_cachep) {
 		vfree(ip_vs_conn_tab);
 		return -ENOMEM;
@@ -1344,18 +1479,23 @@ int __init ip_vs_conn_init(void)
 
 	pr_info("Connection hash table configured "
 		"(size=%d, memory=%ldKbytes)\n",
-		ip_vs_conn_tab_size,
-		(long)(ip_vs_conn_tab_size*sizeof(struct list_head))/1024);
+		IP_VS_CONN_TAB_SIZE,
+		(long)(IP_VS_CONN_TAB_SIZE * sizeof(struct list_head)) / 1024);
 	IP_VS_DBG(0, "Each connection entry needs %Zd bytes at least\n",
-		  sizeof(struct ip_vs_conn));
+		  sizeof(struct ip_vs_conn) +
+		  2 * sizeof(struct ip_vs_conn_idx));
 
-	for (idx = 0; idx < ip_vs_conn_tab_size; idx++)
-		INIT_HLIST_HEAD(&ip_vs_conn_tab[idx]);
+	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
+		INIT_LIST_HEAD(&ip_vs_conn_tab[idx]);
+	}
 
-	for (idx = 0; idx < CT_LOCKARRAY_SIZE; idx++)  {
-		spin_lock_init(&__ip_vs_conntbl_lock_array[idx].l);
+	for (idx = 0; idx < CT_LOCKARRAY_SIZE; idx++) {
+		rwlock_init(&__ip_vs_conntbl_lock_array[idx].l);
 	}
 
+	proc_create("ip_vs_conn", 0, init_net.proc_net, &ip_vs_conn_fops);
+	proc_create("ip_vs_conn_sync", 0, init_net.proc_net, &ip_vs_conn_sync_fops);
+
 	/* calculate the random value for connection hash */
 	get_random_bytes(&ip_vs_conn_rnd, sizeof(ip_vs_conn_rnd));
 
@@ -1364,9 +1504,12 @@ int __init ip_vs_conn_init(void)
 
 void ip_vs_conn_cleanup(void)
 {
-	/* Wait all ip_vs_conn_rcu_free() callbacks to complete */
-	rcu_barrier();
+	/* flush all the connection entries first */
+	ip_vs_conn_flush();
+
 	/* Release the empty cache */
 	kmem_cache_destroy(ip_vs_conn_cachep);
+	remove_proc_entry("ip_vs_conn", init_net.proc_net);
+	remove_proc_entry("ip_vs_conn_sync", init_net.proc_net);
 	vfree(ip_vs_conn_tab);
 }
diff --git a/net/netfilter/ipvs/ip_vs_core.c b/net/netfilter/ipvs/ip_vs_core.c
index ec3ddbc..63064e3 100644
--- a/net/netfilter/ipvs/ip_vs_core.c
+++ b/net/netfilter/ipvs/ip_vs_core.c
@@ -31,17 +31,13 @@
 #include <linux/kernel.h>
 #include <linux/ip.h>
 #include <linux/tcp.h>
-#include <linux/sctp.h>
 #include <linux/icmp.h>
-#include <linux/slab.h>
 
 #include <net/ip.h>
 #include <net/tcp.h>
 #include <net/udp.h>
-#include <net/icmp.h>                   /* for icmp_send */
+#include <net/icmp.h>		/* for icmp_send */
 #include <net/route.h>
-#include <net/ip6_checksum.h>
-#include <net/netns/generic.h>		/* net_generic() */
 
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
@@ -49,18 +45,17 @@
 #ifdef CONFIG_IP_VS_IPV6
 #include <net/ipv6.h>
 #include <linux/netfilter_ipv6.h>
-#include <net/ip6_route.h>
 #endif
 
 #include <net/ip_vs.h>
-
+#include <net/ip_vs_synproxy.h>
 
 EXPORT_SYMBOL(register_ip_vs_scheduler);
 EXPORT_SYMBOL(unregister_ip_vs_scheduler);
+EXPORT_SYMBOL(ip_vs_skb_replace);
 EXPORT_SYMBOL(ip_vs_proto_name);
 EXPORT_SYMBOL(ip_vs_conn_new);
-EXPORT_SYMBOL(ip_vs_conn_in_get);
-EXPORT_SYMBOL(ip_vs_conn_out_get);
+EXPORT_SYMBOL(ip_vs_conn_get);
 #ifdef CONFIG_IP_VS_PROTO_TCP
 EXPORT_SYMBOL(ip_vs_tcp_conn_listen);
 #endif
@@ -69,15 +64,11 @@ EXPORT_SYMBOL(ip_vs_conn_put);
 EXPORT_SYMBOL(ip_vs_get_debug_level);
 #endif
 
-static int ip_vs_net_id __read_mostly;
-/* netns cnt used for uniqueness */
-static atomic_t ipvs_netns_cnt = ATOMIC_INIT(0);
-
 /* ID used in ICMP lookups */
 #define icmp_id(icmph)          (((icmph)->un).echo.id)
 #define icmpv6_id(icmph)        (icmph->icmp6_dataun.u_echo.identifier)
 
-const char *ip_vs_proto_name(unsigned int proto)
+const char *ip_vs_proto_name(unsigned proto)
 {
 	static char buf[20];
 
@@ -88,8 +79,6 @@ const char *ip_vs_proto_name(unsigned int proto)
 		return "UDP";
 	case IPPROTO_TCP:
 		return "TCP";
-	case IPPROTO_SCTP:
-		return "SCTP";
 	case IPPROTO_ICMP:
 		return "ICMP";
 #ifdef CONFIG_IP_VS_IPV6
@@ -108,106 +97,21 @@ void ip_vs_init_hash_table(struct list_head *table, int rows)
 		INIT_LIST_HEAD(&table[rows]);
 }
 
-static inline void
-ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
-{
-	struct ip_vs_dest *dest = cp->dest;
-	struct netns_ipvs *ipvs = net_ipvs(skb_net(skb));
-
-	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
-		struct ip_vs_cpu_stats *s;
-
-		s = this_cpu_ptr(dest->stats.cpustats);
-		s->ustats.inpkts++;
-		u64_stats_update_begin(&s->syncp);
-		s->ustats.inbytes += skb->len;
-		u64_stats_update_end(&s->syncp);
-
-		s = this_cpu_ptr(dest->svc->stats.cpustats);
-		s->ustats.inpkts++;
-		u64_stats_update_begin(&s->syncp);
-		s->ustats.inbytes += skb->len;
-		u64_stats_update_end(&s->syncp);
-
-		s = this_cpu_ptr(ipvs->tot_stats.cpustats);
-		s->ustats.inpkts++;
-		u64_stats_update_begin(&s->syncp);
-		s->ustats.inbytes += skb->len;
-		u64_stats_update_end(&s->syncp);
-	}
-}
-
-
-static inline void
-ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
-{
-	struct ip_vs_dest *dest = cp->dest;
-	struct netns_ipvs *ipvs = net_ipvs(skb_net(skb));
-
-	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
-		struct ip_vs_cpu_stats *s;
-
-		s = this_cpu_ptr(dest->stats.cpustats);
-		s->ustats.outpkts++;
-		u64_stats_update_begin(&s->syncp);
-		s->ustats.outbytes += skb->len;
-		u64_stats_update_end(&s->syncp);
-
-		s = this_cpu_ptr(dest->svc->stats.cpustats);
-		s->ustats.outpkts++;
-		u64_stats_update_begin(&s->syncp);
-		s->ustats.outbytes += skb->len;
-		u64_stats_update_end(&s->syncp);
-
-		s = this_cpu_ptr(ipvs->tot_stats.cpustats);
-		s->ustats.outpkts++;
-		u64_stats_update_begin(&s->syncp);
-		s->ustats.outbytes += skb->len;
-		u64_stats_update_end(&s->syncp);
-	}
-}
-
-
-static inline void
-ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc)
-{
-	struct netns_ipvs *ipvs = net_ipvs(svc->net);
-	struct ip_vs_cpu_stats *s;
-
-	s = this_cpu_ptr(cp->dest->stats.cpustats);
-	s->ustats.conns++;
-
-	s = this_cpu_ptr(svc->stats.cpustats);
-	s->ustats.conns++;
-
-	s = this_cpu_ptr(ipvs->tot_stats.cpustats);
-	s->ustats.conns++;
-}
-
-
-static inline void
+static inline int
 ip_vs_set_state(struct ip_vs_conn *cp, int direction,
-		const struct sk_buff *skb,
-		struct ip_vs_proto_data *pd)
+		const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
-	if (likely(pd->pp->state_transition))
-		pd->pp->state_transition(cp, direction, skb, pd);
+	if (unlikely(!pp->state_transition))
+		return 0;
+	return pp->state_transition(cp, direction, skb, pp);
 }
 
-static inline int
-ip_vs_conn_fill_param_persist(const struct ip_vs_service *svc,
-			      struct sk_buff *skb, int protocol,
-			      const union nf_inet_addr *caddr, __be16 cport,
-			      const union nf_inet_addr *vaddr, __be16 vport,
-			      struct ip_vs_conn_param *p)
+static inline __u16
+ip_vs_onepacket_enabled(struct ip_vs_service *svc, struct ip_vs_iphdr *iph)
 {
-	ip_vs_conn_fill_param(svc->net, svc->af, protocol, caddr, cport, vaddr,
-			      vport, p);
-	p->pe = rcu_dereference(svc->pe);
-	if (p->pe && p->pe->fill_param)
-		return p->pe->fill_param(p, skb);
-
-	return 0;
+	return (svc->flags & IP_VS_SVC_F_ONEPACKET
+		&& iph->protocol == IPPROTO_UDP)
+	    ? IP_VS_CONN_F_ONE_PACKET : 0;
 }
 
 /*
@@ -217,34 +121,33 @@ ip_vs_conn_fill_param_persist(const struct ip_vs_service *svc,
  *  Locking: we are svc user (svc->refcnt), so we hold all dests too
  *  Protocols supported: TCP, UDP
  */
-static struct ip_vs_conn *
-ip_vs_sched_persist(struct ip_vs_service *svc,
-		    struct sk_buff *skb, __be16 src_port, __be16 dst_port,
-		    int *ignored, struct ip_vs_iphdr *iph)
+static struct ip_vs_conn *ip_vs_sched_persist(struct ip_vs_service *svc,
+					      struct sk_buff *skb,
+					      __be16 ports[2],
+					      int is_synproxy_on)
 {
 	struct ip_vs_conn *cp = NULL;
+	struct ip_vs_iphdr iph;
 	struct ip_vs_dest *dest;
 	struct ip_vs_conn *ct;
-	__be16 dport = 0;		/* destination port to forward */
-	unsigned int flags;
-	struct ip_vs_conn_param param;
-	const union nf_inet_addr fwmark = { .ip = htonl(svc->fwmark) };
+	__be16 dport;		/* destination port to forward */
 	union nf_inet_addr snet;	/* source network of the client,
 					   after masking */
 
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
+
 	/* Mask saddr with the netmask to adjust template granularity */
 #ifdef CONFIG_IP_VS_IPV6
 	if (svc->af == AF_INET6)
-		ipv6_addr_prefix(&snet.in6, &iph->saddr.in6,
-				 (__force __u32) svc->netmask);
+		ipv6_addr_prefix(&snet.in6, &iph.saddr.in6, svc->netmask);
 	else
 #endif
-		snet.ip = iph->saddr.ip & svc->netmask;
+		snet.ip = iph.saddr.ip & svc->netmask;
 
 	IP_VS_DBG_BUF(6, "p-schedule: src %s:%u dest %s:%u "
 		      "mnet %s\n",
-		      IP_VS_DBG_ADDR(svc->af, &iph->saddr), ntohs(src_port),
-		      IP_VS_DBG_ADDR(svc->af, &iph->daddr), ntohs(dst_port),
+		      IP_VS_DBG_ADDR(svc->af, &iph.saddr), ntohs(ports[0]),
+		      IP_VS_DBG_ADDR(svc->af, &iph.daddr), ntohs(ports[1]),
 		      IP_VS_DBG_ADDR(svc->af, &snet));
 
 	/*
@@ -260,99 +163,131 @@ ip_vs_sched_persist(struct ip_vs_service *svc,
 	 * service, and a template like <caddr, 0, vaddr, vport, daddr, dport>
 	 * is created for other persistent services.
 	 */
-	{
-		int protocol = iph->protocol;
-		const union nf_inet_addr *vaddr = &iph->daddr;
-		__be16 vport = 0;
-
-		if (dst_port == svc->port) {
-			/* non-FTP template:
-			 * <protocol, caddr, 0, vaddr, vport, daddr, dport>
-			 * FTP template:
-			 * <protocol, caddr, 0, vaddr, 0, daddr, 0>
+	if (ports[1] == svc->port) {
+		/* Check if a template already exists */
+		if (svc->port != FTPPORT)
+			ct = ip_vs_ct_in_get(svc->af, iph.protocol, &snet, 0,
+					     &iph.daddr, ports[1]);
+		else
+			ct = ip_vs_ct_in_get(svc->af, iph.protocol, &snet, 0,
+					     &iph.daddr, 0);
+
+		if (!ct || !ip_vs_check_template(ct)) {
+			/*
+			 * No template found or the dest of the connection
+			 * template is not available.
+			 */
+			dest = svc->scheduler->schedule(svc, skb);
+			if (dest == NULL) {
+				IP_VS_DBG(1, "p-schedule: no dest found.\n");
+				return NULL;
+			}
+
+			/*
+			 * Create a template like <protocol,caddr,0,
+			 * vaddr,vport,daddr,dport> for non-ftp service,
+			 * and <protocol,caddr,0,vaddr,0,daddr,0>
+			 * for ftp service.
 			 */
 			if (svc->port != FTPPORT)
-				vport = dst_port;
+				ct = ip_vs_conn_new(svc->af, iph.protocol,
+						    &snet, 0,
+						    &iph.daddr,
+						    ports[1],
+						    &dest->addr, dest->port,
+						    IP_VS_CONN_F_TEMPLATE,
+						    dest, NULL, 0);
+			else
+				ct = ip_vs_conn_new(svc->af, iph.protocol,
+						    &snet, 0,
+						    &iph.daddr, 0,
+						    &dest->addr, 0,
+						    IP_VS_CONN_F_TEMPLATE,
+						    dest, NULL, 0);
+			if (ct == NULL)
+				return NULL;
+
+			ct->timeout = svc->timeout;
 		} else {
-			/* Note: persistent fwmark-based services and
-			 * persistent port zero service are handled here.
-			 * fwmark template:
-			 * <IPPROTO_IP,caddr,0,fwmark,0,daddr,0>
-			 * port zero template:
-			 * <protocol,caddr,0,vaddr,0,daddr,0>
-			 */
-			if (svc->fwmark) {
-				protocol = IPPROTO_IP;
-				vaddr = &fwmark;
-			}
-		}
-		/* return *ignored = -1 so NF_DROP can be used */
-		if (ip_vs_conn_fill_param_persist(svc, skb, protocol, &snet, 0,
-						  vaddr, vport, &param) < 0) {
-			*ignored = -1;
-			return NULL;
+			/* set destination with the found template */
+			dest = ct->dest;
 		}
-	}
-
-	/* Check if a template already exists */
-	ct = ip_vs_ct_in_get(&param);
-	if (!ct || !ip_vs_check_template(ct)) {
-		struct ip_vs_scheduler *sched;
-
+		dport = dest->port;
+	} else {
 		/*
-		 * No template found or the dest of the connection
-		 * template is not available.
-		 * return *ignored=0 i.e. ICMP and NF_DROP
+		 * Note: persistent fwmark-based services and persistent
+		 * port zero service are handled here.
+		 * fwmark template: <IPPROTO_IP,caddr,0,fwmark,0,daddr,0>
+		 * port zero template: <protocol,caddr,0,vaddr,0,daddr,0>
 		 */
-		sched = rcu_dereference(svc->scheduler);
-		dest = sched->schedule(svc, skb);
-		if (!dest) {
-			IP_VS_DBG(1, "p-schedule: no dest found.\n");
-			kfree(param.pe_data);
-			*ignored = 0;
-			return NULL;
-		}
+		if (svc->fwmark) {
+			union nf_inet_addr fwmark = {
+				.ip = htonl(svc->fwmark)
+			};
+
+			ct = ip_vs_ct_in_get(svc->af, IPPROTO_IP, &snet, 0,
+					     &fwmark, 0);
+		} else
+			ct = ip_vs_ct_in_get(svc->af, iph.protocol, &snet, 0,
+					     &iph.daddr, 0);
+
+		if (!ct || !ip_vs_check_template(ct)) {
+			/*
+			 * If it is not persistent port zero, return NULL,
+			 * otherwise create a connection template.
+			 */
+			if (svc->port)
+				return NULL;
 
-		if (dst_port == svc->port && svc->port != FTPPORT)
-			dport = dest->port;
-
-		/* Create a template
-		 * This adds param.pe_data to the template,
-		 * and thus param.pe_data will be destroyed
-		 * when the template expires */
-		ct = ip_vs_conn_new(&param, &dest->addr, dport,
-				    IP_VS_CONN_F_TEMPLATE, dest, skb->mark);
-		if (ct == NULL) {
-			kfree(param.pe_data);
-			*ignored = -1;
-			return NULL;
-		}
+			dest = svc->scheduler->schedule(svc, skb);
+			if (dest == NULL) {
+				IP_VS_DBG(1, "p-schedule: no dest found.\n");
+				return NULL;
+			}
 
-		ct->timeout = svc->timeout;
-	} else {
-		/* set destination with the found template */
-		dest = ct->dest;
-		kfree(param.pe_data);
+			/*
+			 * Create a template according to the service
+			 */
+			if (svc->fwmark) {
+				union nf_inet_addr fwmark = {
+					.ip = htonl(svc->fwmark)
+				};
+
+				ct = ip_vs_conn_new(svc->af, IPPROTO_IP,
+						    &snet, 0,
+						    &fwmark, 0,
+						    &dest->addr, 0,
+						    IP_VS_CONN_F_TEMPLATE,
+						    dest, NULL, 0);
+			} else
+				ct = ip_vs_conn_new(svc->af, iph.protocol,
+						    &snet, 0,
+						    &iph.daddr, 0,
+						    &dest->addr, 0,
+						    IP_VS_CONN_F_TEMPLATE,
+						    dest, NULL, 0);
+			if (ct == NULL)
+				return NULL;
+
+			ct->timeout = svc->timeout;
+		} else {
+			/* set destination with the found template */
+			dest = ct->dest;
+		}
+		dport = ports[1];
 	}
 
-	dport = dst_port;
-	if (dport == svc->port && dest->port)
-		dport = dest->port;
-
-	flags = (svc->flags & IP_VS_SVC_F_ONEPACKET
-		 && iph->protocol == IPPROTO_UDP) ?
-		IP_VS_CONN_F_ONE_PACKET : 0;
-
 	/*
 	 *    Create a new connection according to the template
 	 */
-	ip_vs_conn_fill_param(svc->net, svc->af, iph->protocol, &iph->saddr,
-			      src_port, &iph->daddr, dst_port, &param);
-
-	cp = ip_vs_conn_new(&param, &dest->addr, dport, flags, dest, skb->mark);
+	cp = ip_vs_conn_new(svc->af, iph.protocol,
+			    &iph.saddr, ports[0],
+			    &iph.daddr, ports[1],
+			    &dest->addr, dport,
+			    ip_vs_onepacket_enabled(svc, &iph),
+			    dest, skb, is_synproxy_on);
 	if (cp == NULL) {
 		ip_vs_conn_put(ct);
-		*ignored = -1;
 		return NULL;
 	}
 
@@ -366,79 +301,30 @@ ip_vs_sched_persist(struct ip_vs_service *svc,
 	return cp;
 }
 
-
 /*
  *  IPVS main scheduling function
  *  It selects a server according to the virtual service, and
  *  creates a connection entry.
  *  Protocols supported: TCP, UDP
- *
- *  Usage of *ignored
- *
- * 1 :   protocol tried to schedule (eg. on SYN), found svc but the
- *       svc/scheduler decides that this packet should be accepted with
- *       NF_ACCEPT because it must not be scheduled.
- *
- * 0 :   scheduler can not find destination, so try bypass or
- *       return ICMP and then NF_DROP (ip_vs_leave).
- *
- * -1 :  scheduler tried to schedule but fatal error occurred, eg.
- *       ip_vs_conn_new failure (ENOMEM) or ip_vs_sip_fill_param
- *       failure such as missing Call-ID, ENOMEM on skb_linearize
- *       or pe_data. In this case we should return NF_DROP without
- *       any attempts to send ICMP with ip_vs_leave.
  */
-struct ip_vs_conn *
-ip_vs_schedule(struct ip_vs_service *svc, struct sk_buff *skb,
-	       struct ip_vs_proto_data *pd, int *ignored,
-	       struct ip_vs_iphdr *iph)
+struct ip_vs_conn *ip_vs_schedule(struct ip_vs_service *svc,
+				  struct sk_buff *skb, int is_synproxy_on)
 {
-	struct ip_vs_protocol *pp = pd->pp;
 	struct ip_vs_conn *cp = NULL;
-	struct ip_vs_scheduler *sched;
+	struct ip_vs_iphdr iph;
 	struct ip_vs_dest *dest;
 	__be16 _ports[2], *pptr;
-	unsigned int flags;
 
-	*ignored = 1;
-	/*
-	 * IPv6 frags, only the first hit here.
-	 */
-	pptr = frag_safe_skb_hp(skb, iph->len, sizeof(_ports), _ports, iph);
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
+	pptr = skb_header_pointer(skb, iph.len, sizeof(_ports), _ports);
 	if (pptr == NULL)
 		return NULL;
 
 	/*
-	 * FTPDATA needs this check when using local real server.
-	 * Never schedule Active FTPDATA connections from real server.
-	 * For LVS-NAT they must be already created. For other methods
-	 * with persistence the connection is created on SYN+ACK.
-	 */
-	if (pptr[0] == FTPDATA) {
-		IP_VS_DBG_PKT(12, svc->af, pp, skb, 0,
-			      "Not scheduling FTPDATA");
-		return NULL;
-	}
-
-	/*
-	 *    Do not schedule replies from local real server.
-	 */
-	if ((!skb->dev || skb->dev->flags & IFF_LOOPBACK) &&
-	    (cp = pp->conn_in_get(svc->af, skb, iph, 1))) {
-		IP_VS_DBG_PKT(12, svc->af, pp, skb, 0,
-			      "Not scheduling reply for existing connection");
-		__ip_vs_conn_put(cp);
-		return NULL;
-	}
-
-	/*
 	 *    Persistent service
 	 */
 	if (svc->flags & IP_VS_SVC_F_PERSISTENT)
-		return ip_vs_sched_persist(svc, skb, pptr[0], pptr[1], ignored,
-					   iph);
-
-	*ignored = 0;
+		return ip_vs_sched_persist(svc, skb, pptr, is_synproxy_on);
 
 	/*
 	 *    Non-persistent service
@@ -451,34 +337,23 @@ ip_vs_schedule(struct ip_vs_service *svc, struct sk_buff *skb,
 		return NULL;
 	}
 
-	sched = rcu_dereference(svc->scheduler);
-	dest = sched->schedule(svc, skb);
+	dest = svc->scheduler->schedule(svc, skb);
 	if (dest == NULL) {
 		IP_VS_DBG(1, "Schedule: no dest found.\n");
 		return NULL;
 	}
 
-	flags = (svc->flags & IP_VS_SVC_F_ONEPACKET
-		 && iph->protocol == IPPROTO_UDP) ?
-		IP_VS_CONN_F_ONE_PACKET : 0;
-
 	/*
 	 *    Create a connection entry.
 	 */
-	{
-		struct ip_vs_conn_param p;
-
-		ip_vs_conn_fill_param(svc->net, svc->af, iph->protocol,
-				      &iph->saddr, pptr[0], &iph->daddr,
-				      pptr[1], &p);
-		cp = ip_vs_conn_new(&p, &dest->addr,
-				    dest->port ? dest->port : pptr[1],
-				    flags, dest, skb->mark);
-		if (!cp) {
-			*ignored = -1;
-			return NULL;
-		}
-	}
+	cp = ip_vs_conn_new(svc->af, iph.protocol,
+			    &iph.saddr, pptr[0],
+			    &iph.daddr, pptr[1],
+			    &dest->addr, dest->port ? dest->port : pptr[1],
+			    ip_vs_onepacket_enabled(svc, &iph),
+			    dest, skb, is_synproxy_on);
+	if (cp == NULL)
+		return NULL;
 
 	IP_VS_DBG_BUF(6, "Schedule fwd:%c c:%s:%u v:%s:%u "
 		      "d:%s:%u conn->flags:%X conn->refcnt:%d\n",
@@ -492,78 +367,68 @@ ip_vs_schedule(struct ip_vs_service *svc, struct sk_buff *skb,
 	return cp;
 }
 
-
 /*
  *  Pass or drop the packet.
  *  Called by ip_vs_in, when the virtual service is available but
  *  no destination is available for a new connection.
  */
 int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
-		struct ip_vs_proto_data *pd, struct ip_vs_iphdr *iph)
+		struct ip_vs_protocol *pp)
 {
 	__be16 _ports[2], *pptr;
-#ifdef CONFIG_SYSCTL
-	struct net *net;
-	struct netns_ipvs *ipvs;
+	struct ip_vs_iphdr iph;
 	int unicast;
-#endif
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
 
-	pptr = frag_safe_skb_hp(skb, iph->len, sizeof(_ports), _ports, iph);
+	pptr = skb_header_pointer(skb, iph.len, sizeof(_ports), _ports);
 	if (pptr == NULL) {
+		ip_vs_service_put(svc);
 		return NF_DROP;
 	}
-
-#ifdef CONFIG_SYSCTL
-	net = skb_net(skb);
-
 #ifdef CONFIG_IP_VS_IPV6
 	if (svc->af == AF_INET6)
-		unicast = ipv6_addr_type(&iph->daddr.in6) & IPV6_ADDR_UNICAST;
+		unicast = ipv6_addr_type(&iph.daddr.in6) & IPV6_ADDR_UNICAST;
 	else
 #endif
-		unicast = (inet_addr_type(net, iph->daddr.ip) == RTN_UNICAST);
+		unicast =
+		    (inet_addr_type(&init_net, iph.daddr.ip) == RTN_UNICAST);
 
 	/* if it is fwmark-based service, the cache_bypass sysctl is up
 	   and the destination is a non-local unicast, then create
 	   a cache_bypass connection entry */
-	ipvs = net_ipvs(net);
-	if (ipvs->sysctl_cache_bypass && svc->fwmark && unicast) {
-		int ret;
+	if (sysctl_ip_vs_cache_bypass && svc->fwmark && unicast) {
+		int ret, cs;
 		struct ip_vs_conn *cp;
-		unsigned int flags = (svc->flags & IP_VS_SVC_F_ONEPACKET &&
-				      iph->protocol == IPPROTO_UDP) ?
-				      IP_VS_CONN_F_ONE_PACKET : 0;
-		union nf_inet_addr daddr =  { .all = { 0, 0, 0, 0 } };
+		union nf_inet_addr daddr = {.all = {0, 0, 0, 0} };
+
+		ip_vs_service_put(svc);
 
 		/* create a new connection entry */
 		IP_VS_DBG(6, "%s(): create a cache_bypass entry\n", __func__);
-		{
-			struct ip_vs_conn_param p;
-			ip_vs_conn_fill_param(svc->net, svc->af, iph->protocol,
-					      &iph->saddr, pptr[0],
-					      &iph->daddr, pptr[1], &p);
-			cp = ip_vs_conn_new(&p, &daddr, 0,
-					    IP_VS_CONN_F_BYPASS | flags,
-					    NULL, skb->mark);
-			if (!cp)
-				return NF_DROP;
-		}
+		cp = ip_vs_conn_new(svc->af, iph.protocol,
+				    &iph.saddr, pptr[0],
+				    &iph.daddr, pptr[1],
+				    &daddr, 0,
+				    IP_VS_CONN_F_BYPASS |
+				    ip_vs_onepacket_enabled(svc, &iph),
+				    NULL, NULL, 0);
+		if (cp == NULL)
+			return NF_DROP;
 
 		/* statistics */
 		ip_vs_in_stats(cp, skb);
 
 		/* set state */
-		ip_vs_set_state(cp, IP_VS_DIR_INPUT, skb, pd);
+		cs = ip_vs_set_state(cp, IP_VS_DIR_INPUT, skb, pp);
 
 		/* transmit the first SYN packet */
-		ret = cp->packet_xmit(skb, cp, pd->pp, iph);
+		ret = cp->packet_xmit(skb, cp, pp);
 		/* do not touch skb anymore */
 
 		atomic_inc(&cp->in_pkts);
 		ip_vs_conn_put(cp);
 		return ret;
 	}
-#endif
 
 	/*
 	 * When the virtual ftp service is presented, packets destined
@@ -571,8 +436,12 @@ int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
 	 * listed in the ipvs table), pass the packets, because it is
 	 * not ipvs job to decide to drop the packets.
 	 */
-	if ((svc->port == FTPPORT) && (pptr[1] != FTPPORT))
+	if ((svc->port == FTPPORT) && (pptr[1] != FTPPORT)) {
+		ip_vs_service_put(svc);
 		return NF_ACCEPT;
+	}
+
+	ip_vs_service_put(svc);
 
 	/*
 	 * Notify the client that the destination is unreachable, and
@@ -584,199 +453,62 @@ int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
 #ifdef CONFIG_IP_VS_IPV6
 	if (svc->af == AF_INET6) {
 		if (!skb->dev) {
-			struct net *net_ = dev_net(skb_dst(skb)->dev);
-
-			skb->dev = net_->loopback_dev;
+			struct net *net = dev_net(skb_dst(skb)->dev);
+			skb->dev = net->loopback_dev;
 		}
+
 		icmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);
 	} else
 #endif
 		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
 
-	return NF_DROP;
-}
-
-#ifdef CONFIG_SYSCTL
-
-static int sysctl_snat_reroute(struct sk_buff *skb)
-{
-	struct netns_ipvs *ipvs = net_ipvs(skb_net(skb));
-	return ipvs->sysctl_snat_reroute;
-}
+	IP_VS_INC_ESTATS(ip_vs_esmib, CONN_SCHED_UNREACH);
 
-static int sysctl_nat_icmp_send(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	return ipvs->sysctl_nat_icmp_send;
+	return NF_DROP;
 }
 
-static int sysctl_expire_nodest_conn(struct netns_ipvs *ipvs)
+/*
+ *      It is hooked before NF_IP_PRI_NAT_SRC at the NF_INET_POST_ROUTING
+ *      chain, and is used for VS/NAT.
+ *      It detects packets for VS/NAT connections and sends the packets
+ *      immediately. This can avoid that iptable_nat mangles the packets
+ *      for VS/NAT.
+ */
+static unsigned int ip_vs_post_routing(const struct nf_hook_ops *ops, struct sk_buff *skb,
+		   const struct net_device *in, const struct net_device *out,
+		   const struct nf_hook_state *state)
 {
-	return ipvs->sysctl_expire_nodest_conn;
+	if (!skb->ipvs_property)
+		return NF_ACCEPT;
+	/* The packet was sent from IPVS, exit this chain */
+	return NF_STOP;
 }
 
-#else
-
-static int sysctl_snat_reroute(struct sk_buff *skb) { return 0; }
-static int sysctl_nat_icmp_send(struct net *net) { return 0; }
-static int sysctl_expire_nodest_conn(struct netns_ipvs *ipvs) { return 0; }
-
-#endif
-
-__sum16 ip_vs_checksum_complete(struct sk_buff *skb, int offset)
+__sum16 ip_vs_checksum_complete(struct sk_buff * skb, int offset)
 {
 	return csum_fold(skb_checksum(skb, offset, skb->len - offset, 0));
 }
 
-static inline enum ip_defrag_users ip_vs_defrag_user(unsigned int hooknum)
-{
-	if (NF_INET_LOCAL_IN == hooknum)
-		return IP_DEFRAG_VS_IN;
-	if (NF_INET_FORWARD == hooknum)
-		return IP_DEFRAG_VS_FWD;
-	return IP_DEFRAG_VS_OUT;
-}
-
 static inline int ip_vs_gather_frags(struct sk_buff *skb, u_int32_t user)
 {
-	int err;
+	int err = ip_defrag(skb, user);
 
-	local_bh_disable();
-	err = ip_defrag(skb, user);
-	local_bh_enable();
 	if (!err)
 		ip_send_check(ip_hdr(skb));
 
 	return err;
 }
 
-static int ip_vs_route_me_harder(int af, struct sk_buff *skb)
-{
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6) {
-		if (sysctl_snat_reroute(skb) && ip6_route_me_harder(skb) != 0)
-			return 1;
-	} else
-#endif
-		if ((sysctl_snat_reroute(skb) ||
-		     skb_rtable(skb)->rt_flags & RTCF_LOCAL) &&
-		    ip_route_me_harder(skb, RTN_LOCAL) != 0)
-			return 1;
-
-	return 0;
-}
-
-/*
- * Packet has been made sufficiently writable in caller
- * - inout: 1=in->out, 0=out->in
- */
-void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int inout)
-{
-	struct iphdr *iph	 = ip_hdr(skb);
-	unsigned int icmp_offset = iph->ihl*4;
-	struct icmphdr *icmph	 = (struct icmphdr *)(skb_network_header(skb) +
-						      icmp_offset);
-	struct iphdr *ciph	 = (struct iphdr *)(icmph + 1);
-
-	if (inout) {
-		iph->saddr = cp->vaddr.ip;
-		ip_send_check(iph);
-		ciph->daddr = cp->vaddr.ip;
-		ip_send_check(ciph);
-	} else {
-		iph->daddr = cp->daddr.ip;
-		ip_send_check(iph);
-		ciph->saddr = cp->daddr.ip;
-		ip_send_check(ciph);
-	}
-
-	/* the TCP/UDP/SCTP port */
-	if (IPPROTO_TCP == ciph->protocol || IPPROTO_UDP == ciph->protocol ||
-	    IPPROTO_SCTP == ciph->protocol) {
-		__be16 *ports = (void *)ciph + ciph->ihl*4;
-
-		if (inout)
-			ports[1] = cp->vport;
-		else
-			ports[0] = cp->dport;
-	}
-
-	/* And finally the ICMP checksum */
-	icmph->checksum = 0;
-	icmph->checksum = ip_vs_checksum_complete(skb, icmp_offset);
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-	if (inout)
-		IP_VS_DBG_PKT(11, AF_INET, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered outgoing ICMP");
-	else
-		IP_VS_DBG_PKT(11, AF_INET, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered incoming ICMP");
-}
-
 #ifdef CONFIG_IP_VS_IPV6
-void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int inout)
+static inline int ip_vs_gather_frags_v6(struct sk_buff *skb, u_int32_t user)
 {
-	struct ipv6hdr *iph	 = ipv6_hdr(skb);
-	unsigned int icmp_offset = 0;
-	unsigned int offs	 = 0; /* header offset*/
-	int protocol;
-	struct icmp6hdr *icmph;
-	struct ipv6hdr *ciph;
-	unsigned short fragoffs;
-
-	ipv6_find_hdr(skb, &icmp_offset, IPPROTO_ICMPV6, &fragoffs, NULL);
-	icmph = (struct icmp6hdr *)(skb_network_header(skb) + icmp_offset);
-	offs = icmp_offset + sizeof(struct icmp6hdr);
-	ciph = (struct ipv6hdr *)(skb_network_header(skb) + offs);
-
-	protocol = ipv6_find_hdr(skb, &offs, -1, &fragoffs, NULL);
-
-	if (inout) {
-		iph->saddr = cp->vaddr.in6;
-		ciph->daddr = cp->vaddr.in6;
-	} else {
-		iph->daddr = cp->daddr.in6;
-		ciph->saddr = cp->daddr.in6;
-	}
-
-	/* the TCP/UDP/SCTP port */
-	if (!fragoffs && (IPPROTO_TCP == protocol || IPPROTO_UDP == protocol ||
-			  IPPROTO_SCTP == protocol)) {
-		__be16 *ports = (void *)(skb_network_header(skb) + offs);
-
-		IP_VS_DBG(11, "%s() changed port %d to %d\n", __func__,
-			      ntohs(inout ? ports[1] : ports[0]),
-			      ntohs(inout ? cp->vport : cp->dport));
-		if (inout)
-			ports[1] = cp->vport;
-		else
-			ports[0] = cp->dport;
-	}
-
-	/* And finally the ICMP checksum */
-	icmph->icmp6_cksum = ~csum_ipv6_magic(&iph->saddr, &iph->daddr,
-					      skb->len - icmp_offset,
-					      IPPROTO_ICMPV6, 0);
-	skb->csum_start = skb_network_header(skb) - skb->head + icmp_offset;
-	skb->csum_offset = offsetof(struct icmp6hdr, icmp6_cksum);
-	skb->ip_summed = CHECKSUM_PARTIAL;
-
-	if (inout)
-		IP_VS_DBG_PKT(11, AF_INET6, pp, skb,
-			      (void *)ciph - (void *)iph,
-			      "Forwarding altered outgoing ICMPv6");
-	else
-		IP_VS_DBG_PKT(11, AF_INET6, pp, skb,
-			      (void *)ciph - (void *)iph,
-			      "Forwarding altered incoming ICMPv6");
+	/* TODO IPv6: Find out what to do here for IPv6 */
+	return 0;
 }
 #endif
 
 /* Handle relevant response ICMP messages - forward to the right
- * destination host.
+ * destination host. Used for NAT / local client / FULLNAT.
  */
 static int handle_response_icmp(int af, struct sk_buff *skb,
 				union nf_inet_addr *snet,
@@ -786,7 +518,8 @@ static int handle_response_icmp(int af, struct sk_buff *skb,
 {
 	unsigned int verdict = NF_DROP;
 
-	if (IP_VS_FWD_METHOD(cp) != 0) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)
+	    && (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		pr_err("shouldn't reach here, because the box is on the "
 		       "half connection in the tun/dr module.\n");
 	}
@@ -799,33 +532,37 @@ static int handle_response_icmp(int af, struct sk_buff *skb,
 		goto out;
 	}
 
-	if (IPPROTO_TCP == protocol || IPPROTO_UDP == protocol ||
-	    IPPROTO_SCTP == protocol)
+	if (IPPROTO_TCP == protocol || IPPROTO_UDP == protocol)
 		offset += 2 * sizeof(__u16);
-	if (!skb_make_writable(skb, offset))
-		goto out;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ip_vs_nat_icmp_v6(skb, pp, cp, 1);
-	else
-#endif
-		ip_vs_nat_icmp(skb, pp, cp, 1);
-
-	if (ip_vs_route_me_harder(af, skb))
-		goto out;
 
 	/* do the statistics and put it back */
 	ip_vs_out_stats(cp, skb);
 
-	skb->ipvs_property = 1;
-	if (!(cp->flags & IP_VS_CONN_F_NFCT))
-		ip_vs_notrack(skb);
-	else
-		ip_vs_update_conntrack(skb, cp, 0);
-	verdict = NF_ACCEPT;
+	if (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			verdict =
+			    ip_vs_fnat_response_icmp_xmit_v6(skb, pp, cp,
+							     offset);
+		else
+#endif
+			verdict =
+			    ip_vs_fnat_response_icmp_xmit(skb, pp, cp, offset);
+
+	} else {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			verdict =
+			    ip_vs_normal_response_icmp_xmit_v6(skb, pp, cp,
+							       offset);
+		else
+#endif
+			verdict =
+			    ip_vs_normal_response_icmp_xmit(skb, pp, cp,
+							    offset);
+	}
 
-out:
+      out:
 	__ip_vs_conn_put(cp);
 
 	return verdict;
@@ -836,23 +573,23 @@ out:
  *	Find any that might be relevant, check against existing connections.
  *	Currently handles error types - unreachable, quench, ttl exceeded.
  */
-static int ip_vs_out_icmp(struct sk_buff *skb, int *related,
-			  unsigned int hooknum)
+static int ip_vs_out_icmp(struct sk_buff *skb, int *related)
 {
 	struct iphdr *iph;
-	struct icmphdr	_icmph, *ic;
-	struct iphdr	_ciph, *cih;	/* The ip header contained within the ICMP */
+	struct icmphdr _icmph, *ic;
+	struct iphdr _ciph, *cih;	/* The ip header contained within the ICMP */
 	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
 	unsigned int offset, ihl;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
 	/* reassemble IP fragments */
-	if (ip_is_fragment(ip_hdr(skb))) {
-		if (ip_vs_gather_frags(skb, ip_vs_defrag_user(hooknum)))
+	if (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET)) {
+		if (ip_vs_gather_frags(skb, IP_DEFRAG_VS_OUT))
 			return NF_STOLEN;
 	}
 
@@ -863,8 +600,7 @@ static int ip_vs_out_icmp(struct sk_buff *skb, int *related,
 		return NF_DROP;
 
 	IP_VS_DBG(12, "Outgoing ICMP (%d,%d) %pI4->%pI4\n",
-		  ic->type, ntohs(icmp_id(ic)),
-		  &iph->saddr, &iph->daddr);
+		  ic->type, ntohs(icmp_id(ic)), &iph->saddr, &iph->daddr);
 
 	/*
 	 * Work through seeing if this is for us.
@@ -884,49 +620,63 @@ static int ip_vs_out_icmp(struct sk_buff *skb, int *related,
 	offset += sizeof(_icmph);
 	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
 	if (cih == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
 
 	pp = ip_vs_proto_get(cih->protocol);
 	if (!pp)
 		return NF_ACCEPT;
 
 	/* Is the embedded protocol header present? */
-	if (unlikely(cih->frag_off & htons(IP_OFFSET) &&
-		     pp->dont_defrag))
+	if (unlikely(cih->frag_off & htons(IP_OFFSET) && pp->dont_defrag))
 		return NF_ACCEPT;
 
-	IP_VS_DBG_PKT(11, AF_INET, pp, skb, offset,
-		      "Checking outgoing ICMP for");
+	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking outgoing ICMP for");
 
-	ip_vs_fill_ip4hdr(cih, &ciph);
-	ciph.len += offset;
+	offset += cih->ihl * 4;
+
+	ip_vs_fill_iphdr(AF_INET, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_out_get(AF_INET, skb, &ciph, 1);
+	cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp)
 		return NF_ACCEPT;
 
 	snet.ip = iph->saddr;
 	return handle_response_icmp(AF_INET, skb, &snet, cih->protocol, cp,
-				    pp, ciph.len, ihl);
+				    pp, offset, ihl);
 }
 
 #ifdef CONFIG_IP_VS_IPV6
-static int ip_vs_out_icmp_v6(struct sk_buff *skb, int *related,
-			     unsigned int hooknum, struct ip_vs_iphdr *ipvsh)
+static int ip_vs_out_icmp_v6(struct sk_buff *skb, int *related)
 {
-	struct icmp6hdr	_icmph, *ic;
-	struct ipv6hdr _ip6h, *ip6h; /* The ip header contained within ICMP */
-	struct ip_vs_iphdr ciph = {.flags = 0, .fragoffs = 0};/*Contained IP */
+	struct ipv6hdr *iph;
+	struct icmp6hdr _icmph, *ic;
+	struct ipv6hdr _ciph, *cih;	/* The ip header contained
+					   within the ICMP */
+	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
+	unsigned int offset;
 	union nf_inet_addr snet;
-	unsigned int writable;
+	int res_dir;
 
 	*related = 1;
-	ic = frag_safe_skb_hp(skb, ipvsh->len, sizeof(_icmph), &_icmph, ipvsh);
+
+	/* reassemble IP fragments */
+	if (ipv6_hdr(skb)->nexthdr == IPPROTO_FRAGMENT) {
+		if (ip_vs_gather_frags_v6(skb, IP_DEFRAG_VS_OUT))
+			return NF_STOLEN;
+	}
+
+	iph = ipv6_hdr(skb);
+	offset = sizeof(struct ipv6hdr);
+	ic = skb_header_pointer(skb, offset, sizeof(_icmph), &_icmph);
 	if (ic == NULL)
 		return NF_DROP;
 
+	IP_VS_DBG(12, "Outgoing ICMPv6 (%d,%d) %pI6->%pI6\n",
+		  ic->icmp6_type, ntohs(icmpv6_id(ic)),
+		  &iph->saddr, &iph->daddr);
+
 	/*
 	 * Work through seeing if this is for us.
 	 * These checks are supposed to be in an order that means easy
@@ -934,63 +684,44 @@ static int ip_vs_out_icmp_v6(struct sk_buff *skb, int *related,
 	 * this means that some packets will manage to get a long way
 	 * down this stack and then be rejected, but that's life.
 	 */
-	if (ic->icmp6_type & ICMPV6_INFOMSG_MASK) {
+	if ((ic->icmp6_type != ICMPV6_DEST_UNREACH) &&
+	    (ic->icmp6_type != ICMPV6_PKT_TOOBIG) &&
+	    (ic->icmp6_type != ICMPV6_TIME_EXCEED)) {
 		*related = 0;
 		return NF_ACCEPT;
 	}
-	/* Fragment header that is before ICMP header tells us that:
-	 * it's not an error message since they can't be fragmented.
-	 */
-	if (ipvsh->flags & IP6_FH_F_FRAG)
-		return NF_DROP;
-
-	IP_VS_DBG(8, "Outgoing ICMPv6 (%d,%d) %pI6c->%pI6c\n",
-		  ic->icmp6_type, ntohs(icmpv6_id(ic)),
-		  &ipvsh->saddr, &ipvsh->daddr);
 
 	/* Now find the contained IP header */
-	ciph.len = ipvsh->len + sizeof(_icmph);
-	ip6h = skb_header_pointer(skb, ciph.len, sizeof(_ip6h), &_ip6h);
-	if (ip6h == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
-	ciph.saddr.in6 = ip6h->saddr; /* conn_out_get() handles reverse order */
-	ciph.daddr.in6 = ip6h->daddr;
-	/* skip possible IPv6 exthdrs of contained IPv6 packet */
-	ciph.protocol = ipv6_find_hdr(skb, &ciph.len, -1, &ciph.fragoffs, NULL);
-	if (ciph.protocol < 0)
-		return NF_ACCEPT; /* Contained IPv6 hdr looks wrong, ignore */
-
-	pp = ip_vs_proto_get(ciph.protocol);
+	offset += sizeof(_icmph);
+	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
+	if (cih == NULL)
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
+
+	pp = ip_vs_proto_get(cih->nexthdr);
 	if (!pp)
 		return NF_ACCEPT;
 
+	/* Is the embedded protocol header present? */
+	/* TODO: we don't support fragmentation at the moment anyways */
+	if (unlikely(cih->nexthdr == IPPROTO_FRAGMENT && pp->dont_defrag))
+		return NF_ACCEPT;
+
+	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking outgoing ICMPv6 for");
+
+	offset += sizeof(struct ipv6hdr);
+
+	ip_vs_fill_iphdr(AF_INET6, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_out_get(AF_INET6, skb, &ciph, 1);
+	cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp)
 		return NF_ACCEPT;
 
-	snet.in6 = ciph.saddr.in6;
-	writable = ciph.len;
-	return handle_response_icmp(AF_INET6, skb, &snet, ciph.protocol, cp,
-				    pp, writable, sizeof(struct ipv6hdr));
+	snet.in6 = iph->saddr;
+	return handle_response_icmp(AF_INET6, skb, &snet, cih->nexthdr, cp,
+				    pp, offset, sizeof(struct ipv6hdr));
 }
 #endif
 
-/*
- * Check if sctp chunc is ABORT chunk
- */
-static inline int is_sctp_abort(const struct sk_buff *skb, int nh_len)
-{
-	sctp_chunkhdr_t *sch, schunk;
-	sch = skb_header_pointer(skb, nh_len + sizeof(sctp_sctphdr_t),
-			sizeof(schunk), &schunk);
-	if (sch == NULL)
-		return 0;
-	if (sch->type == SCTP_CID_ABORT)
-		return 1;
-	return 0;
-}
-
 static inline int is_tcp_reset(const struct sk_buff *skb, int nh_len)
 {
 	struct tcphdr _tcph, *th;
@@ -1001,299 +732,175 @@ static inline int is_tcp_reset(const struct sk_buff *skb, int nh_len)
 	return th->rst;
 }
 
-static inline bool is_new_conn(const struct sk_buff *skb,
-			       struct ip_vs_iphdr *iph)
-{
-	switch (iph->protocol) {
-	case IPPROTO_TCP: {
-		struct tcphdr _tcph, *th;
-
-		th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
-		if (th == NULL)
-			return false;
-		return th->syn;
-	}
-	case IPPROTO_SCTP: {
-		sctp_chunkhdr_t *sch, schunk;
-
-		sch = skb_header_pointer(skb, iph->len + sizeof(sctp_sctphdr_t),
-					 sizeof(schunk), &schunk);
-		if (sch == NULL)
-			return false;
-		return sch->type == SCTP_CID_INIT;
-	}
-	default:
-		return false;
-	}
-}
-
-static inline bool is_new_conn_expected(const struct ip_vs_conn *cp,
-					int conn_reuse_mode)
-{
-	/* Controlled (FTP DATA or persistence)? */
-	if (cp->control)
-		return false;
-
-	switch (cp->protocol) {
-	case IPPROTO_TCP:
-		return (cp->state == IP_VS_TCP_S_TIME_WAIT) ||
-			((conn_reuse_mode & 2) &&
-			 (cp->state == IP_VS_TCP_S_FIN_WAIT) &&
-			 (cp->flags & IP_VS_CONN_F_NOOUTPUT));
-	case IPPROTO_SCTP:
-		return cp->state == IP_VS_SCTP_S_CLOSED;
-	default:
-		return false;
-	}
-}
-
 /* Handle response packets: rewrite addresses and send away...
+ * Used for NAT / local client / FULLNAT.
  */
 static unsigned int
-handle_response(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
-		struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
+handle_response(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
+		struct ip_vs_conn *cp, int ihl)
 {
-	struct ip_vs_protocol *pp = pd->pp;
-
-	IP_VS_DBG_PKT(11, af, pp, skb, 0, "Outgoing packet");
+	unsigned int ret = NF_DROP;
 
-	if (!skb_make_writable(skb, iph->len))
-		goto drop;
-
-	/* mangle the packet */
-	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp, iph))
-		goto drop;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ipv6_hdr(skb)->saddr = cp->vaddr.in6;
-	else
-#endif
-	{
-		ip_hdr(skb)->saddr = cp->vaddr.ip;
-		ip_send_check(ip_hdr(skb));
-	}
+	/* statistics */
+	ip_vs_out_stats(cp, skb);
 
 	/*
-	 * nf_iterate does not expect change in the skb->dst->dev.
-	 * It looks like it is not fatal to enable this code for hooks
-	 * where our handlers are at the end of the chain list and
-	 * when all next handlers use skb->dst->dev and not outdev.
-	 * It will definitely route properly the inout NAT traffic
-	 * when multiple paths are used.
-	 */
-
-	/* For policy routing, packets originating from this
-	 * machine itself may be routed differently to packets
-	 * passing through.  We want this packet to be routed as
-	 * if it came from this machine itself.  So re-compute
-	 * the routing information.
+	 * Syn-proxy step 3 logic: receive syn-ack from rs.
 	 */
-	if (ip_vs_route_me_harder(af, skb))
-		goto drop;
+	if (pp->protocol == IPPROTO_TCP && ip_vs_synproxy_synack_rcv(skb, cp, pp, ihl, &ret) == 0) {
+		goto out;
+	}
 
-	IP_VS_DBG_PKT(10, af, pp, skb, 0, "After SNAT");
+	/* state transition */
+	ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
+	/* transmit */
 
-	ip_vs_out_stats(cp, skb);
-	ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pd);
-	skb->ipvs_property = 1;
-	if (!(cp->flags & IP_VS_CONN_F_NFCT))
-		ip_vs_notrack(skb);
-	else
-		ip_vs_update_conntrack(skb, cp, 0);
-	ip_vs_conn_put(cp);
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			ret = ip_vs_fnat_response_xmit_v6(skb, pp, cp, ihl);
+		} else
+#endif
+		{
+			ret = ip_vs_fnat_response_xmit(skb, pp, cp, ihl);
+		}
+	} else {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			ret = ip_vs_normal_response_xmit_v6(skb, pp, cp, ihl);
+		} else
+#endif
+		{
+			ret = ip_vs_normal_response_xmit(skb, pp, cp, ihl);
+		}
 
-	LeaveFunction(11);
-	return NF_ACCEPT;
+	}
 
-drop:
+      out:
 	ip_vs_conn_put(cp);
-	kfree_skb(skb);
-	LeaveFunction(11);
-	return NF_STOLEN;
+	return ret;
 }
 
 /*
+ *	It is hooked at the NF_INET_FORWARD chain, used only for VS/NAT.
  *	Check if outgoing packet belongs to the established ip_vs_conn.
  */
 static unsigned int
-ip_vs_out(unsigned int hooknum, struct sk_buff *skb, int af)
+ip_vs_out(const struct nf_hook_ops *ops, struct sk_buff *skb,
+		   const struct net_device *in, const struct net_device *out,
+		   const struct nf_hook_state *state)
 {
-	struct net *net = NULL;
 	struct ip_vs_iphdr iph;
 	struct ip_vs_protocol *pp;
-	struct ip_vs_proto_data *pd;
 	struct ip_vs_conn *cp;
+	int af;
+	int res_dir;
 
 	EnterFunction(11);
 
-	/* Already marked as IPVS request or reply? */
-	if (skb->ipvs_property)
-		return NF_ACCEPT;
-
-	/* Bad... Do not break raw sockets */
-	if (unlikely(skb->sk != NULL && hooknum == NF_INET_LOCAL_OUT &&
-		     af == AF_INET)) {
-		struct sock *sk = skb->sk;
-		struct inet_sock *inet = inet_sk(skb->sk);
-
-		if (inet && sk->sk_family == PF_INET && inet->nodefrag)
-			return NF_ACCEPT;
-	}
-
-	if (unlikely(!skb_dst(skb)))
-		return NF_ACCEPT;
+	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
 
-	net = skb_net(skb);
-	if (!net_ipvs(net)->enable)
+	if (skb->ipvs_property)
 		return NF_ACCEPT;
 
-	ip_vs_fill_iph_skb(af, skb, &iph);
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6) {
 		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
-			int related;
-			int verdict = ip_vs_out_icmp_v6(skb, &related,
-							hooknum, &iph);
+			int related, verdict = ip_vs_out_icmp_v6(skb, &related);
 
 			if (related)
 				return verdict;
+			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 		}
 	} else
 #endif
-		if (unlikely(iph.protocol == IPPROTO_ICMP)) {
-			int related;
-			int verdict = ip_vs_out_icmp(skb, &related, hooknum);
+	if (unlikely(iph.protocol == IPPROTO_ICMP)) {
+		int related, verdict = ip_vs_out_icmp(skb, &related);
 
-			if (related)
-				return verdict;
-		}
+		if (related)
+			return verdict;
+		ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+	}
 
-	pd = ip_vs_proto_data_get(net, iph.protocol);
-	if (unlikely(!pd))
+	pp = ip_vs_proto_get(iph.protocol);
+	if (unlikely(!pp))
 		return NF_ACCEPT;
-	pp = pd->pp;
 
 	/* reassemble IP fragments */
 #ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET)
-#endif
-		if (unlikely(ip_is_fragment(ip_hdr(skb)) && !pp->dont_defrag)) {
-			if (ip_vs_gather_frags(skb,
-					       ip_vs_defrag_user(hooknum)))
-				return NF_STOLEN;
+	if (af == AF_INET6) {
+		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
+			int related, verdict = ip_vs_out_icmp_v6(skb, &related);
+
+			if (related)
+				return verdict;
 
-			ip_vs_fill_ip4hdr(skb_network_header(skb), &iph);
+			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 		}
+	} else
+#endif
+	if (unlikely(ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET) &&
+			     !pp->dont_defrag)) {
+		if (ip_vs_gather_frags(skb, IP_DEFRAG_VS_OUT))
+			return NF_STOLEN;
+
+		ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+	}
 
 	/*
 	 * Check if the packet belongs to an existing entry
 	 */
-	cp = pp->conn_out_get(af, skb, &iph, 0);
-
-	if (likely(cp))
-		return handle_response(af, skb, pd, cp, &iph);
-	if (sysctl_nat_icmp_send(net) &&
-	    (pp->protocol == IPPROTO_TCP ||
-	     pp->protocol == IPPROTO_UDP ||
-	     pp->protocol == IPPROTO_SCTP)) {
-		__be16 _ports[2], *pptr;
-
-		pptr = frag_safe_skb_hp(skb, iph.len,
-					 sizeof(_ports), _ports, &iph);
-		if (pptr == NULL)
-			return NF_ACCEPT;	/* Not for me */
-		if (ip_vs_has_real_service(net, af, iph.protocol, &iph.saddr,
-					   pptr[0])) {
-			/*
-			 * Notify the real server: there is no
-			 * existing entry if it is not RST
-			 * packet or not TCP packet.
-			 */
-			if ((iph.protocol != IPPROTO_TCP &&
-			     iph.protocol != IPPROTO_SCTP)
-			     || ((iph.protocol == IPPROTO_TCP
-				  && !is_tcp_reset(skb, iph.len))
-				 || (iph.protocol == IPPROTO_SCTP
-					&& !is_sctp_abort(skb,
-						iph.len)))) {
+	cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0, &res_dir);
+
+	if (unlikely(!cp)) {
+		if (sysctl_ip_vs_nat_icmp_send &&
+		    (pp->protocol == IPPROTO_TCP ||
+		     pp->protocol == IPPROTO_UDP)) {
+			__be16 _ports[2], *pptr;
+
+			pptr = skb_header_pointer(skb, iph.len,
+						  sizeof(_ports), _ports);
+			if (pptr == NULL)
+				return NF_ACCEPT;	/* Not for me */
+			if (ip_vs_lookup_real_service(af, iph.protocol,
+						      &iph.saddr, pptr[0])) {
+				/*
+				 * Notify the real server: there is no
+				 * existing entry if it is not RST
+				 * packet or not TCP packet.
+				 */
+				if (iph.protocol != IPPROTO_TCP
+				    || !is_tcp_reset(skb, iph.len)) {
 #ifdef CONFIG_IP_VS_IPV6
-				if (af == AF_INET6) {
-					if (!skb->dev)
-						skb->dev = net->loopback_dev;
-					icmpv6_send(skb,
-						    ICMPV6_DEST_UNREACH,
-						    ICMPV6_PORT_UNREACH,
-						    0);
-				} else
+					if (af == AF_INET6) {
+						if (!skb->dev) {
+							struct net *net = dev_net(skb_dst(skb)->dev);
+
+							skb->dev = net->loopback_dev;
+						}
+						icmpv6_send(skb,
+							    ICMPV6_DEST_UNREACH,
+							    ICMPV6_PORT_UNREACH,
+							    0);
+					} else
 #endif
-					icmp_send(skb,
-						  ICMP_DEST_UNREACH,
-						  ICMP_PORT_UNREACH, 0);
-				return NF_DROP;
+						icmp_send(skb,
+							  ICMP_DEST_UNREACH,
+							  ICMP_PORT_UNREACH, 0);
+					return NF_DROP;
+				}
 			}
 		}
+		IP_VS_DBG_PKT(12, pp, skb, 0,
+			      "packet continues traversal as normal");
+		return NF_ACCEPT;
 	}
-	IP_VS_DBG_PKT(12, af, pp, skb, 0,
-		      "ip_vs_out: packet continues traversal as normal");
-	return NF_ACCEPT;
-}
-
-/*
- *	It is hooked at the NF_INET_FORWARD and NF_INET_LOCAL_IN chain,
- *	used only for VS/NAT.
- *	Check if packet is reply for established ip_vs_conn.
- */
-static unsigned int
-ip_vs_reply4(const struct nf_hook_ops *ops, struct sk_buff *skb,
-	     const struct net_device *in, const struct net_device *out,
-	     const struct nf_hook_state *state)
-{
-	return ip_vs_out(ops->hooknum, skb, AF_INET);
-}
-
-/*
- *	It is hooked at the NF_INET_LOCAL_OUT chain, used only for VS/NAT.
- *	Check if packet is reply for established ip_vs_conn.
- */
-static unsigned int
-ip_vs_local_reply4(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		   const struct net_device *in, const struct net_device *out,
-		   const struct nf_hook_state *state)
-{
-	return ip_vs_out(ops->hooknum, skb, AF_INET);
-}
-
-#ifdef CONFIG_IP_VS_IPV6
 
-/*
- *	It is hooked at the NF_INET_FORWARD and NF_INET_LOCAL_IN chain,
- *	used only for VS/NAT.
- *	Check if packet is reply for established ip_vs_conn.
- */
-static unsigned int
-ip_vs_reply6(const struct nf_hook_ops *ops, struct sk_buff *skb,
-	     const struct net_device *in, const struct net_device *out,
-	     const struct nf_hook_state *state)
-{
-	return ip_vs_out(ops->hooknum, skb, AF_INET6);
+	return handle_response(af, skb, pp, cp, iph.len);
 }
 
 /*
- *	It is hooked at the NF_INET_LOCAL_OUT chain, used only for VS/NAT.
- *	Check if packet is reply for established ip_vs_conn.
- */
-static unsigned int
-ip_vs_local_reply6(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		   const struct net_device *in, const struct net_device *out,
-		   const struct nf_hook_state *state)
-{
-	return ip_vs_out(ops->hooknum, skb, AF_INET6);
-}
-
-#endif
-
-/*
  *	Handle ICMP messages in the outside-to-inside direction (incoming).
  *	Find any that might be relevant, check against existing connections,
  *	forward to the right destination host if relevant.
@@ -1302,22 +909,22 @@ ip_vs_local_reply6(const struct nf_hook_ops *ops, struct sk_buff *skb,
 static int
 ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 {
-	struct net *net = NULL;
 	struct iphdr *iph;
-	struct icmphdr	_icmph, *ic;
-	struct iphdr	_ciph, *cih;	/* The ip header contained within the ICMP */
+	struct icmphdr _icmph, *ic;
+	struct iphdr _ciph, *cih;	/* The ip header contained within the ICMP */
 	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
-	struct ip_vs_proto_data *pd;
-	unsigned int offset, offset2, ihl, verdict;
-	bool ipip;
+	unsigned int offset, ihl, verdict;
+	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
 	/* reassemble IP fragments */
-	if (ip_is_fragment(ip_hdr(skb))) {
-		if (ip_vs_gather_frags(skb, ip_vs_defrag_user(hooknum)))
+	if (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET)) {
+		if (ip_vs_gather_frags(skb, hooknum == NF_INET_LOCAL_IN ?
+				       IP_DEFRAG_VS_IN : IP_DEFRAG_VS_FWD))
 			return NF_STOLEN;
 	}
 
@@ -1328,8 +935,7 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 		return NF_DROP;
 
 	IP_VS_DBG(12, "Incoming ICMP (%d,%d) %pI4->%pI4\n",
-		  ic->type, ntohs(icmp_id(ic)),
-		  &iph->saddr, &iph->daddr);
+		  ic->type, ntohs(icmp_id(ic)), &iph->saddr, &iph->daddr);
 
 	/*
 	 * Work through seeing if this is for us.
@@ -1349,48 +955,33 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 	offset += sizeof(_icmph);
 	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
 	if (cih == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
-
-	net = skb_net(skb);
-
-	/* Special case for errors for IPIP packets */
-	ipip = false;
-	if (cih->protocol == IPPROTO_IPIP) {
-		if (unlikely(cih->frag_off & htons(IP_OFFSET)))
-			return NF_ACCEPT;
-		/* Error for our IPIP must arrive at LOCAL_IN */
-		if (!(skb_rtable(skb)->rt_flags & RTCF_LOCAL))
-			return NF_ACCEPT;
-		offset += cih->ihl * 4;
-		cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
-		if (cih == NULL)
-			return NF_ACCEPT; /* The packet looks wrong, ignore */
-		ipip = true;
-	}
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
 
-	pd = ip_vs_proto_data_get(net, cih->protocol);
-	if (!pd)
+	pp = ip_vs_proto_get(cih->protocol);
+	if (!pp)
 		return NF_ACCEPT;
-	pp = pd->pp;
 
 	/* Is the embedded protocol header present? */
-	if (unlikely(cih->frag_off & htons(IP_OFFSET) &&
-		     pp->dont_defrag))
+	if (unlikely(cih->frag_off & htons(IP_OFFSET) && pp->dont_defrag))
 		return NF_ACCEPT;
 
-	IP_VS_DBG_PKT(11, AF_INET, pp, skb, offset,
-		      "Checking incoming ICMP for");
+	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking incoming ICMP for");
 
-	offset2 = offset;
-	ip_vs_fill_ip4hdr(cih, &ciph);
-	ciph.len += offset;
-	offset = ciph.len;
-	/* The embedded headers contain source and dest in reverse order.
-	 * For IPIP this is error for request, not for reply.
-	 */
-	cp = pp->conn_in_get(AF_INET, skb, &ciph, ipip ? 0 : 1);
-	if (!cp)
+	offset += cih->ihl * 4;
+
+	ip_vs_fill_iphdr(AF_INET, cih, &ciph);
+	/* The embedded headers contain source and dest in reverse order */
+	cp = pp->conn_in_get(AF_INET, skb, pp, &ciph, offset, 1, &res_dir);
+	if (!cp) {
 		return NF_ACCEPT;
+	}
+
+	if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+		/* The packet belong to a local client / fullnat */
+		snet.ip = iph->saddr;
+		return handle_response_icmp(AF_INET, skb, &snet,
+					    cih->protocol, cp, pp, offset, ihl);
+	}
 
 	verdict = NF_DROP;
 
@@ -1402,90 +993,53 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 		goto out;
 	}
 
-	if (ipip) {
-		__be32 info = ic->un.gateway;
-
-		/* Update the MTU */
-		if (ic->type == ICMP_DEST_UNREACH &&
-		    ic->code == ICMP_FRAG_NEEDED) {
-			struct ip_vs_dest *dest = cp->dest;
-			u32 mtu = ntohs(ic->un.frag.mtu);
-
-			/* Strip outer IP and ICMP, go to IPIP header */
-			__skb_pull(skb, ihl + sizeof(_icmph));
-			offset2 -= ihl + sizeof(_icmph);
-			skb_reset_network_header(skb);
-			IP_VS_DBG(12, "ICMP for IPIP %pI4->%pI4: mtu=%u\n",
-				&ip_hdr(skb)->saddr, &ip_hdr(skb)->daddr, mtu);
-			ipv4_update_pmtu(skb, dev_net(skb->dev),
-					 mtu, 0, 0, 0, 0);
-			/* Client uses PMTUD? */
-			if (!(cih->frag_off & htons(IP_DF)))
-				goto ignore_ipip;
-			/* Prefer the resulting PMTU */
-			if (dest) {
-				struct ip_vs_dest_dst *dest_dst;
-
-				rcu_read_lock();
-				dest_dst = rcu_dereference(dest->dest_dst);
-				if (dest_dst)
-					mtu = dst_mtu(dest_dst->dst_cache);
-				rcu_read_unlock();
-			}
-			if (mtu > 68 + sizeof(struct iphdr))
-				mtu -= sizeof(struct iphdr);
-			info = htonl(mtu);
-		}
-		/* Strip outer IP, ICMP and IPIP, go to IP header of
-		 * original request.
-		 */
-		__skb_pull(skb, offset2);
-		skb_reset_network_header(skb);
-		IP_VS_DBG(12, "Sending ICMP for %pI4->%pI4: t=%u, c=%u, i=%u\n",
-			&ip_hdr(skb)->saddr, &ip_hdr(skb)->daddr,
-			ic->type, ic->code, ntohl(info));
-		icmp_send(skb, ic->type, ic->code, info);
-		/* ICMP can be shorter but anyways, account it */
-		ip_vs_out_stats(cp, skb);
-
-ignore_ipip:
-		consume_skb(skb);
-		verdict = NF_STOLEN;
-		goto out;
-	}
-
 	/* do the statistics and put it back */
 	ip_vs_in_stats(cp, skb);
-	if (IPPROTO_TCP == cih->protocol || IPPROTO_UDP == cih->protocol ||
-	    IPPROTO_SCTP == cih->protocol)
+	if (IPPROTO_TCP == cih->protocol || IPPROTO_UDP == cih->protocol)
 		offset += 2 * sizeof(__u16);
-	verdict = ip_vs_icmp_xmit(skb, cp, pp, offset, hooknum, &ciph);
+	verdict = ip_vs_icmp_xmit(skb, cp, pp, offset);
+	/* do not touch skb anymore */
 
-out:
+      out:
 	__ip_vs_conn_put(cp);
 
 	return verdict;
 }
 
 #ifdef CONFIG_IP_VS_IPV6
-static int ip_vs_in_icmp_v6(struct sk_buff *skb, int *related,
-			    unsigned int hooknum, struct ip_vs_iphdr *iph)
+static int
+ip_vs_in_icmp_v6(struct sk_buff *skb, int *related, unsigned int hooknum)
 {
-	struct net *net = NULL;
-	struct ipv6hdr _ip6h, *ip6h;
-	struct icmp6hdr	_icmph, *ic;
-	struct ip_vs_iphdr ciph = {.flags = 0, .fragoffs = 0};/*Contained IP */
+	struct ipv6hdr *iph;
+	struct icmp6hdr _icmph, *ic;
+	struct ipv6hdr _ciph, *cih;	/* The ip header contained
+					   within the ICMP */
+	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
-	struct ip_vs_proto_data *pd;
-	unsigned int offs_ciph, writable, verdict;
+	unsigned int offset, verdict;
+	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
-	ic = frag_safe_skb_hp(skb, iph->len, sizeof(_icmph), &_icmph, iph);
+	/* reassemble IP fragments */
+	if (ipv6_hdr(skb)->nexthdr == IPPROTO_FRAGMENT) {
+		if (ip_vs_gather_frags_v6(skb, hooknum == NF_INET_LOCAL_IN ?
+					  IP_DEFRAG_VS_IN : IP_DEFRAG_VS_FWD))
+			return NF_STOLEN;
+	}
+
+	iph = ipv6_hdr(skb);
+	offset = sizeof(struct ipv6hdr);
+	ic = skb_header_pointer(skb, offset, sizeof(_icmph), &_icmph);
 	if (ic == NULL)
 		return NF_DROP;
 
+	IP_VS_DBG(12, "Incoming ICMPv6 (%d,%d) %pI6->%pI6\n",
+		  ic->icmp6_type, ntohs(icmpv6_id(ic)),
+		  &iph->saddr, &iph->daddr);
+
 	/*
 	 * Work through seeing if this is for us.
 	 * These checks are supposed to be in an order that means easy
@@ -1493,71 +1047,55 @@ static int ip_vs_in_icmp_v6(struct sk_buff *skb, int *related,
 	 * this means that some packets will manage to get a long way
 	 * down this stack and then be rejected, but that's life.
 	 */
-	if (ic->icmp6_type & ICMPV6_INFOMSG_MASK) {
+	if ((ic->icmp6_type != ICMPV6_DEST_UNREACH) &&
+	    (ic->icmp6_type != ICMPV6_PKT_TOOBIG) &&
+	    (ic->icmp6_type != ICMPV6_TIME_EXCEED)) {
 		*related = 0;
 		return NF_ACCEPT;
 	}
-	/* Fragment header that is before ICMP header tells us that:
-	 * it's not an error message since they can't be fragmented.
-	 */
-	if (iph->flags & IP6_FH_F_FRAG)
-		return NF_DROP;
-
-	IP_VS_DBG(8, "Incoming ICMPv6 (%d,%d) %pI6c->%pI6c\n",
-		  ic->icmp6_type, ntohs(icmpv6_id(ic)),
-		  &iph->saddr, &iph->daddr);
 
 	/* Now find the contained IP header */
-	ciph.len = iph->len + sizeof(_icmph);
-	offs_ciph = ciph.len; /* Save ip header offset */
-	ip6h = skb_header_pointer(skb, ciph.len, sizeof(_ip6h), &_ip6h);
-	if (ip6h == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
-	ciph.saddr.in6 = ip6h->saddr; /* conn_in_get() handles reverse order */
-	ciph.daddr.in6 = ip6h->daddr;
-	/* skip possible IPv6 exthdrs of contained IPv6 packet */
-	ciph.protocol = ipv6_find_hdr(skb, &ciph.len, -1, &ciph.fragoffs, NULL);
-	if (ciph.protocol < 0)
-		return NF_ACCEPT; /* Contained IPv6 hdr looks wrong, ignore */
-
-	net = skb_net(skb);
-	pd = ip_vs_proto_data_get(net, ciph.protocol);
-	if (!pd)
+	offset += sizeof(_icmph);
+	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
+	if (cih == NULL)
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
+
+	pp = ip_vs_proto_get(cih->nexthdr);
+	if (!pp)
 		return NF_ACCEPT;
-	pp = pd->pp;
 
-	/* Cannot handle fragmented embedded protocol */
-	if (ciph.fragoffs)
+	/* Is the embedded protocol header present? */
+	/* TODO: we don't support fragmentation at the moment anyways */
+	if (unlikely(cih->nexthdr == IPPROTO_FRAGMENT && pp->dont_defrag))
 		return NF_ACCEPT;
 
-	IP_VS_DBG_PKT(11, AF_INET6, pp, skb, offs_ciph,
-		      "Checking incoming ICMPv6 for");
+	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking incoming ICMPv6 for");
 
-	/* The embedded headers contain source and dest in reverse order
-	 * if not from localhost
-	 */
-	cp = pp->conn_in_get(AF_INET6, skb, &ciph,
-			     (hooknum == NF_INET_LOCAL_OUT) ? 0 : 1);
+	offset += sizeof(struct ipv6hdr);
 
-	if (!cp)
-		return NF_ACCEPT;
-	/* VS/TUN, VS/DR and LOCALNODE just let it go */
-	if ((hooknum == NF_INET_LOCAL_OUT) &&
-	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)) {
-		__ip_vs_conn_put(cp);
+	ip_vs_fill_iphdr(AF_INET6, cih, &ciph);
+	/* The embedded headers contain source and dest in reverse order */
+	cp = pp->conn_in_get(AF_INET6, skb, pp, &ciph, offset, 1, &res_dir);
+	if (!cp) {
 		return NF_ACCEPT;
 	}
 
-	/* do the statistics and put it back */
-	ip_vs_in_stats(cp, skb);
+	if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+		snet.in6 = iph->saddr;
+		return handle_response_icmp(AF_INET6, skb, &snet,
+					    cih->nexthdr,
+					    cp, pp, offset,
+					    sizeof(struct ipv6hdr));
+	}
 
-	/* Need to mangle contained IPv6 header in ICMPv6 packet */
-	writable = ciph.len;
-	if (IPPROTO_TCP == ciph.protocol || IPPROTO_UDP == ciph.protocol ||
-	    IPPROTO_SCTP == ciph.protocol)
-		writable += 2 * sizeof(__u16); /* Also mangle ports */
+	verdict = NF_DROP;
 
-	verdict = ip_vs_icmp_xmit_v6(skb, cp, pp, writable, hooknum, &ciph);
+	/* do the statistics and put it back */
+	ip_vs_in_stats(cp, skb);
+	if (IPPROTO_TCP == cih->nexthdr || IPPROTO_UDP == cih->nexthdr)
+		offset += 2 * sizeof(__u16);
+	verdict = ip_vs_icmp_xmit_v6(skb, cp, pp, offset);
+	/* do not touch skb anymore */
 
 	__ip_vs_conn_put(cp);
 
@@ -1565,133 +1103,93 @@ static int ip_vs_in_icmp_v6(struct sk_buff *skb, int *related,
 }
 #endif
 
-
 /*
  *	Check if it's for virtual services, look it up,
  *	and send it on its way...
  */
 static unsigned int
-ip_vs_in(unsigned int hooknum, struct sk_buff *skb, int af)
+ip_vs_in(const struct nf_hook_ops *ops, struct sk_buff *skb,
+	     const struct net_device *in, const struct net_device *out,
+	     const struct nf_hook_state *state)
 {
-	struct net *net;
 	struct ip_vs_iphdr iph;
 	struct ip_vs_protocol *pp;
-	struct ip_vs_proto_data *pd;
 	struct ip_vs_conn *cp;
-	int ret, pkts;
-	struct netns_ipvs *ipvs;
-	int conn_reuse_mode;
+	int ret, restart, af, pkts;
+	int v = NF_DROP;
+	int res_dir;
 
-	/* Already marked as IPVS request or reply? */
-	if (skb->ipvs_property)
-		return NF_ACCEPT;
+	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 
 	/*
-	 *	Big tappo:
-	 *	- remote client: only PACKET_HOST
-	 *	- route: used for struct net when skb->dev is unset
+	 *      Big tappo: only PACKET_HOST, including loopback for local client
+	 *      Don't handle local packets on IPv6 for now
 	 */
-	if (unlikely((skb->pkt_type != PACKET_HOST &&
-		      hooknum != NF_INET_LOCAL_OUT) ||
-		     !skb_dst(skb))) {
-		ip_vs_fill_iph_skb(af, skb, &iph);
-		IP_VS_DBG_BUF(12, "packet type=%d proto=%d daddr=%s"
-			      " ignored in hook %u\n",
-			      skb->pkt_type, iph.protocol,
-			      IP_VS_DBG_ADDR(af, &iph.daddr), hooknum);
+	if (unlikely(skb->pkt_type != PACKET_HOST)) {
+		IP_VS_DBG_BUF(12, "packet type=%d proto=%d daddr=%s ignored\n",
+			      skb->pkt_type,
+			      iph.protocol, IP_VS_DBG_ADDR(af, &iph.daddr));
 		return NF_ACCEPT;
 	}
-	/* ipvs enabled in this netns ? */
-	net = skb_net(skb);
-	ipvs = net_ipvs(net);
-	if (unlikely(sysctl_backup_only(ipvs) || !ipvs->enable))
-		return NF_ACCEPT;
-
-	ip_vs_fill_iph_skb(af, skb, &iph);
-
-	/* Bad... Do not break raw sockets */
-	if (unlikely(skb->sk != NULL && hooknum == NF_INET_LOCAL_OUT &&
-		     af == AF_INET)) {
-		struct sock *sk = skb->sk;
-		struct inet_sock *inet = inet_sk(skb->sk);
-
-		if (inet && sk->sk_family == PF_INET && inet->nodefrag)
-			return NF_ACCEPT;
-	}
-
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6) {
 		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
-			int related;
-			int verdict = ip_vs_in_icmp_v6(skb, &related, hooknum,
-						       &iph);
+			int related, verdict =
+			    ip_vs_in_icmp_v6(skb, &related, ops->hooknum);
 
 			if (related)
 				return verdict;
+			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 		}
 	} else
 #endif
-		if (unlikely(iph.protocol == IPPROTO_ICMP)) {
-			int related;
-			int verdict = ip_vs_in_icmp(skb, &related, hooknum);
+	if (unlikely(iph.protocol == IPPROTO_ICMP)) {
+		int related, verdict = ip_vs_in_icmp(skb, &related, ops->hooknum);
 
-			if (related)
-				return verdict;
-		}
+		if (related)
+			return verdict;
+		ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+	}
 
 	/* Protocol supported? */
-	pd = ip_vs_proto_data_get(net, iph.protocol);
-	if (unlikely(!pd))
+	pp = ip_vs_proto_get(iph.protocol);
+	if (unlikely(!pp))
 		return NF_ACCEPT;
-	pp = pd->pp;
+
 	/*
 	 * Check if the packet belongs to an existing connection entry
 	 */
-	cp = pp->conn_in_get(af, skb, &iph, 0);
-
-	conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);
-	if (conn_reuse_mode && !iph.fragoffs &&
-	    is_new_conn(skb, &iph) && cp &&
-	    ((unlikely(sysctl_expire_nodest_conn(ipvs)) && cp->dest &&
-	      unlikely(!atomic_read(&cp->dest->weight))) ||
-	     unlikely(is_new_conn_expected(cp, conn_reuse_mode)))) {
-		if (!atomic_read(&cp->n_control))
-			ip_vs_conn_expire_now(cp);
-		__ip_vs_conn_put(cp);
-		cp = NULL;
-	}
+	cp = pp->conn_in_get(af, skb, pp, &iph, iph.len, 0, &res_dir);
 
-	if (unlikely(!cp) && !iph.fragoffs) {
-		/* No (second) fragments need to enter here, as nf_defrag_ipv6
-		 * replayed fragment zero will already have created the cp
-		 */
+	if (likely(cp)) {
+		/* For full-nat/local-client packets, it could be a response */
+		if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+			return handle_response(af, skb, pp, cp, iph.len);
+		}
+	} else {
+		/* create a new connection */
 		int v;
 
-		/* Schedule and create new connection entry into &cp */
-		if (!pp->conn_schedule(af, skb, pd, &v, &cp, &iph))
+		if (!pp->conn_schedule(af, skb, pp, &v, &cp))
 			return v;
 	}
 
 	if (unlikely(!cp)) {
 		/* sorry, all this trouble for a no-hit :) */
-		IP_VS_DBG_PKT(12, af, pp, skb, 0,
-			      "ip_vs_in: packet continues traversal as normal");
-		if (iph.fragoffs) {
-			/* Fragment that couldn't be mapped to a conn entry
-			 * is missing module nf_defrag_ipv6
-			 */
-			IP_VS_DBG_RL("Unhandled frag, load nf_defrag_ipv6\n");
-			IP_VS_DBG_PKT(7, af, pp, skb, 0, "unhandled fragment");
-		}
+		IP_VS_DBG_PKT(12, pp, skb, 0,
+			      "packet continues traversal as normal");
 		return NF_ACCEPT;
 	}
 
-	IP_VS_DBG_PKT(11, af, pp, skb, 0, "Incoming packet");
+	IP_VS_DBG_PKT(11, pp, skb, 0, "Incoming packet");
+
 	/* Check the server status */
 	if (cp->dest && !(cp->dest->flags & IP_VS_DEST_F_AVAILABLE)) {
 		/* the destination server is not available */
 
-		if (sysctl_expire_nodest_conn(ipvs)) {
+		if (sysctl_ip_vs_expire_nodest_conn) {
 			/* try to expire the connection immediately */
 			ip_vs_conn_expire_now(cp);
 		}
@@ -1702,10 +1200,36 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb, int af)
 	}
 
 	ip_vs_in_stats(cp, skb);
-	ip_vs_set_state(cp, IP_VS_DIR_INPUT, skb, pd);
+
+	/*
+	 * Filter out-in ack packet when cp is at SYN_SENT state.
+	 * DROP it if not a valid packet, STORE it if we have 
+	 * space left. 
+	 */
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (0 == ip_vs_synproxy_filter_ack(skb, cp, pp, &iph, &v))) {
+		ip_vs_conn_put(cp);
+		return v;
+	}
+
+	/*
+	 * "Reuse" syn-proxy sessions.
+	 * "Reuse" means update syn_proxy_seq struct and clean ack_skb etc.
+	 */
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (0 != sysctl_ip_vs_synproxy_conn_reuse)) {
+		int v = NF_DROP;
+
+		if (0 == ip_vs_synproxy_reuse_conn(af, skb, cp, pp, &iph, &v)) {
+			ip_vs_conn_put(cp);
+			return v;
+		}
+	}
+
+	restart = ip_vs_set_state(cp, IP_VS_DIR_INPUT, skb, pp);
 	if (cp->packet_xmit)
-		ret = cp->packet_xmit(skb, cp, pp, &iph);
-		/* do not touch skb anymore */
+		ret = cp->packet_xmit(skb, cp, pp);
+	/* do not touch skb anymore */
 	else {
 		IP_VS_DBG_RL("warning: packet_xmit is null");
 		ret = NF_ACCEPT;
@@ -1716,78 +1240,26 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb, int af)
 	 *
 	 * Sync connection if it is about to close to
 	 * encorage the standby servers to update the connections timeout
-	 *
-	 * For ONE_PKT let ip_vs_sync_conn() do the filter work.
 	 */
-
-	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
-		pkts = sysctl_sync_threshold(ipvs);
-	else
-		pkts = atomic_add_return(1, &cp->in_pkts);
-
-	if (ipvs->sync_state & IP_VS_STATE_MASTER)
-		ip_vs_sync_conn(net, cp, pkts);
+	pkts = atomic_add_return(1, &cp->in_pkts);
+	if (af == AF_INET &&
+	    (ip_vs_sync_state & IP_VS_STATE_MASTER) &&
+	    (((cp->protocol != IPPROTO_TCP ||
+	       cp->state == IP_VS_TCP_S_ESTABLISHED) &&
+	      (pkts % sysctl_ip_vs_sync_threshold[1]
+	       == sysctl_ip_vs_sync_threshold[0])) ||
+	     ((cp->protocol == IPPROTO_TCP) && (cp->old_state != cp->state) &&
+	      ((cp->state == IP_VS_TCP_S_FIN_WAIT) ||
+	       (cp->state == IP_VS_TCP_S_CLOSE_WAIT) ||
+	       (cp->state == IP_VS_TCP_S_TIME_WAIT)))))
+		ip_vs_sync_conn(cp);
+	cp->old_state = cp->state;
 
 	ip_vs_conn_put(cp);
 	return ret;
 }
 
 /*
- *	AF_INET handler in NF_INET_LOCAL_IN chain
- *	Schedule and forward packets from remote clients
- */
-static unsigned int
-ip_vs_remote_request4(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		      const struct net_device *in,
-		      const struct net_device *out,
-		      const struct nf_hook_state *state)
-{
-	return ip_vs_in(ops->hooknum, skb, AF_INET);
-}
-
-/*
- *	AF_INET handler in NF_INET_LOCAL_OUT chain
- *	Schedule and forward packets from local clients
- */
-static unsigned int
-ip_vs_local_request4(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		     const struct net_device *in, const struct net_device *out,
-		     const struct nf_hook_state *state)
-{
-	return ip_vs_in(ops->hooknum, skb, AF_INET);
-}
-
-#ifdef CONFIG_IP_VS_IPV6
-
-/*
- *	AF_INET6 handler in NF_INET_LOCAL_IN chain
- *	Schedule and forward packets from remote clients
- */
-static unsigned int
-ip_vs_remote_request6(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		      const struct net_device *in,
-		      const struct net_device *out,
-		      const struct nf_hook_state *state)
-{
-	return ip_vs_in(ops->hooknum, skb, AF_INET6);
-}
-
-/*
- *	AF_INET6 handler in NF_INET_LOCAL_OUT chain
- *	Schedule and forward packets from local clients
- */
-static unsigned int
-ip_vs_local_request6(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		     const struct net_device *in, const struct net_device *out,
-		     const struct nf_hook_state *state)
-{
-	return ip_vs_in(ops->hooknum, skb, AF_INET6);
-}
-
-#endif
-
-
-/*
  *	It is hooked at the NF_INET_FORWARD chain, in order to catch ICMP
  *      related packets destined for 0.0.0.0/0.
  *      When fwmark-based virtual service is used, such as transparent
@@ -1802,243 +1274,168 @@ ip_vs_forward_icmp(const struct nf_hook_ops *ops, struct sk_buff *skb,
 		   const struct nf_hook_state *state)
 {
 	int r;
-	struct net *net;
-	struct netns_ipvs *ipvs;
 
 	if (ip_hdr(skb)->protocol != IPPROTO_ICMP)
 		return NF_ACCEPT;
 
-	/* ipvs enabled in this netns ? */
-	net = skb_net(skb);
-	ipvs = net_ipvs(net);
-	if (unlikely(sysctl_backup_only(ipvs) || !ipvs->enable))
-		return NF_ACCEPT;
-
 	return ip_vs_in_icmp(skb, &r, ops->hooknum);
 }
 
 #ifdef CONFIG_IP_VS_IPV6
 static unsigned int
 ip_vs_forward_icmp_v6(const struct nf_hook_ops *ops, struct sk_buff *skb,
-		      const struct net_device *in, const struct net_device *out,
-		      const struct nf_hook_state *state)
+		   const struct net_device *in, const struct net_device *out,
+		   const struct nf_hook_state *state)
 {
 	int r;
-	struct net *net;
-	struct netns_ipvs *ipvs;
-	struct ip_vs_iphdr iphdr;
-
-	ip_vs_fill_iph_skb(AF_INET6, skb, &iphdr);
-	if (iphdr.protocol != IPPROTO_ICMPV6)
-		return NF_ACCEPT;
 
-	/* ipvs enabled in this netns ? */
-	net = skb_net(skb);
-	ipvs = net_ipvs(net);
-	if (unlikely(sysctl_backup_only(ipvs) || !ipvs->enable))
+	if (ipv6_hdr(skb)->nexthdr != IPPROTO_ICMPV6)
 		return NF_ACCEPT;
 
-	return ip_vs_in_icmp_v6(skb, &r, ops->hooknum, &iphdr);
+	return ip_vs_in_icmp_v6(skb, &r, ops->hooknum);
 }
 #endif
 
+#define IPPROTO_OSPF 89
+static unsigned int
+ip_vs_pre_routing(const struct nf_hook_ops *ops, struct sk_buff *skb,
+		   const struct net_device *in, const struct net_device *out,
+		   const struct nf_hook_state *state)
+{
+	struct ip_vs_iphdr iph;
+	int af;
+	struct ip_vs_service *svc;
+
+	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+
+	/* drop all ip fragment except ospf */
+	if ((af == AF_INET)
+	    && (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET))
+	    && (iph.protocol != IPPROTO_OSPF)) {
+		if(sysctl_ip_vs_frag_drop_entry == 1) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_IP_FRAG_DROP);
+			return NF_DROP;
+		} else {
+			if (ip_vs_gather_frags(skb, IP_DEFRAG_VS_IN))
+				return NF_STOLEN;
+
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_IP_FRAG_GATHER);
+			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+		}
+	}
+
+	/* drop udp packet which send to tcp-vip */
+	if ((sysctl_ip_vs_udp_drop_entry == 1) && (iph.protocol == IPPROTO_UDP)) {
+		if ((svc =
+		     ip_vs_lookup_vip(af, IPPROTO_TCP, &iph.daddr)) != NULL) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_UDP_DROP);
+			return NF_DROP;
+		}
+	}
+
+	/* synproxy: defence synflood */
+	if (iph.protocol == IPPROTO_TCP) {
+		int v = NF_ACCEPT;
+		if (0 == ip_vs_synproxy_syn_rcv(af, skb, &iph, &v)) {
+			return v;
+		}
+	}
+
+	return NF_ACCEPT;
+}
 
 static struct nf_hook_ops ip_vs_ops[] __read_mostly = {
-	/* After packet filtering, change source only for VS/NAT */
-	{
-		.hook		= ip_vs_reply4,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_LOCAL_IN,
-		.priority	= NF_IP_PRI_NAT_SRC - 2,
-	},
 	/* After packet filtering, forward packet through VS/DR, VS/TUN,
 	 * or VS/NAT(change destination), so that filtering rules can be
 	 * applied to IPVS. */
 	{
-		.hook		= ip_vs_remote_request4,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_LOCAL_IN,
-		.priority	= NF_IP_PRI_NAT_SRC - 1,
-	},
-	/* Before ip_vs_in, change source only for VS/NAT */
-	{
-		.hook		= ip_vs_local_reply4,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_LOCAL_OUT,
-		.priority	= NF_IP_PRI_NAT_DST + 1,
+		.hook = ip_vs_in,
+		.owner = THIS_MODULE,
+		.pf = PF_INET,
+		.hooknum = NF_INET_PRE_ROUTING,
+		.priority = NF_IP_PRI_CONNTRACK - 1,
 	},
-	/* After mangle, schedule and forward local requests */
+	/* After packet filtering, change source only for VS/NAT */
 	{
-		.hook		= ip_vs_local_request4,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_LOCAL_OUT,
-		.priority	= NF_IP_PRI_NAT_DST + 2,
-	},
+	 .hook = ip_vs_out,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 100,
+	 },
 	/* After packet filtering (but before ip_vs_out_icmp), catch icmp
 	 * destined for 0.0.0.0/0, which is for incoming IPVS connections */
 	{
-		.hook		= ip_vs_forward_icmp,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_FORWARD,
-		.priority	= 99,
-	},
-	/* After packet filtering, change source only for VS/NAT */
+	 .hook = ip_vs_forward_icmp,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 99,
+	 },
+	/* Before the netfilter connection tracking, exit from POST_ROUTING */
 	{
-		.hook		= ip_vs_reply4,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_FORWARD,
-		.priority	= 100,
-	},
-#ifdef CONFIG_IP_VS_IPV6
-	/* After packet filtering, change source only for VS/NAT */
+	 .hook = ip_vs_post_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_POST_ROUTING,
+	 .priority = NF_IP_PRI_NAT_SRC - 1,
+	 },
+	/* Before the netfilter connection tracking, only deal with FULLNAT/NAT-SynProxy */
 	{
-		.hook		= ip_vs_reply6,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV6,
-		.hooknum	= NF_INET_LOCAL_IN,
-		.priority	= NF_IP6_PRI_NAT_SRC - 2,
-	},
+	 .hook = ip_vs_pre_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP_PRI_CONNTRACK - 2,
+	 },
+#ifdef CONFIG_IP_VS_IPV6
 	/* After packet filtering, forward packet through VS/DR, VS/TUN,
 	 * or VS/NAT(change destination), so that filtering rules can be
 	 * applied to IPVS. */
 	{
-		.hook		= ip_vs_remote_request6,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV6,
-		.hooknum	= NF_INET_LOCAL_IN,
-		.priority	= NF_IP6_PRI_NAT_SRC - 1,
-	},
-	/* Before ip_vs_in, change source only for VS/NAT */
-	{
-		.hook		= ip_vs_local_reply6,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV4,
-		.hooknum	= NF_INET_LOCAL_OUT,
-		.priority	= NF_IP6_PRI_NAT_DST + 1,
-	},
-	/* After mangle, schedule and forward local requests */
+	 .hook = ip_vs_in,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP6_PRI_CONNTRACK - 1,
+	 },
+	/* After packet filtering, change source only for VS/NAT */
 	{
-		.hook		= ip_vs_local_request6,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV6,
-		.hooknum	= NF_INET_LOCAL_OUT,
-		.priority	= NF_IP6_PRI_NAT_DST + 2,
-	},
+	 .hook = ip_vs_out,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 100,
+	 },
 	/* After packet filtering (but before ip_vs_out_icmp), catch icmp
 	 * destined for 0.0.0.0/0, which is for incoming IPVS connections */
 	{
-		.hook		= ip_vs_forward_icmp_v6,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV6,
-		.hooknum	= NF_INET_FORWARD,
-		.priority	= 99,
-	},
-	/* After packet filtering, change source only for VS/NAT */
+	 .hook = ip_vs_forward_icmp_v6,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 99,
+	 },
+	/* Before the netfilter connection tracking, exit from POST_ROUTING */
 	{
-		.hook		= ip_vs_reply6,
-		.owner		= THIS_MODULE,
-		.pf		= NFPROTO_IPV6,
-		.hooknum	= NF_INET_FORWARD,
-		.priority	= 100,
-	},
+	 .hook = ip_vs_post_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_POST_ROUTING,
+	 .priority = NF_IP6_PRI_NAT_SRC - 1,
+	 },
+	/* Before the netfilter connection tracking, only deal with FULLNAT/NAT-SynProxy */
+	{
+	 .hook = ip_vs_pre_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP6_PRI_CONNTRACK - 2,
+	 },
 #endif
 };
-/*
- *	Initialize IP Virtual Server netns mem.
- */
-static int __net_init __ip_vs_init(struct net *net)
-{
-	struct netns_ipvs *ipvs;
-
-	ipvs = net_generic(net, ip_vs_net_id);
-	if (ipvs == NULL)
-		return -ENOMEM;
-
-	/* Hold the beast until a service is registerd */
-	ipvs->enable = 0;
-	ipvs->net = net;
-	/* Counters used for creating unique names */
-	ipvs->gen = atomic_read(&ipvs_netns_cnt);
-	atomic_inc(&ipvs_netns_cnt);
-	net->ipvs = ipvs;
-
-	if (ip_vs_estimator_net_init(net) < 0)
-		goto estimator_fail;
-
-	if (ip_vs_control_net_init(net) < 0)
-		goto control_fail;
-
-	if (ip_vs_protocol_net_init(net) < 0)
-		goto protocol_fail;
-
-	if (ip_vs_app_net_init(net) < 0)
-		goto app_fail;
-
-	if (ip_vs_conn_net_init(net) < 0)
-		goto conn_fail;
-
-	if (ip_vs_sync_net_init(net) < 0)
-		goto sync_fail;
-
-	printk(KERN_INFO "IPVS: Creating netns size=%zu id=%d\n",
-			 sizeof(struct netns_ipvs), ipvs->gen);
-	return 0;
-/*
- * Error handling
- */
-
-sync_fail:
-	ip_vs_conn_net_cleanup(net);
-conn_fail:
-	ip_vs_app_net_cleanup(net);
-app_fail:
-	ip_vs_protocol_net_cleanup(net);
-protocol_fail:
-	ip_vs_control_net_cleanup(net);
-control_fail:
-	ip_vs_estimator_net_cleanup(net);
-estimator_fail:
-	net->ipvs = NULL;
-	return -ENOMEM;
-}
-
-static void __net_exit __ip_vs_cleanup(struct net *net)
-{
-	ip_vs_service_net_cleanup(net);	/* ip_vs_flush() with locks */
-	ip_vs_conn_net_cleanup(net);
-	ip_vs_app_net_cleanup(net);
-	ip_vs_protocol_net_cleanup(net);
-	ip_vs_control_net_cleanup(net);
-	ip_vs_estimator_net_cleanup(net);
-	IP_VS_DBG(2, "ipvs netns %d released\n", net_ipvs(net)->gen);
-	net->ipvs = NULL;
-}
-
-static void __net_exit __ip_vs_dev_cleanup(struct net *net)
-{
-	EnterFunction(2);
-	net_ipvs(net)->enable = 0;	/* Disable packet reception */
-	smp_wmb();
-	ip_vs_sync_net_cleanup(net);
-	LeaveFunction(2);
-}
-
-static struct pernet_operations ipvs_core_ops = {
-	.init = __ip_vs_init,
-	.exit = __ip_vs_cleanup,
-	.id   = &ip_vs_net_id,
-	.size = sizeof(struct netns_ipvs),
-};
-
-static struct pernet_operations ipvs_core_dev_ops = {
-	.exit = __ip_vs_dev_cleanup,
-};
 
 /*
  *	Initialize IP Virtual Server
@@ -2050,63 +1447,48 @@ static int __init ip_vs_init(void)
 	ret = ip_vs_control_init();
 	if (ret < 0) {
 		pr_err("can't setup control.\n");
-		goto exit;
+		goto out_err;
 	}
 
 	ip_vs_protocol_init();
 
-	ret = ip_vs_conn_init();
+	ret = ip_vs_app_init();
 	if (ret < 0) {
-		pr_err("can't setup connection table.\n");
+		pr_err("can't setup application helper.\n");
 		goto cleanup_protocol;
 	}
 
-	ret = register_pernet_subsys(&ipvs_core_ops);	/* Alloc ip_vs struct */
-	if (ret < 0)
-		goto cleanup_conn;
-
-	ret = register_pernet_device(&ipvs_core_dev_ops);
-	if (ret < 0)
-		goto cleanup_sub;
-
-	ret = nf_register_hooks(ip_vs_ops, ARRAY_SIZE(ip_vs_ops));
+	ret = ip_vs_conn_init();
 	if (ret < 0) {
-		pr_err("can't register hooks.\n");
-		goto cleanup_dev;
+		pr_err("can't setup connection table.\n");
+		goto cleanup_app;
 	}
 
-	ret = ip_vs_register_nl_ioctl();
+	ret = nf_register_hooks(ip_vs_ops, ARRAY_SIZE(ip_vs_ops));
 	if (ret < 0) {
-		pr_err("can't register netlink/ioctl.\n");
-		goto cleanup_hooks;
+		pr_err("can't register hooks.\n");
+		goto cleanup_conn;
 	}
 
 	pr_info("ipvs loaded.\n");
-
 	return ret;
 
-cleanup_hooks:
-	nf_unregister_hooks(ip_vs_ops, ARRAY_SIZE(ip_vs_ops));
-cleanup_dev:
-	unregister_pernet_device(&ipvs_core_dev_ops);
-cleanup_sub:
-	unregister_pernet_subsys(&ipvs_core_ops);
 cleanup_conn:
 	ip_vs_conn_cleanup();
+cleanup_app:
+	ip_vs_app_cleanup();
 cleanup_protocol:
 	ip_vs_protocol_cleanup();
 	ip_vs_control_cleanup();
-exit:
+out_err:
 	return ret;
 }
 
 static void __exit ip_vs_cleanup(void)
 {
-	ip_vs_unregister_nl_ioctl();
 	nf_unregister_hooks(ip_vs_ops, ARRAY_SIZE(ip_vs_ops));
-	unregister_pernet_device(&ipvs_core_dev_ops);
-	unregister_pernet_subsys(&ipvs_core_ops);	/* free ip_vs struct */
 	ip_vs_conn_cleanup();
+	ip_vs_app_cleanup();
 	ip_vs_protocol_cleanup();
 	ip_vs_control_cleanup();
 	pr_info("ipvs unloaded.\n");
diff --git a/net/netfilter/ipvs/ip_vs_ctl.c b/net/netfilter/ipvs/ip_vs_ctl.c
index 13c85fb..b7168be 100644
--- a/net/netfilter/ipvs/ip_vs_ctl.c
+++ b/net/netfilter/ipvs/ip_vs_ctl.c
@@ -31,14 +31,12 @@
 #include <linux/workqueue.h>
 #include <linux/swap.h>
 #include <linux/seq_file.h>
-#include <linux/slab.h>
 
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
 #include <linux/mutex.h>
 
 #include <net/net_namespace.h>
-#include <linux/nsproxy.h>
 #include <net/ip.h>
 #ifdef CONFIG_IP_VS_IPV6
 #include <net/ipv6.h>
@@ -51,11 +49,107 @@
 #include <asm/uaccess.h>
 
 #include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
 
 /* semaphore for IPVS sockopts. And, [gs]etsockopt may sleep. */
 static DEFINE_MUTEX(__ip_vs_mutex);
 
+/* lock for service table */
+static DEFINE_RWLOCK(__ip_vs_svc_lock);
+
+/* lock for table with the real services */
+static DEFINE_RWLOCK(__ip_vs_rs_lock);
+
+/* lock for state and timeout tables */
+static DEFINE_RWLOCK(__ip_vs_securetcp_lock);
+
+/* lock for drop entry handling */
+static DEFINE_SPINLOCK(__ip_vs_dropentry_lock);
+
+/* lock for drop packet handling */
+static DEFINE_SPINLOCK(__ip_vs_droppacket_lock);
+
+/* 1/rate drop and drop-entry variables */
+int ip_vs_drop_rate = 0;
+int ip_vs_drop_counter = 0;
+static atomic_t ip_vs_dropentry = ATOMIC_INIT(0);
+
+/* number of virtual services */
+static int ip_vs_num_services = 0;
+
 /* sysctl variables */
+static int sysctl_ip_vs_drop_entry = 0;
+static int sysctl_ip_vs_drop_packet = 0;
+static int sysctl_ip_vs_secure_tcp = 0;
+static int sysctl_ip_vs_amemthresh = 1024;
+static int sysctl_ip_vs_am_droprate = 10;
+int sysctl_ip_vs_cache_bypass = 0;
+int sysctl_ip_vs_expire_nodest_conn = 0;
+int sysctl_ip_vs_expire_quiescent_template = 0;
+int sysctl_ip_vs_sync_threshold[2] = { 3, 50 };
+int sysctl_ip_vs_nat_icmp_send = 0;
+/*
+ * sysctl for FULLNAT
+ */
+int sysctl_ip_vs_timestamp_remove_entry = 1;
+int sysctl_ip_vs_mss_adjust_entry = 1;
+int sysctl_ip_vs_conn_reused_entry = 1;
+int sysctl_ip_vs_toa_entry = 1;
+static int ip_vs_entry_min = 0;
+static int ip_vs_entry_max = 1;
+extern int sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST + 1];
+/*
+ * sysctl for SYNPROXY
+ */
+/* syn-proxy sysctl variables */
+int sysctl_ip_vs_synproxy_init_mss = IP_VS_SYNPROXY_INIT_MSS_DEFAULT;
+int sysctl_ip_vs_synproxy_sack = IP_VS_SYNPROXY_SACK_DEFAULT;
+int sysctl_ip_vs_synproxy_wscale = IP_VS_SYNPROXY_WSCALE_DEFAULT;
+int sysctl_ip_vs_synproxy_timestamp = IP_VS_SYNPROXY_TIMESTAMP_DEFAULT;
+int sysctl_ip_vs_synproxy_synack_ttl = IP_VS_SYNPROXY_TTL_DEFAULT;
+int sysctl_ip_vs_synproxy_defer = IP_VS_SYNPROXY_DEFER_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse = IP_VS_SYNPROXY_CONN_REUSE_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_cl = IP_VS_SYNPROXY_CONN_REUSE_CL_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_tw = IP_VS_SYNPROXY_CONN_REUSE_TW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_fw = IP_VS_SYNPROXY_CONN_REUSE_FW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_cw = IP_VS_SYNPROXY_CONN_REUSE_CW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_la = IP_VS_SYNPROXY_CONN_REUSE_LA_DEFAULT;
+int sysctl_ip_vs_synproxy_dup_ack_thresh = IP_VS_SYNPROXY_DUP_ACK_DEFAULT;
+int sysctl_ip_vs_synproxy_skb_store_thresh = IP_VS_SYNPROXY_SKB_STORE_DEFAULT;
+int sysctl_ip_vs_synproxy_syn_retry = IP_VS_SYNPROXY_SYN_RETRY_DEFAULT;
+
+static int ip_vs_synproxy_switch_min = 0;
+static int ip_vs_synproxy_switch_max = 1;
+static int ip_vs_synproxy_wscale_min = 0;
+static int ip_vs_synproxy_wscale_max = IP_VS_SYNPROXY_WSCALE_MAX;
+static int ip_vs_synproxy_init_mss_min = 0;
+static int ip_vs_synproxy_init_mss_max = 65535;
+static int ip_vs_synproxy_synack_ttl_min = IP_VS_SYNPROXY_TTL_MIN;
+static int ip_vs_synproxy_synack_ttl_max = IP_VS_SYNPROXY_TTL_MAX;
+static int ip_vs_synproxy_dup_ack_cnt_min = 0;
+static int ip_vs_synproxy_dup_ack_cnt_max = 65535;
+static int ip_vs_synproxy_syn_retry_min = 0;
+static int ip_vs_synproxy_syn_retry_max = 6;
+static int ip_vs_synproxy_skb_store_thresh_min = 0;
+static int ip_vs_synproxy_skb_store_thresh_max = 5;
+/* local address port range */
+int sysctl_ip_vs_lport_max = 65535;
+int sysctl_ip_vs_lport_min = 5000;
+int sysctl_ip_vs_lport_tries = 10000;
+static int ip_vs_port_min = 1025;
+static int ip_vs_port_max = 65535;
+static int ip_vs_port_try_min = 10;
+static int ip_vs_port_try_max = 60000;
+/*
+ * sysctl for DEFENCE ATTACK
+ */
+int sysctl_ip_vs_frag_drop_entry = 0;
+int sysctl_ip_vs_tcp_drop_entry = 1;
+int sysctl_ip_vs_udp_drop_entry = 1;
+/* send rst when tcp session expire */
+int sysctl_ip_vs_conn_expire_tcp_rst = 1;
+/* L2 fast xmit, response only (to client) */
+int sysctl_ip_vs_fast_xmit = 1;
 
 #ifdef CONFIG_IP_VS_DEBUG
 static int sysctl_ip_vs_debug_level = 0;
@@ -66,35 +160,24 @@ int ip_vs_get_debug_level(void)
 }
 #endif
 
-
-/*  Protos */
-static void __ip_vs_del_service(struct ip_vs_service *svc, bool cleanup);
-
-
 #ifdef CONFIG_IP_VS_IPV6
 /* Taken from rt6_fill_node() in net/ipv6/route.c, is there a better way? */
-static bool __ip_vs_addr_is_local_v6(struct net *net,
-				     const struct in6_addr *addr)
+static int __ip_vs_addr_is_local_v6(const struct in6_addr *addr)
 {
-	struct flowi6 fl6 = {
-		.daddr = *addr,
-	};
-	struct dst_entry *dst = ip6_route_output(net, NULL, &fl6);
-	bool is_local;
-
-	is_local = !dst->error && dst->dev && (dst->dev->flags & IFF_LOOPBACK);
+	struct rt6_info *rt;
+	struct flowi6 fl;
+	rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
+	if (rt && rt->dst.dev && (rt->dst.dev->flags & IFF_LOOPBACK))
+		return 1;
 
-	dst_release(dst);
-	return is_local;
+	return 0;
 }
 #endif
-
-#ifdef CONFIG_SYSCTL
 /*
  *	update_defense_level is called from keventd and from sysctl,
  *	so it needs to protect itself from softirqs
  */
-static void update_defense_level(struct netns_ipvs *ipvs)
+static void update_defense_level(void)
 {
 	struct sysinfo i;
 	static int old_secure_tcp = 0;
@@ -110,73 +193,73 @@ static void update_defense_level(struct netns_ipvs *ipvs)
 	/* si_swapinfo(&i); */
 	/* availmem = availmem - (i.totalswap - i.freeswap); */
 
-	nomem = (availmem < ipvs->sysctl_amemthresh);
+	nomem = (availmem < sysctl_ip_vs_amemthresh);
 
 	local_bh_disable();
 
 	/* drop_entry */
-	spin_lock(&ipvs->dropentry_lock);
-	switch (ipvs->sysctl_drop_entry) {
+	spin_lock(&__ip_vs_dropentry_lock);
+	switch (sysctl_ip_vs_drop_entry) {
 	case 0:
-		atomic_set(&ipvs->dropentry, 0);
+		atomic_set(&ip_vs_dropentry, 0);
 		break;
 	case 1:
 		if (nomem) {
-			atomic_set(&ipvs->dropentry, 1);
-			ipvs->sysctl_drop_entry = 2;
+			atomic_set(&ip_vs_dropentry, 1);
+			sysctl_ip_vs_drop_entry = 2;
 		} else {
-			atomic_set(&ipvs->dropentry, 0);
+			atomic_set(&ip_vs_dropentry, 0);
 		}
 		break;
 	case 2:
 		if (nomem) {
-			atomic_set(&ipvs->dropentry, 1);
+			atomic_set(&ip_vs_dropentry, 1);
 		} else {
-			atomic_set(&ipvs->dropentry, 0);
-			ipvs->sysctl_drop_entry = 1;
+			atomic_set(&ip_vs_dropentry, 0);
+			sysctl_ip_vs_drop_entry = 1;
 		};
 		break;
 	case 3:
-		atomic_set(&ipvs->dropentry, 1);
+		atomic_set(&ip_vs_dropentry, 1);
 		break;
 	}
-	spin_unlock(&ipvs->dropentry_lock);
+	spin_unlock(&__ip_vs_dropentry_lock);
 
 	/* drop_packet */
-	spin_lock(&ipvs->droppacket_lock);
-	switch (ipvs->sysctl_drop_packet) {
+	spin_lock(&__ip_vs_droppacket_lock);
+	switch (sysctl_ip_vs_drop_packet) {
 	case 0:
-		ipvs->drop_rate = 0;
+		ip_vs_drop_rate = 0;
 		break;
 	case 1:
 		if (nomem) {
-			ipvs->drop_rate = ipvs->drop_counter
-				= ipvs->sysctl_amemthresh /
-				(ipvs->sysctl_amemthresh-availmem);
-			ipvs->sysctl_drop_packet = 2;
+			ip_vs_drop_rate = ip_vs_drop_counter
+			    = sysctl_ip_vs_amemthresh /
+			    (sysctl_ip_vs_amemthresh - availmem);
+			sysctl_ip_vs_drop_packet = 2;
 		} else {
-			ipvs->drop_rate = 0;
+			ip_vs_drop_rate = 0;
 		}
 		break;
 	case 2:
 		if (nomem) {
-			ipvs->drop_rate = ipvs->drop_counter
-				= ipvs->sysctl_amemthresh /
-				(ipvs->sysctl_amemthresh-availmem);
+			ip_vs_drop_rate = ip_vs_drop_counter
+			    = sysctl_ip_vs_amemthresh /
+			    (sysctl_ip_vs_amemthresh - availmem);
 		} else {
-			ipvs->drop_rate = 0;
-			ipvs->sysctl_drop_packet = 1;
+			ip_vs_drop_rate = 0;
+			sysctl_ip_vs_drop_packet = 1;
 		}
 		break;
 	case 3:
-		ipvs->drop_rate = ipvs->sysctl_am_droprate;
+		ip_vs_drop_rate = sysctl_ip_vs_am_droprate;
 		break;
 	}
-	spin_unlock(&ipvs->droppacket_lock);
+	spin_unlock(&__ip_vs_droppacket_lock);
 
 	/* secure_tcp */
-	spin_lock(&ipvs->securetcp_lock);
-	switch (ipvs->sysctl_secure_tcp) {
+	write_lock(&__ip_vs_securetcp_lock);
+	switch (sysctl_ip_vs_secure_tcp) {
 	case 0:
 		if (old_secure_tcp >= 2)
 			to_change = 0;
@@ -185,7 +268,7 @@ static void update_defense_level(struct netns_ipvs *ipvs)
 		if (nomem) {
 			if (old_secure_tcp < 2)
 				to_change = 1;
-			ipvs->sysctl_secure_tcp = 2;
+			sysctl_ip_vs_secure_tcp = 2;
 		} else {
 			if (old_secure_tcp >= 2)
 				to_change = 0;
@@ -198,7 +281,7 @@ static void update_defense_level(struct netns_ipvs *ipvs)
 		} else {
 			if (old_secure_tcp >= 2)
 				to_change = 0;
-			ipvs->sysctl_secure_tcp = 1;
+			sysctl_ip_vs_secure_tcp = 1;
 		}
 		break;
 	case 3:
@@ -206,46 +289,40 @@ static void update_defense_level(struct netns_ipvs *ipvs)
 			to_change = 1;
 		break;
 	}
-	old_secure_tcp = ipvs->sysctl_secure_tcp;
+	old_secure_tcp = sysctl_ip_vs_secure_tcp;
 	if (to_change >= 0)
-		ip_vs_protocol_timeout_change(ipvs,
-					      ipvs->sysctl_secure_tcp > 1);
-	spin_unlock(&ipvs->securetcp_lock);
+		ip_vs_protocol_timeout_change(sysctl_ip_vs_secure_tcp > 1);
+	write_unlock(&__ip_vs_securetcp_lock);
 
 	local_bh_enable();
 }
 
-
 /*
  *	Timer for checking the defense
  */
 #define DEFENSE_TIMER_PERIOD	1*HZ
+static void defense_work_handler(struct work_struct *work);
+static DECLARE_DELAYED_WORK(defense_work, defense_work_handler);
 
 static void defense_work_handler(struct work_struct *work)
 {
-	struct netns_ipvs *ipvs =
-		container_of(work, struct netns_ipvs, defense_work.work);
+	update_defense_level();
+	if (atomic_read(&ip_vs_dropentry))
+		ip_vs_random_dropentry();
 
-	update_defense_level(ipvs);
-	if (atomic_read(&ipvs->dropentry))
-		ip_vs_random_dropentry(ipvs->net);
-	schedule_delayed_work(&ipvs->defense_work, DEFENSE_TIMER_PERIOD);
+	schedule_delayed_work(&defense_work, DEFENSE_TIMER_PERIOD);
 }
-#endif
 
-int
-ip_vs_use_count_inc(void)
+int ip_vs_use_count_inc(void)
 {
 	return try_module_get(THIS_MODULE);
 }
 
-void
-ip_vs_use_count_dec(void)
+void ip_vs_use_count_dec(void)
 {
 	module_put(THIS_MODULE);
 }
 
-
 /*
  *	Hash table: for virtual service lookups
  */
@@ -254,50 +331,63 @@ ip_vs_use_count_dec(void)
 #define IP_VS_SVC_TAB_MASK (IP_VS_SVC_TAB_SIZE - 1)
 
 /* the service table hashed by <protocol, addr, port> */
-static struct hlist_head ip_vs_svc_table[IP_VS_SVC_TAB_SIZE];
+static struct list_head ip_vs_svc_table[IP_VS_SVC_TAB_SIZE];
 /* the service table hashed by fwmark */
-static struct hlist_head ip_vs_svc_fwm_table[IP_VS_SVC_TAB_SIZE];
+static struct list_head ip_vs_svc_fwm_table[IP_VS_SVC_TAB_SIZE];
+
+/*
+ *	Hash table: for real service lookups
+ */
+#define IP_VS_RTAB_BITS 4
+#define IP_VS_RTAB_SIZE (1 << IP_VS_RTAB_BITS)
+#define IP_VS_RTAB_MASK (IP_VS_RTAB_SIZE - 1)
+
+static struct list_head ip_vs_rtable[IP_VS_RTAB_SIZE];
+
+/*
+ *	Trash for destinations
+ */
+static LIST_HEAD(ip_vs_dest_trash);
 
+/*
+ *	FTP & NULL virtual service counters
+ */
+static atomic_t ip_vs_ftpsvc_counter = ATOMIC_INIT(0);
+static atomic_t ip_vs_nullsvc_counter = ATOMIC_INIT(0);
 
 /*
  *	Returns hash value for virtual service
  */
-static inline unsigned int
-ip_vs_svc_hashkey(struct net *net, int af, unsigned int proto,
-		  const union nf_inet_addr *addr, __be16 port)
+static __inline__ unsigned
+ip_vs_svc_hashkey(int af, unsigned proto, const union nf_inet_addr *addr)
 {
-	register unsigned int porth = ntohs(port);
 	__be32 addr_fold = addr->ip;
-	__u32 ahash;
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
-	ahash = ntohl(addr_fold);
-	ahash ^= ((size_t) net >> 8);
 
-	return (proto ^ ahash ^ (porth >> IP_VS_SVC_TAB_BITS) ^ porth) &
-	       IP_VS_SVC_TAB_MASK;
+	return (proto ^ ntohl(addr_fold)) & IP_VS_SVC_TAB_MASK;
 }
 
 /*
  *	Returns hash value of fwmark for virtual service lookup
  */
-static inline unsigned int ip_vs_svc_fwm_hashkey(struct net *net, __u32 fwmark)
+static __inline__ unsigned ip_vs_svc_fwm_hashkey(__u32 fwmark)
 {
-	return (((size_t)net>>8) ^ fwmark) & IP_VS_SVC_TAB_MASK;
+	return fwmark & IP_VS_SVC_TAB_MASK;
 }
 
 /*
- *	Hashes a service in the ip_vs_svc_table by <netns,proto,addr,port>
+ *	Hashes a service in the ip_vs_svc_table by <proto,addr,port>
  *	or in the ip_vs_svc_fwm_table by fwmark.
  *	Should be called with locked tables.
  */
 static int ip_vs_svc_hash(struct ip_vs_service *svc)
 {
-	unsigned int hash;
+	unsigned hash;
 
 	if (svc->flags & IP_VS_SVC_F_HASHED) {
 		pr_err("%s(): request for already hashed, called from %pF\n",
@@ -307,17 +397,16 @@ static int ip_vs_svc_hash(struct ip_vs_service *svc)
 
 	if (svc->fwmark == 0) {
 		/*
-		 *  Hash it by <netns,protocol,addr,port> in ip_vs_svc_table
+		 *  Hash it by <protocol,addr,port> in ip_vs_svc_table
 		 */
-		hash = ip_vs_svc_hashkey(svc->net, svc->af, svc->protocol,
-					 &svc->addr, svc->port);
-		hlist_add_head_rcu(&svc->s_list, &ip_vs_svc_table[hash]);
+		hash = ip_vs_svc_hashkey(svc->af, svc->protocol, &svc->addr);
+		list_add(&svc->s_list, &ip_vs_svc_table[hash]);
 	} else {
 		/*
-		 *  Hash it by fwmark in svc_fwm_table
+		 *  Hash it by fwmark in ip_vs_svc_fwm_table
 		 */
-		hash = ip_vs_svc_fwm_hashkey(svc->net, svc->fwmark);
-		hlist_add_head_rcu(&svc->f_list, &ip_vs_svc_fwm_table[hash]);
+		hash = ip_vs_svc_fwm_hashkey(svc->fwmark);
+		list_add(&svc->f_list, &ip_vs_svc_fwm_table[hash]);
 	}
 
 	svc->flags |= IP_VS_SVC_F_HASHED;
@@ -326,9 +415,8 @@ static int ip_vs_svc_hash(struct ip_vs_service *svc)
 	return 1;
 }
 
-
 /*
- *	Unhashes a service from svc_table / svc_fwm_table.
+ *	Unhashes a service from ip_vs_svc_table/ip_vs_svc_fwm_table.
  *	Should be called with locked tables.
  */
 static int ip_vs_svc_unhash(struct ip_vs_service *svc)
@@ -340,11 +428,11 @@ static int ip_vs_svc_unhash(struct ip_vs_service *svc)
 	}
 
 	if (svc->fwmark == 0) {
-		/* Remove it from the svc_table table */
-		hlist_del_rcu(&svc->s_list);
+		/* Remove it from the ip_vs_svc_table table */
+		list_del(&svc->s_list);
 	} else {
-		/* Remove it from the svc_fwm_table table */
-		hlist_del_rcu(&svc->f_list);
+		/* Remove it from the ip_vs_svc_fwm_table table */
+		list_del(&svc->f_list);
 	}
 
 	svc->flags &= ~IP_VS_SVC_F_HASHED;
@@ -352,27 +440,26 @@ static int ip_vs_svc_unhash(struct ip_vs_service *svc)
 	return 1;
 }
 
-
 /*
- *	Get service by {netns, proto,addr,port} in the service table.
+ *	Get service by {proto,addr,port} in the service table.
  */
-static inline struct ip_vs_service *
-__ip_vs_service_find(struct net *net, int af, __u16 protocol,
-		     const union nf_inet_addr *vaddr, __be16 vport)
+static inline struct ip_vs_service *__ip_vs_service_get(int af, __u16 protocol,
+							const union nf_inet_addr
+							*vaddr, __be16 vport)
 {
-	unsigned int hash;
+	unsigned hash;
 	struct ip_vs_service *svc;
 
 	/* Check for "full" addressed entries */
-	hash = ip_vs_svc_hashkey(net, af, protocol, vaddr, vport);
+	hash = ip_vs_svc_hashkey(af, protocol, vaddr);
 
-	hlist_for_each_entry_rcu(svc, &ip_vs_svc_table[hash], s_list) {
+	list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
 		if ((svc->af == af)
 		    && ip_vs_addr_equal(af, &svc->addr, vaddr)
 		    && (svc->port == vport)
-		    && (svc->protocol == protocol)
-		    && net_eq(svc->net, net)) {
+		    && (svc->protocol == protocol)) {
 			/* HIT */
+			atomic_inc(&svc->usecnt);
 			return svc;
 		}
 	}
@@ -380,23 +467,21 @@ __ip_vs_service_find(struct net *net, int af, __u16 protocol,
 	return NULL;
 }
 
-
 /*
  *	Get service by {fwmark} in the service table.
  */
-static inline struct ip_vs_service *
-__ip_vs_svc_fwm_find(struct net *net, int af, __u32 fwmark)
+static inline struct ip_vs_service *__ip_vs_svc_fwm_get(int af, __u32 fwmark)
 {
-	unsigned int hash;
+	unsigned hash;
 	struct ip_vs_service *svc;
 
 	/* Check for fwmark addressed entries */
-	hash = ip_vs_svc_fwm_hashkey(net, fwmark);
+	hash = ip_vs_svc_fwm_hashkey(fwmark);
 
-	hlist_for_each_entry_rcu(svc, &ip_vs_svc_fwm_table[hash], f_list) {
-		if (svc->fwmark == fwmark && svc->af == af
-		    && net_eq(svc->net, net)) {
+	list_for_each_entry(svc, &ip_vs_svc_fwm_table[hash], f_list) {
+		if (svc->fwmark == fwmark && svc->af == af) {
 			/* HIT */
+			atomic_inc(&svc->usecnt);
 			return svc;
 		}
 	}
@@ -404,49 +489,46 @@ __ip_vs_svc_fwm_find(struct net *net, int af, __u32 fwmark)
 	return NULL;
 }
 
-/* Find service, called under RCU lock */
-struct ip_vs_service *
-ip_vs_service_find(struct net *net, int af, __u32 fwmark, __u16 protocol,
-		   const union nf_inet_addr *vaddr, __be16 vport)
+struct ip_vs_service *ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
+					const union nf_inet_addr *vaddr,
+					__be16 vport)
 {
 	struct ip_vs_service *svc;
-	struct netns_ipvs *ipvs = net_ipvs(net);
+
+	read_lock(&__ip_vs_svc_lock);
 
 	/*
-	 *	Check the table hashed by fwmark first
+	 *      Check the table hashed by fwmark first
 	 */
-	if (fwmark) {
-		svc = __ip_vs_svc_fwm_find(net, af, fwmark);
-		if (svc)
-			goto out;
-	}
+	if (fwmark && (svc = __ip_vs_svc_fwm_get(af, fwmark)))
+		goto out;
 
 	/*
-	 *	Check the table hashed by <protocol,addr,port>
-	 *	for "full" addressed entries
+	 *      Check the table hashed by <protocol,addr,port>
+	 *      for "full" addressed entries
 	 */
-	svc = __ip_vs_service_find(net, af, protocol, vaddr, vport);
+	svc = __ip_vs_service_get(af, protocol, vaddr, vport);
 
 	if (svc == NULL
-	    && protocol == IPPROTO_TCP
-	    && atomic_read(&ipvs->ftpsvc_counter)
+	    && protocol == IPPROTO_TCP && atomic_read(&ip_vs_ftpsvc_counter)
 	    && (vport == FTPDATA || ntohs(vport) >= PROT_SOCK)) {
 		/*
 		 * Check if ftp service entry exists, the packet
 		 * might belong to FTP data connections.
 		 */
-		svc = __ip_vs_service_find(net, af, protocol, vaddr, FTPPORT);
+		svc = __ip_vs_service_get(af, protocol, vaddr, FTPPORT);
 	}
 
-	if (svc == NULL
-	    && atomic_read(&ipvs->nullsvc_counter)) {
+	if (svc == NULL && atomic_read(&ip_vs_nullsvc_counter)) {
 		/*
 		 * Check if the catch-all port (port zero) exists
 		 */
-		svc = __ip_vs_service_find(net, af, protocol, vaddr, 0);
+		svc = __ip_vs_service_get(af, protocol, vaddr, 0);
 	}
 
-  out:
+      out:
+	read_unlock(&__ip_vs_svc_lock);
+
 	IP_VS_DBG_BUF(9, "lookup service: fwm %u %s %s:%u %s\n",
 		      fwmark, ip_vs_proto_name(protocol),
 		      IP_VS_DBG_ADDR(af, vaddr), ntohs(vport),
@@ -455,6 +537,28 @@ ip_vs_service_find(struct net *net, int af, __u32 fwmark, __u16 protocol,
 	return svc;
 }
 
+struct ip_vs_service *ip_vs_lookup_vip(int af, __u16 protocol,
+				       const union nf_inet_addr *vaddr)
+{
+	struct ip_vs_service *svc;
+	unsigned hash;
+
+	read_lock(&__ip_vs_svc_lock);
+
+	hash = ip_vs_svc_hashkey(af, protocol, vaddr);
+	list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+		if ((svc->af == af)
+		    && ip_vs_addr_equal(af, &svc->addr, vaddr)
+		    && (svc->protocol == protocol)) {
+			/* HIT */
+			read_unlock(&__ip_vs_svc_lock);
+			return svc;
+		}
+	}
+
+	read_unlock(&__ip_vs_svc_lock);
+	return NULL;
+}
 
 static inline void
 __ip_vs_bind_svc(struct ip_vs_dest *dest, struct ip_vs_service *svc)
@@ -463,119 +567,120 @@ __ip_vs_bind_svc(struct ip_vs_dest *dest, struct ip_vs_service *svc)
 	dest->svc = svc;
 }
 
-static void ip_vs_service_free(struct ip_vs_service *svc)
-{
-	if (svc->stats.cpustats)
-		free_percpu(svc->stats.cpustats);
-	kfree(svc);
-}
-
-static void
-__ip_vs_unbind_svc(struct ip_vs_dest *dest)
+static inline void __ip_vs_unbind_svc(struct ip_vs_dest *dest)
 {
 	struct ip_vs_service *svc = dest->svc;
 
 	dest->svc = NULL;
-	if (atomic_dec_and_test(&svc->refcnt)) {
-		IP_VS_DBG_BUF(3, "Removing service %u/%s:%u\n",
-			      svc->fwmark,
-			      IP_VS_DBG_ADDR(svc->af, &svc->addr),
-			      ntohs(svc->port));
-		ip_vs_service_free(svc);
-	}
+	if (atomic_dec_and_test(&svc->refcnt))
+		kfree(svc);
 }
 
-
 /*
  *	Returns hash value for real service
  */
-static inline unsigned int ip_vs_rs_hashkey(int af,
-					    const union nf_inet_addr *addr,
-					    __be16 port)
+static inline unsigned ip_vs_rs_hashkey(int af,
+					const union nf_inet_addr *addr,
+					__be16 port)
 {
-	register unsigned int porth = ntohs(port);
+	register unsigned porth = ntohs(port);
 	__be32 addr_fold = addr->ip;
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
 
-	return (ntohl(addr_fold)^(porth>>IP_VS_RTAB_BITS)^porth)
-		& IP_VS_RTAB_MASK;
+	return (ntohl(addr_fold) ^ (porth >> IP_VS_RTAB_BITS) ^ porth)
+	    & IP_VS_RTAB_MASK;
 }
 
-/* Hash ip_vs_dest in rs_table by <proto,addr,port>. */
-static void ip_vs_rs_hash(struct netns_ipvs *ipvs, struct ip_vs_dest *dest)
+/*
+ *	Hashes ip_vs_dest in ip_vs_rtable by <proto,addr,port>.
+ *	should be called with locked tables.
+ */
+static int ip_vs_rs_hash(struct ip_vs_dest *dest)
 {
-	unsigned int hash;
+	unsigned hash;
 
-	if (dest->in_rs_table)
-		return;
+	if (!list_empty(&dest->d_list)) {
+		return 0;
+	}
 
 	/*
-	 *	Hash by proto,addr,port,
-	 *	which are the parameters of the real service.
+	 *      Hash by proto,addr,port,
+	 *      which are the parameters of the real service.
 	 */
 	hash = ip_vs_rs_hashkey(dest->af, &dest->addr, dest->port);
 
-	hlist_add_head_rcu(&dest->d_list, &ipvs->rs_table[hash]);
-	dest->in_rs_table = 1;
+	list_add(&dest->d_list, &ip_vs_rtable[hash]);
+
+	return 1;
 }
 
-/* Unhash ip_vs_dest from rs_table. */
-static void ip_vs_rs_unhash(struct ip_vs_dest *dest)
+/*
+ *	UNhashes ip_vs_dest from ip_vs_rtable.
+ *	should be called with locked tables.
+ */
+static int ip_vs_rs_unhash(struct ip_vs_dest *dest)
 {
 	/*
-	 * Remove it from the rs_table table.
+	 * Remove it from the ip_vs_rtable table.
 	 */
-	if (dest->in_rs_table) {
-		hlist_del_rcu(&dest->d_list);
-		dest->in_rs_table = 0;
+	if (!list_empty(&dest->d_list)) {
+		list_del(&dest->d_list);
+		INIT_LIST_HEAD(&dest->d_list);
 	}
+
+	return 1;
 }
 
-/* Check if real service by <proto,addr,port> is present */
-bool ip_vs_has_real_service(struct net *net, int af, __u16 protocol,
-			    const union nf_inet_addr *daddr, __be16 dport)
+/*
+ *	Lookup real service by <proto,addr,port> in the real service table.
+ */
+struct ip_vs_dest *ip_vs_lookup_real_service(int af, __u16 protocol,
+					     const union nf_inet_addr *daddr,
+					     __be16 dport)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	unsigned int hash;
+	unsigned hash;
 	struct ip_vs_dest *dest;
 
-	/* Check for "full" addressed entries */
+	/*
+	 *      Check for "full" addressed entries
+	 *      Return the first found entry
+	 */
 	hash = ip_vs_rs_hashkey(af, daddr, dport);
 
-	rcu_read_lock();
-	hlist_for_each_entry_rcu(dest, &ipvs->rs_table[hash], d_list) {
-		if (dest->port == dport &&
-		    dest->af == af &&
-		    ip_vs_addr_equal(af, &dest->addr, daddr) &&
-		    (dest->protocol == protocol || dest->vfwmark)) {
+	read_lock(&__ip_vs_rs_lock);
+	list_for_each_entry(dest, &ip_vs_rtable[hash], d_list) {
+		if ((dest->af == af)
+		    && ip_vs_addr_equal(af, &dest->addr, daddr)
+		    && (dest->port == dport)
+		    && ((dest->protocol == protocol) || dest->vfwmark)) {
 			/* HIT */
-			rcu_read_unlock();
-			return true;
+			read_unlock(&__ip_vs_rs_lock);
+			return dest;
 		}
 	}
-	rcu_read_unlock();
+	read_unlock(&__ip_vs_rs_lock);
 
-	return false;
+	return NULL;
 }
 
-/* Lookup destination by {addr,port} in the given service
- * Called under RCU lock.
+/*
+ *	Lookup destination by {addr,port} in the given service
  */
-static struct ip_vs_dest *
-ip_vs_lookup_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
-		  __be16 dport)
+static struct ip_vs_dest *ip_vs_lookup_dest(struct ip_vs_service *svc,
+					    const union nf_inet_addr *daddr,
+					    __be16 dport)
 {
 	struct ip_vs_dest *dest;
 
 	/*
 	 * Find the destination for the given service
 	 */
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 		if ((dest->af == svc->af)
 		    && ip_vs_addr_equal(svc->af, &dest->addr, daddr)
 		    && (dest->port == dport)) {
@@ -589,56 +694,32 @@ ip_vs_lookup_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
 
 /*
  * Find destination by {daddr,dport,vaddr,protocol}
- * Created to be used in ip_vs_process_message() in
+ * Cretaed to be used in ip_vs_process_message() in
  * the backup synchronization daemon. It finds the
  * destination to be bound to the received connection
  * on the backup.
- * Called under RCU lock, no refcnt is returned.
+ *
+ * ip_vs_lookup_real_service() looked promissing, but
+ * seems not working as expected.
  */
-struct ip_vs_dest *ip_vs_find_dest(struct net  *net, int af,
-				   const union nf_inet_addr *daddr,
+struct ip_vs_dest *ip_vs_find_dest(int af, const union nf_inet_addr *daddr,
 				   __be16 dport,
 				   const union nf_inet_addr *vaddr,
-				   __be16 vport, __u16 protocol, __u32 fwmark,
-				   __u32 flags)
+				   __be16 vport, __u16 protocol)
 {
 	struct ip_vs_dest *dest;
 	struct ip_vs_service *svc;
-	__be16 port = dport;
 
-	svc = ip_vs_service_find(net, af, fwmark, protocol, vaddr, vport);
+	svc = ip_vs_service_get(af, 0, protocol, vaddr, vport);
 	if (!svc)
 		return NULL;
-	if (fwmark && (flags & IP_VS_CONN_F_FWD_MASK) != IP_VS_CONN_F_MASQ)
-		port = 0;
-	dest = ip_vs_lookup_dest(svc, daddr, port);
-	if (!dest)
-		dest = ip_vs_lookup_dest(svc, daddr, port ^ dport);
+	dest = ip_vs_lookup_dest(svc, daddr, dport);
+	if (dest)
+		atomic_inc(&dest->refcnt);
+	ip_vs_service_put(svc);
 	return dest;
 }
 
-void ip_vs_dest_dst_rcu_free(struct rcu_head *head)
-{
-	struct ip_vs_dest_dst *dest_dst = container_of(head,
-						       struct ip_vs_dest_dst,
-						       rcu_head);
-
-	dst_release(dest_dst->dst_cache);
-	kfree(dest_dst);
-}
-
-/* Release dest_dst and dst_cache for dest in user context */
-static void __ip_vs_dst_cache_reset(struct ip_vs_dest *dest)
-{
-	struct ip_vs_dest_dst *old;
-
-	old = rcu_dereference_protected(dest->dest_dst, 1);
-	if (old) {
-		RCU_INIT_POINTER(dest->dest_dst, NULL);
-		call_rcu(&old->rcu_head, ip_vs_dest_dst_rcu_free);
-	}
-}
-
 /*
  *  Lookup dest by {svc,addr,port} in the destination trash.
  *  The destination trash is used to hold the destinations that are removed
@@ -649,29 +730,21 @@ static void __ip_vs_dst_cache_reset(struct ip_vs_dest *dest)
  *  continue, and the counting information of the dest is also useful for
  *  scheduling.
  */
-static struct ip_vs_dest *
-ip_vs_trash_get_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
-		     __be16 dport)
+static struct ip_vs_dest *ip_vs_trash_get_dest(struct ip_vs_service *svc,
+					       const union nf_inet_addr *daddr,
+					       __be16 dport)
 {
-	struct ip_vs_dest *dest;
-	struct netns_ipvs *ipvs = net_ipvs(svc->net);
+	struct ip_vs_dest *dest, *nxt;
 
 	/*
 	 * Find the destination in trash
 	 */
-	spin_lock_bh(&ipvs->dest_trash_lock);
-	list_for_each_entry(dest, &ipvs->dest_trash, t_list) {
+	list_for_each_entry_safe(dest, nxt, &ip_vs_dest_trash, n_list) {
 		IP_VS_DBG_BUF(3, "Destination %u/%s:%u still in trash, "
 			      "dest->refcnt=%d\n",
 			      dest->vfwmark,
 			      IP_VS_DBG_ADDR(svc->af, &dest->addr),
-			      ntohs(dest->port),
-			      atomic_read(&dest->refcnt));
-		/* We can not reuse dest while in grace period
-		 * because conns still can use dest->svc
-		 */
-		if (test_bit(IP_VS_DEST_STATE_REMOVING, &dest->state))
-			continue;
+			      ntohs(dest->port), atomic_read(&dest->refcnt));
 		if (dest->af == svc->af &&
 		    ip_vs_addr_equal(svc->af, &dest->addr, daddr) &&
 		    dest->port == dport &&
@@ -681,26 +754,30 @@ ip_vs_trash_get_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
 		     (ip_vs_addr_equal(svc->af, &dest->vaddr, &svc->addr) &&
 		      dest->vport == svc->port))) {
 			/* HIT */
-			list_del(&dest->t_list);
-			ip_vs_dest_hold(dest);
-			goto out;
+			return dest;
 		}
-	}
 
-	dest = NULL;
+		/*
+		 * Try to purge the destination from trash if not referenced
+		 */
+		if (atomic_read(&dest->refcnt) == 1) {
+			IP_VS_DBG_BUF(3, "Removing destination %u/%s:%u "
+				      "from trash\n",
+				      dest->vfwmark,
+				      IP_VS_DBG_ADDR(svc->af, &dest->addr),
+				      ntohs(dest->port));
+			list_del(&dest->n_list);
+			ip_vs_dst_reset(dest);
+			__ip_vs_unbind_svc(dest);
 
-out:
-	spin_unlock_bh(&ipvs->dest_trash_lock);
+			/* Delete dest dedicated statistic varible which is percpu type */
+			ip_vs_del_stats(dest->stats);
 
-	return dest;
-}
+			kfree(dest);
+		}
+	}
 
-static void ip_vs_dest_free(struct ip_vs_dest *dest)
-{
-	__ip_vs_dst_cache_reset(dest);
-	__ip_vs_unbind_svc(dest);
-	free_percpu(dest->stats.cpustats);
-	kfree(dest);
+	return NULL;
 }
 
 /*
@@ -710,84 +787,59 @@ static void ip_vs_dest_free(struct ip_vs_dest *dest)
  *  When the ip_vs_control_clearup is activated by ipvs module exit,
  *  the service tables must have been flushed and all the connections
  *  are expired, and the refcnt of each destination in the trash must
- *  be 0, so we simply release them here.
+ *  be 1, so we simply release them here.
  */
-static void ip_vs_trash_cleanup(struct net *net)
+static void ip_vs_trash_cleanup(void)
 {
 	struct ip_vs_dest *dest, *nxt;
-	struct netns_ipvs *ipvs = net_ipvs(net);
 
-	del_timer_sync(&ipvs->dest_trash_timer);
-	/* No need to use dest_trash_lock */
-	list_for_each_entry_safe(dest, nxt, &ipvs->dest_trash, t_list) {
-		list_del(&dest->t_list);
-		ip_vs_dest_free(dest);
+	list_for_each_entry_safe(dest, nxt, &ip_vs_dest_trash, n_list) {
+		list_del(&dest->n_list);
+		ip_vs_dst_reset(dest);
+		__ip_vs_unbind_svc(dest);
+		ip_vs_del_stats(dest->stats);
+		kfree(dest);
 	}
 }
 
-static void
-ip_vs_copy_stats(struct ip_vs_stats_user *dst, struct ip_vs_stats *src)
-{
-#define IP_VS_SHOW_STATS_COUNTER(c) dst->c = src->ustats.c - src->ustats0.c
-
-	spin_lock_bh(&src->lock);
-
-	IP_VS_SHOW_STATS_COUNTER(conns);
-	IP_VS_SHOW_STATS_COUNTER(inpkts);
-	IP_VS_SHOW_STATS_COUNTER(outpkts);
-	IP_VS_SHOW_STATS_COUNTER(inbytes);
-	IP_VS_SHOW_STATS_COUNTER(outbytes);
-
-	ip_vs_read_estimator(dst, src);
-
-	spin_unlock_bh(&src->lock);
-}
-
-static void
-ip_vs_zero_stats(struct ip_vs_stats *stats)
-{
-	spin_lock_bh(&stats->lock);
-
-	/* get current counters as zero point, rates are zeroed */
-
-#define IP_VS_ZERO_STATS_COUNTER(c) stats->ustats0.c = stats->ustats.c
-
-	IP_VS_ZERO_STATS_COUNTER(conns);
-	IP_VS_ZERO_STATS_COUNTER(inpkts);
-	IP_VS_ZERO_STATS_COUNTER(outpkts);
-	IP_VS_ZERO_STATS_COUNTER(inbytes);
-	IP_VS_ZERO_STATS_COUNTER(outbytes);
-
-	ip_vs_zero_estimator(stats);
-
-	spin_unlock_bh(&stats->lock);
-}
-
 /*
  *	Update a destination in the given service
  */
 static void
-__ip_vs_update_dest(struct ip_vs_service *svc, struct ip_vs_dest *dest,
-		    struct ip_vs_dest_user_kern *udest, int add)
+__ip_vs_update_dest(struct ip_vs_service *svc,
+		    struct ip_vs_dest *dest, struct ip_vs_dest_user_kern *udest)
 {
-	struct netns_ipvs *ipvs = net_ipvs(svc->net);
-	struct ip_vs_scheduler *sched;
 	int conn_flags;
 
 	/* set the weight and the flags */
 	atomic_set(&dest->weight, udest->weight);
-	conn_flags = udest->conn_flags & IP_VS_CONN_F_DEST_MASK;
-	conn_flags |= IP_VS_CONN_F_INACTIVE;
+	conn_flags = udest->conn_flags | IP_VS_CONN_F_INACTIVE;
+
+	/* check if local node and update the flags */
+#ifdef CONFIG_IP_VS_IPV6
+	if (svc->af == AF_INET6) {
+		if (__ip_vs_addr_is_local_v6(&udest->addr.in6)) {
+			conn_flags = (conn_flags & ~IP_VS_CONN_F_FWD_MASK)
+			    | IP_VS_CONN_F_LOCALNODE;
+		}
+	} else
+#endif
+	if (inet_addr_type(&init_net, udest->addr.ip) == RTN_LOCAL) {
+		conn_flags = (conn_flags & ~IP_VS_CONN_F_FWD_MASK)
+		    | IP_VS_CONN_F_LOCALNODE;
+	}
 
 	/* set the IP_VS_CONN_F_NOOUTPUT flag if not masquerading/NAT */
-	if ((conn_flags & IP_VS_CONN_F_FWD_MASK) != IP_VS_CONN_F_MASQ) {
+	if ((conn_flags & IP_VS_CONN_F_FWD_MASK) != 0) {
 		conn_flags |= IP_VS_CONN_F_NOOUTPUT;
 	} else {
 		/*
-		 *    Put the real service in rs_table if not present.
+		 *    Put the real service in ip_vs_rtable if not present.
 		 *    For now only for NAT!
 		 */
-		ip_vs_rs_hash(ipvs, dest);
+		write_lock_bh(&__ip_vs_rs_lock);
+		ip_vs_rs_hash(dest);
+		write_unlock_bh(&__ip_vs_rs_lock);
 	}
 	atomic_set(&dest->conn_flags, conn_flags);
 
@@ -797,7 +849,7 @@ __ip_vs_update_dest(struct ip_vs_service *svc, struct ip_vs_dest *dest,
 	} else {
 		if (dest->svc != svc) {
 			__ip_vs_unbind_svc(dest);
-			ip_vs_zero_stats(&dest->stats);
+			ip_vs_zero_stats(dest->stats);
 			__ip_vs_bind_svc(dest, svc);
 		}
 	}
@@ -809,25 +861,8 @@ __ip_vs_update_dest(struct ip_vs_service *svc, struct ip_vs_dest *dest,
 		dest->flags &= ~IP_VS_DEST_F_OVERLOAD;
 	dest->u_threshold = udest->u_threshold;
 	dest->l_threshold = udest->l_threshold;
-
-	spin_lock_bh(&dest->dst_lock);
-	__ip_vs_dst_cache_reset(dest);
-	spin_unlock_bh(&dest->dst_lock);
-
-	sched = rcu_dereference_protected(svc->scheduler, 1);
-	if (add) {
-		ip_vs_start_estimator(svc->net, &dest->stats);
-		list_add_rcu(&dest->n_list, &svc->destinations);
-		svc->num_dests++;
-		if (sched->add_dest)
-			sched->add_dest(svc, dest);
-	} else {
-		if (sched->upd_dest)
-			sched->upd_dest(svc, dest);
-	}
 }
 
-
 /*
  *	Create a destination for the given service
  */
@@ -835,8 +870,9 @@ static int
 ip_vs_new_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest,
 	       struct ip_vs_dest **dest_p)
 {
+	int ret = 0;
 	struct ip_vs_dest *dest;
-	unsigned int atype;
+	unsigned atype;
 
 	EnterFunction(2);
 
@@ -844,24 +880,22 @@ ip_vs_new_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest,
 	if (svc->af == AF_INET6) {
 		atype = ipv6_addr_type(&udest->addr.in6);
 		if ((!(atype & IPV6_ADDR_UNICAST) ||
-			atype & IPV6_ADDR_LINKLOCAL) &&
-			!__ip_vs_addr_is_local_v6(svc->net, &udest->addr.in6))
+		     atype & IPV6_ADDR_LINKLOCAL) &&
+		    !__ip_vs_addr_is_local_v6(&udest->addr.in6))
 			return -EINVAL;
 	} else
 #endif
 	{
-		atype = inet_addr_type(svc->net, udest->addr.ip);
+		atype = inet_addr_type(&init_net, udest->addr.ip);
 		if (atype != RTN_LOCAL && atype != RTN_UNICAST)
 			return -EINVAL;
 	}
 
-	dest = kzalloc(sizeof(struct ip_vs_dest), GFP_KERNEL);
-	if (dest == NULL)
+	dest = kzalloc(sizeof(struct ip_vs_dest), GFP_ATOMIC);
+	if (dest == NULL) {
+		pr_err("%s(): no memory.\n", __func__);
 		return -ENOMEM;
-
-	dest->stats.cpustats = alloc_percpu(struct ip_vs_cpu_stats);
-	if (!dest->stats.cpustats)
-		goto err_alloc;
+	}
 
 	dest->af = svc->af;
 	dest->protocol = svc->protocol;
@@ -874,24 +908,29 @@ ip_vs_new_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest,
 	atomic_set(&dest->activeconns, 0);
 	atomic_set(&dest->inactconns, 0);
 	atomic_set(&dest->persistconns, 0);
-	atomic_set(&dest->refcnt, 1);
+	atomic_set(&dest->refcnt, 0);
 
-	INIT_HLIST_NODE(&dest->d_list);
+	INIT_LIST_HEAD(&dest->d_list);
 	spin_lock_init(&dest->dst_lock);
-	spin_lock_init(&dest->stats.lock);
-	__ip_vs_update_dest(svc, dest, udest, 1);
+
+	/* Init statistic */
+	ret = ip_vs_new_stats(&(dest->stats));
+	if(ret)
+		goto out_err;
+
+	__ip_vs_update_dest(svc, dest, udest);
+
 
 	*dest_p = dest;
 
 	LeaveFunction(2);
 	return 0;
 
-err_alloc:
+out_err:
 	kfree(dest);
-	return -ENOMEM;
+	return ret;
 }
 
-
 /*
  *	Add a destination into an existing service
  */
@@ -912,16 +951,16 @@ ip_vs_add_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
 
 	if (udest->l_threshold > udest->u_threshold) {
 		pr_err("%s(): lower threshold is higher than upper threshold\n",
-			__func__);
+		       __func__);
 		return -ERANGE;
 	}
 
 	ip_vs_addr_copy(svc->af, &daddr, &udest->addr);
 
-	/* We use function that requires RCU lock */
-	rcu_read_lock();
+	/*
+	 * Check if the dest already exists in the list
+	 */
 	dest = ip_vs_lookup_dest(svc, &daddr, dport);
-	rcu_read_unlock();
 
 	if (dest != NULL) {
 		IP_VS_DBG(1, "%s(): dest already exists\n", __func__);
@@ -943,20 +982,68 @@ ip_vs_add_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
 			      IP_VS_DBG_ADDR(svc->af, &dest->vaddr),
 			      ntohs(dest->vport));
 
-		__ip_vs_update_dest(svc, dest, udest, 1);
-		ret = 0;
-	} else {
+		__ip_vs_update_dest(svc, dest, udest);
+
 		/*
-		 * Allocate and initialize the dest structure
+		 * Get the destination from the trash
 		 */
-		ret = ip_vs_new_dest(svc, udest, &dest);
+		list_del(&dest->n_list);
+
+		/* Reset the statistic value */
+		ip_vs_zero_stats(dest->stats);
+
+		write_lock_bh(&__ip_vs_svc_lock);
+
+		/*
+		 * Wait until all other svc users go away.
+		 */
+		IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
+		list_add(&dest->n_list, &svc->destinations);
+		svc->num_dests++;
+
+		/* call the update_service function of its scheduler */
+		if (svc->scheduler->update_service)
+			svc->scheduler->update_service(svc);
+
+		write_unlock_bh(&__ip_vs_svc_lock);
+		return 0;
+	}
+
+	/*
+	 * Allocate and initialize the dest structure
+	 */
+	ret = ip_vs_new_dest(svc, udest, &dest);
+	if (ret) {
+		return ret;
 	}
+
+	/*
+	 * Add the dest entry into the list
+	 */
+	atomic_inc(&dest->refcnt);
+
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 * Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
+	list_add(&dest->n_list, &svc->destinations);
+	svc->num_dests++;
+
+	/* call the update_service function of its scheduler */
+	if (svc->scheduler->update_service)
+		svc->scheduler->update_service(svc);
+
+	write_unlock_bh(&__ip_vs_svc_lock);
+
 	LeaveFunction(2);
 
-	return ret;
+	return 0;
 }
 
-
 /*
  *	Edit a destination in the given service
  */
@@ -976,97 +1063,101 @@ ip_vs_edit_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
 
 	if (udest->l_threshold > udest->u_threshold) {
 		pr_err("%s(): lower threshold is higher than upper threshold\n",
-			__func__);
+		       __func__);
 		return -ERANGE;
 	}
 
 	ip_vs_addr_copy(svc->af, &daddr, &udest->addr);
 
-	/* We use function that requires RCU lock */
-	rcu_read_lock();
+	/*
+	 *  Lookup the destination list
+	 */
 	dest = ip_vs_lookup_dest(svc, &daddr, dport);
-	rcu_read_unlock();
 
 	if (dest == NULL) {
 		IP_VS_DBG(1, "%s(): dest doesn't exist\n", __func__);
 		return -ENOENT;
 	}
 
-	__ip_vs_update_dest(svc, dest, udest, 0);
-	LeaveFunction(2);
+	__ip_vs_update_dest(svc, dest, udest);
 
-	return 0;
-}
+	write_lock_bh(&__ip_vs_svc_lock);
 
-static void ip_vs_dest_wait_readers(struct rcu_head *head)
-{
-	struct ip_vs_dest *dest = container_of(head, struct ip_vs_dest,
-					       rcu_head);
+	/* Wait until all other svc users go away */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
 
-	/* End of grace period after unlinking */
-	clear_bit(IP_VS_DEST_STATE_REMOVING, &dest->state);
-}
+	/* call the update_service, because server weight may be changed */
+	if (svc->scheduler->update_service)
+		svc->scheduler->update_service(svc);
+
+	write_unlock_bh(&__ip_vs_svc_lock);
+
+	LeaveFunction(2);
 
+	return 0;
+}
 
 /*
  *	Delete a destination (must be already unlinked from the service)
  */
-static void __ip_vs_del_dest(struct net *net, struct ip_vs_dest *dest,
-			     bool cleanup)
+static void __ip_vs_del_dest(struct ip_vs_dest *dest)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	ip_vs_stop_estimator(net, &dest->stats);
-
 	/*
 	 *  Remove it from the d-linked list with the real services.
 	 */
+	write_lock_bh(&__ip_vs_rs_lock);
 	ip_vs_rs_unhash(dest);
+	write_unlock_bh(&__ip_vs_rs_lock);
 
-	if (!cleanup) {
-		set_bit(IP_VS_DEST_STATE_REMOVING, &dest->state);
-		call_rcu(&dest->rcu_head, ip_vs_dest_wait_readers);
+	/*
+	 *  Decrease the refcnt of the dest, and free the dest
+	 *  if nobody refers to it (refcnt=0). Otherwise, throw
+	 *  the destination into the trash.
+	 */
+	if (atomic_dec_and_test(&dest->refcnt)) {
+		ip_vs_dst_reset(dest);
+		/* simply decrease svc->refcnt here, let the caller check
+		   and release the service if nobody refers to it.
+		   Only user context can release destination and service,
+		   and only one user context can update virtual service at a
+		   time, so the operation here is OK */
+		atomic_dec(&dest->svc->refcnt);
+
+		/* Delete dest dedicated statistic varible which is percpu type */
+		ip_vs_del_stats(dest->stats);
+
+		kfree(dest);
+	} else {
+		IP_VS_DBG_BUF(3, "Moving dest %s:%u into trash, "
+			      "dest->refcnt=%d\n",
+			      IP_VS_DBG_ADDR(dest->af, &dest->addr),
+			      ntohs(dest->port), atomic_read(&dest->refcnt));
+		list_add(&dest->n_list, &ip_vs_dest_trash);
+		atomic_inc(&dest->refcnt);
 	}
-
-	spin_lock_bh(&ipvs->dest_trash_lock);
-	IP_VS_DBG_BUF(3, "Moving dest %s:%u into trash, dest->refcnt=%d\n",
-		      IP_VS_DBG_ADDR(dest->af, &dest->addr), ntohs(dest->port),
-		      atomic_read(&dest->refcnt));
-	if (list_empty(&ipvs->dest_trash) && !cleanup)
-		mod_timer(&ipvs->dest_trash_timer,
-			  jiffies + IP_VS_DEST_TRASH_PERIOD);
-	/* dest lives in trash without reference */
-	list_add(&dest->t_list, &ipvs->dest_trash);
-	spin_unlock_bh(&ipvs->dest_trash_lock);
-	ip_vs_dest_put(dest);
 }
 
-
 /*
  *	Unlink a destination from the given service
  */
 static void __ip_vs_unlink_dest(struct ip_vs_service *svc,
-				struct ip_vs_dest *dest,
-				int svcupd)
+				struct ip_vs_dest *dest, int svcupd)
 {
 	dest->flags &= ~IP_VS_DEST_F_AVAILABLE;
 
 	/*
 	 *  Remove it from the d-linked destination list.
 	 */
-	list_del_rcu(&dest->n_list);
+	list_del(&dest->n_list);
 	svc->num_dests--;
 
-	if (svcupd) {
-		struct ip_vs_scheduler *sched;
-
-		sched = rcu_dereference_protected(svc->scheduler, 1);
-		if (sched->del_dest)
-			sched->del_dest(svc, dest);
-	}
+	/*
+	 *  Call the update_service function of its scheduler
+	 */
+	if (svcupd && svc->scheduler->update_service)
+		svc->scheduler->update_service(svc);
 }
 
-
 /*
  *	Delete a destination server in the given service
  */
@@ -1078,115 +1169,236 @@ ip_vs_del_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
 
 	EnterFunction(2);
 
-	/* We use function that requires RCU lock */
-	rcu_read_lock();
 	dest = ip_vs_lookup_dest(svc, &udest->addr, dport);
-	rcu_read_unlock();
 
 	if (dest == NULL) {
 		IP_VS_DBG(1, "%s(): destination not found!\n", __func__);
 		return -ENOENT;
 	}
 
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 *      Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
 	/*
-	 *	Unlink dest from the service
+	 *      Unlink dest from the service
 	 */
 	__ip_vs_unlink_dest(svc, dest, 1);
 
+	write_unlock_bh(&__ip_vs_svc_lock);
+
 	/*
-	 *	Delete the destination
+	 *      Delete the destination
 	 */
-	__ip_vs_del_dest(svc->net, dest, false);
+	__ip_vs_del_dest(dest);
 
 	LeaveFunction(2);
 
 	return 0;
 }
 
-static void ip_vs_dest_trash_expire(unsigned long data)
+void ip_vs_laddr_hold(struct ip_vs_laddr *laddr)
 {
-	struct net *net = (struct net *) data;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_dest *dest, *next;
+	atomic_inc(&laddr->refcnt);
+}
 
-	spin_lock(&ipvs->dest_trash_lock);
-	list_for_each_entry_safe(dest, next, &ipvs->dest_trash, t_list) {
-		/* Skip if dest is in grace period */
-		if (test_bit(IP_VS_DEST_STATE_REMOVING, &dest->state))
-			continue;
-		if (atomic_read(&dest->refcnt) > 0)
-			continue;
-		IP_VS_DBG_BUF(3, "Removing destination %u/%s:%u from trash\n",
-			      dest->vfwmark,
-			      IP_VS_DBG_ADDR(dest->svc->af, &dest->addr),
-			      ntohs(dest->port));
-		list_del(&dest->t_list);
-		ip_vs_dest_free(dest);
+void ip_vs_laddr_put(struct ip_vs_laddr *laddr)
+{
+	if (atomic_dec_and_test(&laddr->refcnt)) {
+		kfree(laddr);
 	}
-	if (!list_empty(&ipvs->dest_trash))
-		mod_timer(&ipvs->dest_trash_timer,
-			  jiffies + IP_VS_DEST_TRASH_PERIOD);
-	spin_unlock(&ipvs->dest_trash_lock);
 }
 
-/*
- *	Add a service into the service hash table
- */
 static int
-ip_vs_add_service(struct net *net, struct ip_vs_service_user_kern *u,
-		  struct ip_vs_service **svc_p)
+ip_vs_new_laddr(struct ip_vs_service *svc, struct ip_vs_laddr_user_kern *uladdr,
+		struct ip_vs_laddr **laddr_p)
 {
-	int ret = 0;
-	struct ip_vs_scheduler *sched = NULL;
-	struct ip_vs_pe *pe = NULL;
-	struct ip_vs_service *svc = NULL;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	/* increase the module use count */
-	ip_vs_use_count_inc();
+	struct ip_vs_laddr *laddr;
 
-	/* Lookup the scheduler by 'u->sched_name' */
-	sched = ip_vs_scheduler_get(u->sched_name);
-	if (sched == NULL) {
-		pr_info("Scheduler module ip_vs_%s not found\n", u->sched_name);
-		ret = -ENOENT;
-		goto out_err;
+	laddr = kzalloc(sizeof(struct ip_vs_laddr), GFP_ATOMIC);
+	if (!laddr) {
+		pr_err("%s(): no memory.\n", __func__);
+		return -ENOMEM;
 	}
 
-	if (u->pe_name && *u->pe_name) {
-		pe = ip_vs_pe_getbyname(u->pe_name);
-		if (pe == NULL) {
-			pr_info("persistence engine module ip_vs_pe_%s "
-				"not found\n", u->pe_name);
-			ret = -ENOENT;
-			goto out_err;
-		}
-	}
+	laddr->af = svc->af;
+	ip_vs_addr_copy(svc->af, &laddr->addr, &uladdr->addr);
+	atomic64_set(&laddr->port_conflict, 0);
+	atomic64_set(&laddr->port, 0);
+	atomic_set(&laddr->refcnt, 0);
+	atomic_set(&laddr->conn_counts, 0);
 
-#ifdef CONFIG_IP_VS_IPV6
-	if (u->af == AF_INET6) {
-		__u32 plen = (__force __u32) u->netmask;
+	*laddr_p = laddr;
 
-		if (plen < 1 || plen > 128) {
-			ret = -EINVAL;
-			goto out_err;
+	return 0;
+}
+
+static struct ip_vs_laddr *ip_vs_lookup_laddr(struct ip_vs_service *svc,
+					      const union nf_inet_addr *addr)
+{
+	struct ip_vs_laddr *laddr;
+
+	/*
+	 * Find the local address for the given service
+	 */
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		if ((laddr->af == svc->af)
+		    && ip_vs_addr_equal(svc->af, &laddr->addr, addr)) {
+			/* HIT */
+			return laddr;
 		}
 	}
-#endif
 
-	svc = kzalloc(sizeof(struct ip_vs_service), GFP_KERNEL);
-	if (svc == NULL) {
-		IP_VS_DBG(1, "%s(): no memory\n", __func__);
-		ret = -ENOMEM;
-		goto out_err;
-	}
-	svc->stats.cpustats = alloc_percpu(struct ip_vs_cpu_stats);
-	if (!svc->stats.cpustats) {
-		ret = -ENOMEM;
-		goto out_err;
-	}
+	return NULL;
+}
+
+static int
+ip_vs_add_laddr(struct ip_vs_service *svc, struct ip_vs_laddr_user_kern *uladdr)
+{
+	struct ip_vs_laddr *laddr;
+	int ret;
+
+	IP_VS_DBG_BUF(0, "vip %s:%d add local address %s\n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port),
+		      IP_VS_DBG_ADDR(svc->af, &uladdr->addr));
+
+	/*
+	 * Check if the local address already exists in the list
+	 */
+	laddr = ip_vs_lookup_laddr(svc, &uladdr->addr);
+	if (laddr) {
+		IP_VS_DBG(1, "%s(): local address already exists\n", __func__);
+		return -EEXIST;
+	}
+
+	/*
+	 * Allocate and initialize the dest structure
+	 */
+	ret = ip_vs_new_laddr(svc, uladdr, &laddr);
+	if (ret) {
+		return ret;
+	}
+
+	/*
+	 * Add the local adress entry into the list
+	 */
+	ip_vs_laddr_hold(laddr);
+
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 * Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
+	list_add_tail(&laddr->n_list, &svc->laddr_list);
+	svc->num_laddrs++;
+
+#ifdef CONFIG_IP_VS_DEBUG
+	/* Dump the destinations */
+	IP_VS_DBG_BUF(0, "		svc %s:%d num %d curr %p \n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr),
+		      ntohs(svc->port), svc->num_laddrs, svc->curr_laddr);
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		IP_VS_DBG_BUF(0, "		laddr %p %s:%d \n",
+			      laddr, IP_VS_DBG_ADDR(svc->af, &laddr->addr), 0);
+	}
+#endif
+
+	write_unlock_bh(&__ip_vs_svc_lock);
+
+	return 0;
+}
+
+static int
+ip_vs_del_laddr(struct ip_vs_service *svc, struct ip_vs_laddr_user_kern *uladdr)
+{
+	struct ip_vs_laddr *laddr;
+
+	IP_VS_DBG_BUF(0, "vip %s:%d del local address %s\n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port),
+		      IP_VS_DBG_ADDR(svc->af, &uladdr->addr));
+
+	laddr = ip_vs_lookup_laddr(svc, &uladdr->addr);
+
+	if (laddr == NULL) {
+		IP_VS_DBG(1, "%s(): local address not found!\n", __func__);
+		return -ENOENT;
+	}
+
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 *      Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
+	/* update svc->curr_laddr */
+	if (svc->curr_laddr == &laddr->n_list)
+		svc->curr_laddr = laddr->n_list.next;
+	/*
+	 *      Unlink dest from the service
+	 */
+	list_del(&laddr->n_list);
+	svc->num_laddrs--;
+
+#ifdef CONFIG_IP_VS_DEBUG
+	IP_VS_DBG_BUF(0, "	svc %s:%d num %d curr %p \n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr),
+		      ntohs(svc->port), svc->num_laddrs, svc->curr_laddr);
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		IP_VS_DBG_BUF(0, "		laddr %p %s:%d \n",
+			      laddr, IP_VS_DBG_ADDR(svc->af, &laddr->addr), 0);
+	}
+#endif
+
+	ip_vs_laddr_put(laddr);
+
+	write_unlock_bh(&__ip_vs_svc_lock);
+
+	return 0;
+}
+
+/*
+ *	Add a service into the service hash table
+ */
+static int
+ip_vs_add_service(struct ip_vs_service_user_kern *u,
+		  struct ip_vs_service **svc_p)
+{
+	int ret = 0;
+	struct ip_vs_scheduler *sched = NULL;
+	struct ip_vs_service *svc = NULL;
+
+	/* increase the module use count */
+	ip_vs_use_count_inc();
+
+	/* Lookup the scheduler by 'u->sched_name' */
+	sched = ip_vs_scheduler_get(u->sched_name);
+	if (sched == NULL) {
+		pr_info("Scheduler module ip_vs_%s not found\n", u->sched_name);
+		ret = -ENOENT;
+		goto out_mod_dec;
+	}
+#ifdef CONFIG_IP_VS_IPV6
+	if (u->af == AF_INET6 && (u->netmask < 1 || u->netmask > 128)) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+#endif
+
+	svc = kzalloc(sizeof(struct ip_vs_service), GFP_ATOMIC);
+	if (svc == NULL) {
+		IP_VS_DBG(1, "%s(): no memory\n", __func__);
+		ret = -ENOMEM;
+		goto out_err;
+	}
 
 	/* I'm the first user of the service */
+	atomic_set(&svc->usecnt, 1);
 	atomic_set(&svc->refcnt, 0);
 
 	svc->af = u->af;
@@ -1197,11 +1409,15 @@ ip_vs_add_service(struct net *net, struct ip_vs_service_user_kern *u,
 	svc->flags = u->flags;
 	svc->timeout = u->timeout * HZ;
 	svc->netmask = u->netmask;
-	svc->net = net;
+
+	/* Init the local address stuff */
+	rwlock_init(&svc->laddr_lock);
+	INIT_LIST_HEAD(&svc->laddr_list);
+	svc->num_laddrs = 0;
+	svc->curr_laddr = &svc->laddr_list;
 
 	INIT_LIST_HEAD(&svc->destinations);
-	spin_lock_init(&svc->sched_lock);
-	spin_lock_init(&svc->stats.lock);
+	rwlock_init(&svc->sched_lock);
 
 	/* Bind the scheduler */
 	ret = ip_vs_bind_scheduler(svc, sched);
@@ -1209,46 +1425,49 @@ ip_vs_add_service(struct net *net, struct ip_vs_service_user_kern *u,
 		goto out_err;
 	sched = NULL;
 
-	/* Bind the ct retriever */
-	RCU_INIT_POINTER(svc->pe, pe);
-	pe = NULL;
-
 	/* Update the virtual service counters */
 	if (svc->port == FTPPORT)
-		atomic_inc(&ipvs->ftpsvc_counter);
+		atomic_inc(&ip_vs_ftpsvc_counter);
 	else if (svc->port == 0)
-		atomic_inc(&ipvs->nullsvc_counter);
+		atomic_inc(&ip_vs_nullsvc_counter);
 
-	ip_vs_start_estimator(net, &svc->stats);
+	/* Init statistic */
+	ret = ip_vs_new_stats(&(svc->stats));
+	if(ret)
+		goto out_err;
 
 	/* Count only IPv4 services for old get/setsockopt interface */
 	if (svc->af == AF_INET)
-		ipvs->num_services++;
+		ip_vs_num_services++;
 
 	/* Hash the service into the service table */
+	write_lock_bh(&__ip_vs_svc_lock);
 	ip_vs_svc_hash(svc);
+	write_unlock_bh(&__ip_vs_svc_lock);
 
 	*svc_p = svc;
-	/* Now there is a service - full throttle */
-	ipvs->enable = 1;
 	return 0;
 
-
- out_err:
+      out_err:
 	if (svc != NULL) {
-		ip_vs_unbind_scheduler(svc, sched);
-		ip_vs_service_free(svc);
+		if (svc->scheduler)
+			ip_vs_unbind_scheduler(svc);
+		if (svc->inc) {
+			local_bh_disable();
+			ip_vs_app_inc_put(svc->inc);
+			local_bh_enable();
+		}
+		kfree(svc);
 	}
 	ip_vs_scheduler_put(sched);
-	ip_vs_pe_put(pe);
 
+      out_mod_dec:
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
 
 	return ret;
 }
 
-
 /*
  *	Edit a service and bind it with a new scheduler
  */
@@ -1256,7 +1475,6 @@ static int
 ip_vs_edit_service(struct ip_vs_service *svc, struct ip_vs_service_user_kern *u)
 {
 	struct ip_vs_scheduler *sched, *old_sched;
-	struct ip_vs_pe *pe = NULL, *old_pe = NULL;
 	int ret = 0;
 
 	/*
@@ -1269,39 +1487,19 @@ ip_vs_edit_service(struct ip_vs_service *svc, struct ip_vs_service_user_kern *u)
 	}
 	old_sched = sched;
 
-	if (u->pe_name && *u->pe_name) {
-		pe = ip_vs_pe_getbyname(u->pe_name);
-		if (pe == NULL) {
-			pr_info("persistence engine module ip_vs_pe_%s "
-				"not found\n", u->pe_name);
-			ret = -ENOENT;
-			goto out;
-		}
-		old_pe = pe;
-	}
-
 #ifdef CONFIG_IP_VS_IPV6
-	if (u->af == AF_INET6) {
-		__u32 plen = (__force __u32) u->netmask;
-
-		if (plen < 1 || plen > 128) {
-			ret = -EINVAL;
-			goto out;
-		}
+	if (u->af == AF_INET6 && (u->netmask < 1 || u->netmask > 128)) {
+		ret = -EINVAL;
+		goto out;
 	}
 #endif
 
-	old_sched = rcu_dereference_protected(svc->scheduler, 1);
-	if (sched != old_sched) {
-		/* Bind the new scheduler */
-		ret = ip_vs_bind_scheduler(svc, sched);
-		if (ret) {
-			old_sched = sched;
-			goto out;
-		}
-		/* Unbind the old scheduler on success */
-		ip_vs_unbind_scheduler(svc, old_sched);
-	}
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 * Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
 
 	/*
 	 * Set the flags and timeout value
@@ -1310,22 +1508,46 @@ ip_vs_edit_service(struct ip_vs_service *svc, struct ip_vs_service_user_kern *u)
 	svc->timeout = u->timeout * HZ;
 	svc->netmask = u->netmask;
 
-	old_pe = rcu_dereference_protected(svc->pe, 1);
-	if (pe != old_pe)
-		rcu_assign_pointer(svc->pe, pe);
+	old_sched = svc->scheduler;
+	if (sched != old_sched) {
+		/*
+		 * Unbind the old scheduler
+		 */
+		if ((ret = ip_vs_unbind_scheduler(svc))) {
+			old_sched = sched;
+			goto out_unlock;
+		}
+
+		/*
+		 * Bind the new scheduler
+		 */
+		if ((ret = ip_vs_bind_scheduler(svc, sched))) {
+			/*
+			 * If ip_vs_bind_scheduler fails, restore the old
+			 * scheduler.
+			 * The main reason of failure is out of memory.
+			 *
+			 * The question is if the old scheduler can be
+			 * restored all the time. TODO: if it cannot be
+			 * restored some time, we must delete the service,
+			 * otherwise the system may crash.
+			 */
+			ip_vs_bind_scheduler(svc, old_sched);
+			old_sched = sched;
+			goto out_unlock;
+		}
+	}
 
-out:
-	ip_vs_scheduler_put(old_sched);
-	ip_vs_pe_put(old_pe);
-	return ret;
-}
+      out_unlock:
+	write_unlock_bh(&__ip_vs_svc_lock);
+#ifdef CONFIG_IP_VS_IPV6
+      out:
+#endif
 
-static void ip_vs_service_rcu_free(struct rcu_head *head)
-{
-	struct ip_vs_service *svc;
+	if (old_sched)
+		ip_vs_scheduler_put(old_sched);
 
-	svc = container_of(head, struct ip_vs_service, rcu_head);
-	ip_vs_service_free(svc);
+	return ret;
 }
 
 /*
@@ -1333,203 +1555,137 @@ static void ip_vs_service_rcu_free(struct rcu_head *head)
  *	- The service must be unlinked, unlocked and not referenced!
  *	- We are called under _bh lock
  */
-static void __ip_vs_del_service(struct ip_vs_service *svc, bool cleanup)
+static void __ip_vs_del_service(struct ip_vs_service *svc)
 {
 	struct ip_vs_dest *dest, *nxt;
+	struct ip_vs_laddr *laddr, *laddr_next;
 	struct ip_vs_scheduler *old_sched;
-	struct ip_vs_pe *old_pe;
-	struct netns_ipvs *ipvs = net_ipvs(svc->net);
-
-	pr_info("%s: enter\n", __func__);
 
 	/* Count only IPv4 services for old get/setsockopt interface */
 	if (svc->af == AF_INET)
-		ipvs->num_services--;
+		ip_vs_num_services--;
+
+
+	/*
+	 *    Free statistic related per cpu memory
+	 */
+	ip_vs_del_stats(svc->stats);
 
-	ip_vs_stop_estimator(svc->net, &svc->stats);
 
 	/* Unbind scheduler */
-	old_sched = rcu_dereference_protected(svc->scheduler, 1);
-	ip_vs_unbind_scheduler(svc, old_sched);
-	ip_vs_scheduler_put(old_sched);
+	old_sched = svc->scheduler;
+	ip_vs_unbind_scheduler(svc);
+	if (old_sched)
+		ip_vs_scheduler_put(old_sched);
+
+	/* Unbind app inc */
+	if (svc->inc) {
+		ip_vs_app_inc_put(svc->inc);
+		svc->inc = NULL;
+	}
 
-	/* Unbind persistence engine, keep svc->pe */
-	old_pe = rcu_dereference_protected(svc->pe, 1);
-	ip_vs_pe_put(old_pe);
+	/* Unlink the whole local address list */
+	list_for_each_entry_safe(laddr, laddr_next, &svc->laddr_list, n_list) {
+		list_del(&laddr->n_list);
+		ip_vs_laddr_put(laddr);
+	}
 
 	/*
 	 *    Unlink the whole destination list
 	 */
 	list_for_each_entry_safe(dest, nxt, &svc->destinations, n_list) {
 		__ip_vs_unlink_dest(svc, dest, 0);
-		__ip_vs_del_dest(svc->net, dest, cleanup);
+		__ip_vs_del_dest(dest);
 	}
 
 	/*
 	 *    Update the virtual service counters
 	 */
 	if (svc->port == FTPPORT)
-		atomic_dec(&ipvs->ftpsvc_counter);
+		atomic_dec(&ip_vs_ftpsvc_counter);
 	else if (svc->port == 0)
-		atomic_dec(&ipvs->nullsvc_counter);
+		atomic_dec(&ip_vs_nullsvc_counter);
 
 	/*
 	 *    Free the service if nobody refers to it
 	 */
-	if (atomic_dec_and_test(&svc->refcnt)) {
-		IP_VS_DBG_BUF(3, "Removing service %u/%s:%u\n",
-			      svc->fwmark,
-			      IP_VS_DBG_ADDR(svc->af, &svc->addr),
-			      ntohs(svc->port));
-		call_rcu(&svc->rcu_head, ip_vs_service_rcu_free);
-	}
+	if (atomic_read(&svc->refcnt) == 0)
+		kfree(svc);
 
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
 }
 
 /*
- * Unlink a service from list and try to delete it if its refcnt reached 0
+ *	Delete a service from the service list
  */
-static void ip_vs_unlink_service(struct ip_vs_service *svc, bool cleanup)
+static int ip_vs_del_service(struct ip_vs_service *svc)
 {
-	/* Hold svc to avoid double release from dest_trash */
-	atomic_inc(&svc->refcnt);
+	if (svc == NULL)
+		return -EEXIST;
+
 	/*
 	 * Unhash it from the service table
 	 */
+	write_lock_bh(&__ip_vs_svc_lock);
+
 	ip_vs_svc_unhash(svc);
 
-	__ip_vs_del_service(svc, cleanup);
-}
+	/*
+	 * Wait until all the svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
 
-/*
- *	Delete a service from the service list
- */
-static int ip_vs_del_service(struct ip_vs_service *svc)
-{
-	if (svc == NULL)
-		return -EEXIST;
-	ip_vs_unlink_service(svc, false);
+	__ip_vs_del_service(svc);
+
+	write_unlock_bh(&__ip_vs_svc_lock);
 
 	return 0;
 }
 
-
 /*
  *	Flush all the virtual services
  */
-static int ip_vs_flush(struct net *net, bool cleanup)
+static int ip_vs_flush(void)
 {
 	int idx;
-	struct ip_vs_service *svc;
-	struct hlist_node *n;
+	struct ip_vs_service *svc, *nxt;
 
 	/*
-	 * Flush the service table hashed by <netns,protocol,addr,port>
+	 * Flush the service table hashed by <protocol,addr,port>
 	 */
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry_safe(svc, n, &ip_vs_svc_table[idx],
-					  s_list) {
-			if (net_eq(svc->net, net))
-				ip_vs_unlink_service(svc, cleanup);
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+		list_for_each_entry_safe(svc, nxt, &ip_vs_svc_table[idx],
+					 s_list) {
+			write_lock_bh(&__ip_vs_svc_lock);
+			ip_vs_svc_unhash(svc);
+			/*
+			 * Wait until all the svc users go away.
+			 */
+			IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 0);
+			__ip_vs_del_service(svc);
+			write_unlock_bh(&__ip_vs_svc_lock);
 		}
 	}
 
 	/*
 	 * Flush the service table hashed by fwmark
 	 */
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry_safe(svc, n, &ip_vs_svc_fwm_table[idx],
-					  f_list) {
-			if (net_eq(svc->net, net))
-				ip_vs_unlink_service(svc, cleanup);
-		}
-	}
-
-	return 0;
-}
-
-/*
- *	Delete service by {netns} in the service table.
- *	Called by __ip_vs_cleanup()
- */
-void ip_vs_service_net_cleanup(struct net *net)
-{
-	EnterFunction(2);
-	/* Check for "full" addressed entries */
-	mutex_lock(&__ip_vs_mutex);
-	ip_vs_flush(net, true);
-	mutex_unlock(&__ip_vs_mutex);
-	LeaveFunction(2);
-}
-
-/* Put all references for device (dst_cache) */
-static inline void
-ip_vs_forget_dev(struct ip_vs_dest *dest, struct net_device *dev)
-{
-	struct ip_vs_dest_dst *dest_dst;
-
-	spin_lock_bh(&dest->dst_lock);
-	dest_dst = rcu_dereference_protected(dest->dest_dst, 1);
-	if (dest_dst && dest_dst->dst_cache->dev == dev) {
-		IP_VS_DBG_BUF(3, "Reset dev:%s dest %s:%u ,dest->refcnt=%d\n",
-			      dev->name,
-			      IP_VS_DBG_ADDR(dest->af, &dest->addr),
-			      ntohs(dest->port),
-			      atomic_read(&dest->refcnt));
-		__ip_vs_dst_cache_reset(dest);
-	}
-	spin_unlock_bh(&dest->dst_lock);
-
-}
-/* Netdev event receiver
- * Currently only NETDEV_DOWN is handled to release refs to cached dsts
- */
-static int ip_vs_dst_event(struct notifier_block *this, unsigned long event,
-			    void *ptr)
-{
-	struct net_device *dev = ptr;
-	struct net *net = dev_net(dev);
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_service *svc;
-	struct ip_vs_dest *dest;
-	unsigned int idx;
-
-	if (event != NETDEV_DOWN || !ipvs)
-		return NOTIFY_DONE;
-	IP_VS_DBG(3, "%s() dev=%s\n", __func__, dev->name);
-	EnterFunction(2);
-	mutex_lock(&__ip_vs_mutex);
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
-			if (net_eq(svc->net, net)) {
-				list_for_each_entry(dest, &svc->destinations,
-						    n_list) {
-					ip_vs_forget_dev(dest, dev);
-				}
-			}
-		}
-
-		hlist_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
-			if (net_eq(svc->net, net)) {
-				list_for_each_entry(dest, &svc->destinations,
-						    n_list) {
-					ip_vs_forget_dev(dest, dev);
-				}
-			}
-
+		list_for_each_entry_safe(svc, nxt,
+					 &ip_vs_svc_fwm_table[idx], f_list) {
+			write_lock_bh(&__ip_vs_svc_lock);
+			ip_vs_svc_unhash(svc);
+			/*
+			 * Wait until all the svc users go away.
+			 */
+			IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 0);
+			__ip_vs_del_service(svc);
+			write_unlock_bh(&__ip_vs_svc_lock);
 		}
 	}
 
-	spin_lock_bh(&ipvs->dest_trash_lock);
-	list_for_each_entry(dest, &ipvs->dest_trash, t_list) {
-		ip_vs_forget_dev(dest, dev);
-	}
-	spin_unlock_bh(&ipvs->dest_trash_lock);
-	mutex_unlock(&__ip_vs_mutex);
-	LeaveFunction(2);
-	return NOTIFY_DONE;
+	return 0;
 }
 
 /*
@@ -1539,46 +1695,40 @@ static int ip_vs_zero_service(struct ip_vs_service *svc)
 {
 	struct ip_vs_dest *dest;
 
+	write_lock_bh(&__ip_vs_svc_lock);
 	list_for_each_entry(dest, &svc->destinations, n_list) {
-		ip_vs_zero_stats(&dest->stats);
+		ip_vs_zero_stats(dest->stats);
 	}
-	ip_vs_zero_stats(&svc->stats);
+	ip_vs_zero_stats(svc->stats);
+	write_unlock_bh(&__ip_vs_svc_lock);
 	return 0;
 }
 
-static int ip_vs_zero_all(struct net *net)
+static int ip_vs_zero_all(void)
 {
 	int idx;
 	struct ip_vs_service *svc;
 
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
-			if (net_eq(svc->net, net))
-				ip_vs_zero_service(svc);
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+		list_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
+			ip_vs_zero_service(svc);
 		}
 	}
 
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
-			if (net_eq(svc->net, net))
-				ip_vs_zero_service(svc);
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+		list_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
+			ip_vs_zero_service(svc);
 		}
 	}
 
-	ip_vs_zero_stats(&net_ipvs(net)->tot_stats);
+	ip_vs_zero_stats(ip_vs_stats);
 	return 0;
 }
 
-#ifdef CONFIG_SYSCTL
-
-static int zero;
-static int three = 3;
-
 static int
-proc_do_defense_mode(struct ctl_table *table, int write,
-		     void __user *buffer, size_t *lenp, loff_t *ppos)
+proc_do_defense_mode(ctl_table * table, int write,
+		     void __user * buffer, size_t * lenp, loff_t * ppos)
 {
-	struct net *net = current->nsproxy->net_ns;
 	int *valp = table->data;
 	int val = *valp;
 	int rc;
@@ -1589,15 +1739,15 @@ proc_do_defense_mode(struct ctl_table *table, int write,
 			/* Restore the correct value */
 			*valp = val;
 		} else {
-			update_defense_level(net_ipvs(net));
+			update_defense_level();
 		}
 	}
 	return rc;
 }
 
 static int
-proc_do_sync_threshold(struct ctl_table *table, int write,
-		       void __user *buffer, size_t *lenp, loff_t *ppos)
+proc_do_sync_threshold(ctl_table * table, int write,
+		       void __user * buffer, size_t * lenp, loff_t * ppos)
 {
 	int *valp = table->data;
 	int val[2];
@@ -1607,293 +1757,423 @@ proc_do_sync_threshold(struct ctl_table *table, int write,
 	memcpy(val, valp, sizeof(val));
 
 	rc = proc_dointvec(table, write, buffer, lenp, ppos);
-	if (write && (valp[0] < 0 || valp[1] < 0 ||
-	    (valp[0] >= valp[1] && valp[1]))) {
+	if (write && (valp[0] < 0 || valp[1] < 0 || valp[0] >= valp[1])) {
 		/* Restore the correct value */
 		memcpy(valp, val, sizeof(val));
 	}
 	return rc;
 }
 
-static int
-proc_do_sync_mode(struct ctl_table *table, int write,
-		     void __user *buffer, size_t *lenp, loff_t *ppos)
-{
-	int *valp = table->data;
-	int val = *valp;
-	int rc;
-
-	rc = proc_dointvec(table, write, buffer, lenp, ppos);
-	if (write && (*valp != val)) {
-		if ((*valp < 0) || (*valp > 1)) {
-			/* Restore the correct value */
-			*valp = val;
-		}
-	}
-	return rc;
-}
-
-static int
-proc_do_sync_ports(struct ctl_table *table, int write,
-		   void __user *buffer, size_t *lenp, loff_t *ppos)
-{
-	int *valp = table->data;
-	int val = *valp;
-	int rc;
-
-	rc = proc_dointvec(table, write, buffer, lenp, ppos);
-	if (write && (*valp != val)) {
-		if (*valp < 1 || !is_power_of_2(*valp)) {
-			/* Restore the correct value */
-			*valp = val;
-		}
-	}
-	return rc;
-}
-
 /*
  *	IPVS sysctl table (under the /proc/sys/net/ipv4/vs/)
- *	Do not change order or insert new entries without
- *	align with netns init in ip_vs_control_net_init()
  */
 
 static struct ctl_table vs_vars[] = {
 	{
-		.procname	= "amemthresh",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "am_droprate",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "drop_entry",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_do_defense_mode,
-	},
-	{
-		.procname	= "drop_packet",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_do_defense_mode,
-	},
-#ifdef CONFIG_IP_VS_NFCT
-	{
-		.procname	= "conntrack",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
-	},
-#endif
-	{
-		.procname	= "secure_tcp",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_do_defense_mode,
-	},
-	{
-		.procname	= "snat_reroute",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
-	},
-	{
-		.procname	= "sync_version",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= &proc_do_sync_mode,
-	},
-	{
-		.procname	= "sync_ports",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= &proc_do_sync_ports,
-	},
-	{
-		.procname	= "sync_qlen_max",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "sync_sock_size",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "cache_bypass",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "expire_nodest_conn",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "expire_quiescent_template",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "sync_threshold",
-		.maxlen		=
-			sizeof(((struct netns_ipvs *)0)->sysctl_sync_threshold),
-		.mode		= 0644,
-		.proc_handler	= proc_do_sync_threshold,
-	},
-	{
-		.procname	= "sync_refresh_period",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "sync_retries",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_minmax,
-		.extra1		= &zero,
-		.extra2		= &three,
-	},
-	{
-		.procname	= "nat_icmp_send",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "pmtu_disc",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "backup_only",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "conn_reuse_mode",
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
+	 .procname = "amemthresh",
+	 .data = &sysctl_ip_vs_amemthresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
 #ifdef CONFIG_IP_VS_DEBUG
 	{
-		.procname	= "debug_level",
-		.data		= &sysctl_ip_vs_debug_level,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
+	 .procname = "debug_level",
+	 .data = &sysctl_ip_vs_debug_level,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
 #endif
-#if 0
-	{
-		.procname	= "timeout_established",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_ESTABLISHED],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synsent",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYN_SENT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synrecv",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYN_RECV],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_finwait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_FIN_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_timewait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_TIME_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_close",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_CLOSE],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_closewait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_CLOSE_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_lastack",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_LAST_ACK],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_listen",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_LISTEN],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synack",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYNACK],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_udp",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_UDP],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_icmp",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_ICMP],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-#endif
-	{ }
+	{
+	 .procname = "am_droprate",
+	 .data = &sysctl_ip_vs_am_droprate,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "drop_entry",
+	 .data = &sysctl_ip_vs_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_do_defense_mode,
+	 },
+	{
+	 .procname = "drop_packet",
+	 .data = &sysctl_ip_vs_drop_packet,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_do_defense_mode,
+	 },
+	{
+	 .procname = "secure_tcp",
+	 .data = &sysctl_ip_vs_secure_tcp,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_do_defense_mode,
+	 },
+	{
+	 .procname = "timeout_established",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_ESTABLISHED],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_synsent",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYN_SENT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_synrecv",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYN_RECV],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_finwait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_FIN_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_timewait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_TIME_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_close",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_CLOSE],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_closewait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_CLOSE_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_lastack",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST_ACK],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_listen",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LISTEN],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_synack",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYNACK],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "cache_bypass",
+	 .data = &sysctl_ip_vs_cache_bypass,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "expire_nodest_conn",
+	 .data = &sysctl_ip_vs_expire_nodest_conn,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "expire_quiescent_template",
+	 .data = &sysctl_ip_vs_expire_quiescent_template,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "sync_threshold",
+	 .data = &sysctl_ip_vs_sync_threshold,
+	 .maxlen = sizeof(sysctl_ip_vs_sync_threshold),
+	 .mode = 0644,
+	 .proc_handler = proc_do_sync_threshold,
+	 }
+	,
+	{
+	 .procname = "nat_icmp_send",
+	 .data = &sysctl_ip_vs_nat_icmp_send,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "fullnat_timestamp_remove_entry",
+	 .data = &sysctl_ip_vs_timestamp_remove_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_mss_adjust_entry",
+	 .data = &sysctl_ip_vs_mss_adjust_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_conn_reused_entry",
+	 .data = &sysctl_ip_vs_conn_reused_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_toa_entry",
+	 .data = &sysctl_ip_vs_toa_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_lport_max",
+	 .data = &sysctl_ip_vs_lport_max,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_port_min,
+	 .extra2 = &ip_vs_port_max,
+	 },
+	{
+	 .procname = "fullnat_lport_min",
+	 .data = &sysctl_ip_vs_lport_min,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_port_min,
+	 .extra2 = &ip_vs_port_max,
+	 },
+	{
+	 .procname = "fullnat_lport_tries",
+	 .data = &sysctl_ip_vs_lport_tries,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_port_try_min,
+	 .extra2 = &ip_vs_port_try_max,
+	 },
+	/* syn-proxy sysctl variables */
+	{
+	 .procname = "synproxy_init_mss",
+	 .data = &sysctl_ip_vs_synproxy_init_mss,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_init_mss_min,
+	 .extra2 = &ip_vs_synproxy_init_mss_max,
+	 },
+	{
+	 .procname = "synproxy_sack",
+	 .data = &sysctl_ip_vs_synproxy_sack,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_wscale",
+	 .data = &sysctl_ip_vs_synproxy_wscale,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_wscale_min,
+	 .extra2 = &ip_vs_synproxy_wscale_max,
+	 },
+	{
+	 .procname = "synproxy_timestamp",
+	 .data = &sysctl_ip_vs_synproxy_timestamp,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_synack_ttl",
+	 .data = &sysctl_ip_vs_synproxy_synack_ttl,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_synack_ttl_min,
+	 .extra2 = &ip_vs_synproxy_synack_ttl_max,
+	 },
+	{
+	 .procname = "synproxy_defer",
+	 .data = &sysctl_ip_vs_synproxy_defer,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_close",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_cl,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_time_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_tw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_fin_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_fw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_close_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_cw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_last_ack",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_la,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 },
+	{
+	 .procname = "synproxy_ack_skb_store_thresh",
+	 .data = &sysctl_ip_vs_synproxy_skb_store_thresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_skb_store_thresh_min,
+	 .extra2 = &ip_vs_synproxy_skb_store_thresh_max,
+	 },
+	{
+	 .procname = "synproxy_ack_storm_thresh",
+	 .data = &sysctl_ip_vs_synproxy_dup_ack_thresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_dup_ack_cnt_min,
+	 .extra2 = &ip_vs_synproxy_dup_ack_cnt_max,
+	 },
+	{
+	 .procname = "synproxy_syn_retry",
+	 .data = &sysctl_ip_vs_synproxy_syn_retry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_syn_retry_min,
+	 .extra2 = &ip_vs_synproxy_syn_retry_max,
+	 },
+	/* attack-defence sysctl variables */
+	{
+	 .procname = "defence_tcp_drop",
+	 .data = &sysctl_ip_vs_tcp_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "defence_udp_drop",
+	 .data = &sysctl_ip_vs_udp_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "defence_frag_drop",
+	 .data = &sysctl_ip_vs_frag_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	/* send rst sysctl variables */
+	{
+	 .procname = "conn_expire_tcp_rst",
+	 .data = &sysctl_ip_vs_conn_expire_tcp_rst,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,	/* zero */
+	 .extra2 = &ip_vs_entry_max,	/* one */
+	 },
+	{
+	 .procname = "fast_response_xmit",
+	 .data = &sysctl_ip_vs_fast_xmit,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_entry_min,	/* zero */
+	 .extra2 = &ip_vs_entry_max,	/* one */
+	 },
+	{}
 };
 
-#endif
+static struct ctl_table_header *sysctl_header;
 
 #ifdef CONFIG_PROC_FS
 
 struct ip_vs_iter {
-	struct seq_net_private p;  /* Do not move this, netns depends upon it*/
-	struct hlist_head *table;
+	struct list_head *table;
 	int bucket;
 };
 
@@ -1901,7 +2181,7 @@ struct ip_vs_iter {
  *	Write the contents of the VS rule table to a PROCfs file.
  *	(It is kept just for backward compatibility)
  */
-static inline const char *ip_vs_fwd_name(unsigned int flags)
+static inline const char *ip_vs_fwd_name(unsigned flags)
 {
 	switch (flags & IP_VS_CONN_F_FWD_MASK) {
 	case IP_VS_CONN_F_LOCALNODE:
@@ -1910,24 +2190,24 @@ static inline const char *ip_vs_fwd_name(unsigned int flags)
 		return "Tunnel";
 	case IP_VS_CONN_F_DROUTE:
 		return "Route";
+	case IP_VS_CONN_F_FULLNAT:
+		return "FullNat";
 	default:
 		return "Masq";
 	}
 }
 
-
 /* Get the Nth entry in the two lists */
 static struct ip_vs_service *ip_vs_info_array(struct seq_file *seq, loff_t pos)
 {
-	struct net *net = seq_file_net(seq);
 	struct ip_vs_iter *iter = seq->private;
 	int idx;
 	struct ip_vs_service *svc;
 
 	/* look in hash by protocol */
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry_rcu(svc, &ip_vs_svc_table[idx], s_list) {
-			if (net_eq(svc->net, net) && pos-- == 0) {
+		list_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
+			if (pos-- == 0) {
 				iter->table = ip_vs_svc_table;
 				iter->bucket = idx;
 				return svc;
@@ -1937,9 +2217,8 @@ static struct ip_vs_service *ip_vs_info_array(struct seq_file *seq, loff_t pos)
 
 	/* keep looking in fwmark */
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry_rcu(svc, &ip_vs_svc_fwm_table[idx],
-					 f_list) {
-			if (net_eq(svc->net, net) && pos-- == 0) {
+		list_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
+			if (pos-- == 0) {
 				iter->table = ip_vs_svc_fwm_table;
 				iter->bucket = idx;
 				return svc;
@@ -1950,37 +2229,35 @@ static struct ip_vs_service *ip_vs_info_array(struct seq_file *seq, loff_t pos)
 	return NULL;
 }
 
-static void *ip_vs_info_seq_start(struct seq_file *seq, loff_t *pos)
-	__acquires(RCU)
+static void *ip_vs_info_seq_start(struct seq_file *seq, loff_t * pos)
+__acquires(__ip_vs_svc_lock)
 {
-	rcu_read_lock();
+
+	read_lock_bh(&__ip_vs_svc_lock);
 	return *pos ? ip_vs_info_array(seq, *pos - 1) : SEQ_START_TOKEN;
 }
 
-
-static void *ip_vs_info_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+static void *ip_vs_info_seq_next(struct seq_file *seq, void *v, loff_t * pos)
 {
-	struct hlist_node *e;
+	struct list_head *e;
 	struct ip_vs_iter *iter;
 	struct ip_vs_service *svc;
 
 	++*pos;
 	if (v == SEQ_START_TOKEN)
-		return ip_vs_info_array(seq,0);
+		return ip_vs_info_array(seq, 0);
 
 	svc = v;
 	iter = seq->private;
 
 	if (iter->table == ip_vs_svc_table) {
 		/* next service in table hashed by protocol */
-		e = rcu_dereference(hlist_next_rcu(&svc->s_list));
-		if (e)
-			return hlist_entry(e, struct ip_vs_service, s_list);
+		if ((e = svc->s_list.next) != &ip_vs_svc_table[iter->bucket])
+			return list_entry(e, struct ip_vs_service, s_list);
 
 		while (++iter->bucket < IP_VS_SVC_TAB_SIZE) {
-			hlist_for_each_entry_rcu(svc,
-						 &ip_vs_svc_table[iter->bucket],
-						 s_list) {
+			list_for_each_entry(svc, &ip_vs_svc_table[iter->bucket],
+					    s_list) {
 				return svc;
 			}
 		}
@@ -1991,74 +2268,74 @@ static void *ip_vs_info_seq_next(struct seq_file *seq, void *v, loff_t *pos)
 	}
 
 	/* next service in hashed by fwmark */
-	e = rcu_dereference(hlist_next_rcu(&svc->f_list));
-	if (e)
-		return hlist_entry(e, struct ip_vs_service, f_list);
+	if ((e = svc->f_list.next) != &ip_vs_svc_fwm_table[iter->bucket])
+		return list_entry(e, struct ip_vs_service, f_list);
 
- scan_fwmark:
+      scan_fwmark:
 	while (++iter->bucket < IP_VS_SVC_TAB_SIZE) {
-		hlist_for_each_entry_rcu(svc,
-					 &ip_vs_svc_fwm_table[iter->bucket],
-					 f_list)
-			return svc;
+		list_for_each_entry(svc, &ip_vs_svc_fwm_table[iter->bucket],
+				    f_list)
+		    return svc;
 	}
 
 	return NULL;
 }
 
 static void ip_vs_info_seq_stop(struct seq_file *seq, void *v)
-	__releases(RCU)
+__releases(__ip_vs_svc_lock)
 {
-	rcu_read_unlock();
+	read_unlock_bh(&__ip_vs_svc_lock);
 }
 
-
 static int ip_vs_info_seq_show(struct seq_file *seq, void *v)
 {
 	if (v == SEQ_START_TOKEN) {
 		seq_printf(seq,
-			"IP Virtual Server version %d.%d.%d (size=%d)\n",
-			NVERSION(IP_VS_VERSION_CODE), ip_vs_conn_tab_size);
-		seq_puts(seq,
-			 "Prot LocalAddress:Port Scheduler Flags\n");
+			   "IP Virtual Server version %d.%d.%d (size=%d)\n",
+			   NVERSION(IP_VS_VERSION_CODE), IP_VS_CONN_TAB_SIZE);
+		seq_puts(seq, "Prot LocalAddress:Port Scheduler Flags\n");
 		seq_puts(seq,
 			 "  -> RemoteAddress:Port Forward Weight ActiveConn InActConn\n");
 	} else {
 		const struct ip_vs_service *svc = v;
 		const struct ip_vs_iter *iter = seq->private;
 		const struct ip_vs_dest *dest;
-		struct ip_vs_scheduler *sched = rcu_dereference(svc->scheduler);
 
 		if (iter->table == ip_vs_svc_table) {
 #ifdef CONFIG_IP_VS_IPV6
 			if (svc->af == AF_INET6)
-				seq_printf(seq, "%s  [%pI6]:%04X %s ",
+				seq_printf(seq, "%s  [%pI6]:%04X %s%s ",
 					   ip_vs_proto_name(svc->protocol),
 					   &svc->addr.in6,
 					   ntohs(svc->port),
-					   sched->name);
+					   svc->scheduler->name,
+					   (svc->
+					    flags & IP_VS_SVC_F_ONEPACKET) ?
+					   " ops" : "");
 			else
 #endif
-				seq_printf(seq, "%s  %08X:%04X %s %s ",
+				seq_printf(seq, "%s  %08X:%04X %s%s ",
 					   ip_vs_proto_name(svc->protocol),
 					   ntohl(svc->addr.ip),
 					   ntohs(svc->port),
-					   sched->name,
-					   (svc->flags & IP_VS_SVC_F_ONEPACKET)?"ops ":"");
+					   svc->scheduler->name,
+					   (svc->
+					    flags & IP_VS_SVC_F_ONEPACKET) ?
+					   " ops" : "");
 		} else {
-			seq_printf(seq, "FWM  %08X %s %s",
-				   svc->fwmark, sched->name,
-				   (svc->flags & IP_VS_SVC_F_ONEPACKET)?"ops ":"");
+			seq_printf(seq, "FWM  %08X %s%s ",
+				   svc->fwmark, svc->scheduler->name,
+				   (svc->flags & IP_VS_SVC_F_ONEPACKET) ?
+				   " ops" : "");
 		}
 
 		if (svc->flags & IP_VS_SVC_F_PERSISTENT)
 			seq_printf(seq, "persistent %d %08X\n",
-				svc->timeout,
-				ntohl(svc->netmask));
+				   svc->timeout, ntohl(svc->netmask));
 		else
 			seq_putc(seq, '\n');
 
-		list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+		list_for_each_entry(dest, &svc->destinations, n_list) {
 #ifdef CONFIG_IP_VS_IPV6
 			if (dest->af == AF_INET6)
 				seq_printf(seq,
@@ -2066,7 +2343,8 @@ static int ip_vs_info_seq_show(struct seq_file *seq, void *v)
 					   "      %-7s %-6d %-10d %-10d\n",
 					   &dest->addr.in6,
 					   ntohs(dest->port),
-					   ip_vs_fwd_name(atomic_read(&dest->conn_flags)),
+					   ip_vs_fwd_name(atomic_read
+							  (&dest->conn_flags)),
 					   atomic_read(&dest->weight),
 					   atomic_read(&dest->activeconns),
 					   atomic_read(&dest->inactconns));
@@ -2077,7 +2355,8 @@ static int ip_vs_info_seq_show(struct seq_file *seq, void *v)
 					   "%-7s %-6d %-10d %-10d\n",
 					   ntohl(dest->addr.ip),
 					   ntohs(dest->port),
-					   ip_vs_fwd_name(atomic_read(&dest->conn_flags)),
+					   ip_vs_fwd_name(atomic_read
+							  (&dest->conn_flags)),
 					   atomic_read(&dest->weight),
 					   atomic_read(&dest->activeconns),
 					   atomic_read(&dest->inactconns));
@@ -2089,55 +2368,55 @@ static int ip_vs_info_seq_show(struct seq_file *seq, void *v)
 
 static const struct seq_operations ip_vs_info_seq_ops = {
 	.start = ip_vs_info_seq_start,
-	.next  = ip_vs_info_seq_next,
-	.stop  = ip_vs_info_seq_stop,
-	.show  = ip_vs_info_seq_show,
+	.next = ip_vs_info_seq_next,
+	.stop = ip_vs_info_seq_stop,
+	.show = ip_vs_info_seq_show,
 };
 
 static int ip_vs_info_open(struct inode *inode, struct file *file)
 {
-	return seq_open_net(inode, file, &ip_vs_info_seq_ops,
-			sizeof(struct ip_vs_iter));
+	return seq_open_private(file, &ip_vs_info_seq_ops,
+				sizeof(struct ip_vs_iter));
 }
 
 static const struct file_operations ip_vs_info_fops = {
-	.owner	 = THIS_MODULE,
-	.open    = ip_vs_info_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
-	.release = seq_release_net,
+	.owner = THIS_MODULE,
+	.open = ip_vs_info_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release_private,
 };
 
+#endif
+
+struct ip_vs_stats *ip_vs_stats;
+
+#ifdef CONFIG_PROC_FS
 static int ip_vs_stats_show(struct seq_file *seq, void *v)
 {
-	struct net *net = seq_file_single_net(seq);
-	struct ip_vs_stats_user show;
+	int i = 0;
 
-/*               01234567 01234567 01234567 0123456701234567 0123456701234567 */
 	seq_puts(seq,
-		 "   Total Incoming Outgoing         Incoming         Outgoing\n");
-	seq_printf(seq,
-		   "   Conns  Packets  Packets            Bytes            Bytes\n");
-
-	ip_vs_copy_stats(&show, &net_ipvs(net)->tot_stats);
-	seq_printf(seq, "%8X %8X %8X %16LX %16LX\n\n", show.conns,
-		   show.inpkts, show.outpkts,
-		   (unsigned long long) show.inbytes,
-		   (unsigned long long) show.outbytes);
-
-/*                 01234567 01234567 01234567 0123456701234567 0123456701234567 */
+	       /* ++++01234567890123456++++01234567890123456++++01234567890123456++++01234567890123456++++01234567890123456*/
+		"	          Total             Incoming             Outgoing             Incoming             Outgoing\n");
 	seq_puts(seq,
-		   " Conns/s   Pkts/s   Pkts/s          Bytes/s          Bytes/s\n");
-	seq_printf(seq, "%8X %8X %8X %16X %16X\n",
-			show.cps, show.inpps, show.outpps,
-			show.inbps, show.outbps);
+		"	          Conns	             Packets		  Packets                Bytes                Bytes\n");
+
+	for_each_online_cpu(i) {
+		seq_printf(seq, "CPU%2d:%17Ld    %17Ld    %17Ld    %17Ld    %17Ld\n", i,
+			ip_vs_stats_cpu(ip_vs_stats, i).conns,
+			ip_vs_stats_cpu(ip_vs_stats, i).inpkts,
+			ip_vs_stats_cpu(ip_vs_stats, i).outpkts,
+			ip_vs_stats_cpu(ip_vs_stats, i).inbytes,
+			ip_vs_stats_cpu(ip_vs_stats, i).outbytes);
+	}
 
 	return 0;
 }
 
 static int ip_vs_stats_seq_open(struct inode *inode, struct file *file)
 {
-	return single_open_net(inode, file, ip_vs_stats_show);
+	return single_open(file, ip_vs_stats_show, NULL);
 }
 
 static const struct file_operations ip_vs_stats_fops = {
@@ -2145,176 +2424,213 @@ static const struct file_operations ip_vs_stats_fops = {
 	.open = ip_vs_stats_seq_open,
 	.read = seq_read,
 	.llseek = seq_lseek,
-	.release = single_release_net,
+	.release = single_release,
 };
 
-static int ip_vs_stats_percpu_show(struct seq_file *seq, void *v)
-{
-	struct net *net = seq_file_single_net(seq);
-	struct ip_vs_stats *tot_stats = &net_ipvs(net)->tot_stats;
-	struct ip_vs_cpu_stats __percpu *cpustats = tot_stats->cpustats;
-	struct ip_vs_stats_user rates;
-	int i;
-
-/*               01234567 01234567 01234567 0123456701234567 0123456701234567 */
-	seq_puts(seq,
-		 "       Total Incoming Outgoing         Incoming         Outgoing\n");
-	seq_printf(seq,
-		   "CPU    Conns  Packets  Packets            Bytes            Bytes\n");
+#endif
 
-	for_each_possible_cpu(i) {
-		struct ip_vs_cpu_stats *u = per_cpu_ptr(cpustats, i);
-		unsigned int start;
-		__u64 inbytes, outbytes;
+#ifdef CONFIG_PROC_FS
+/*
+ * Statistics for FULLNAT and SYNPROXY
+ * in /proc/net/ip_vs_ext_stats
+ */
 
-		do {
-			start = u64_stats_fetch_begin_irq(&u->syncp);
-			inbytes = u->ustats.inbytes;
-			outbytes = u->ustats.outbytes;
-		} while (u64_stats_fetch_retry_irq(&u->syncp, start));
+struct ip_vs_estats_mib *ip_vs_esmib;
+
+static struct ip_vs_estats_entry ext_stats[] = {
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_ok", FULLNAT_ADD_TOA_OK),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_len", FULLNAT_ADD_TOA_FAIL_LEN),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_head_full", FULLNAT_ADD_TOA_HEAD_FULL),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_mem", FULLNAT_ADD_TOA_FAIL_MEM),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_proto",
+			  FULLNAT_ADD_TOA_FAIL_PROTO),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused", FULLNAT_CONN_REUSED),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_close",
+			  FULLNAT_CONN_REUSED_CLOSE),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_timewait",
+			  FULLNAT_CONN_REUSED_TIMEWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_finwait",
+			  FULLNAT_CONN_REUSED_FINWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_closewait",
+			  FULLNAT_CONN_REUSED_CLOSEWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_lastack",
+			  FULLNAT_CONN_REUSED_LASTACK),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_estab",
+			  FULLNAT_CONN_REUSED_ESTAB),
+	IP_VS_ESTATS_ITEM("synproxy_rs_error", SYNPROXY_RS_ERROR),
+	IP_VS_ESTATS_ITEM("synproxy_null_ack", SYNPROXY_NULL_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_bad_ack", SYNPROXY_BAD_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_ok_ack", SYNPROXY_OK_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_syn_cnt", SYNPROXY_SYN_CNT),
+	IP_VS_ESTATS_ITEM("synproxy_ackstorm", SYNPROXY_ACK_STORM),
+	IP_VS_ESTATS_ITEM("synproxy_synsend_qlen", SYNPROXY_SYNSEND_QLEN),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused", SYNPROXY_CONN_REUSED),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_close",
+			  SYNPROXY_CONN_REUSED_CLOSE),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_timewait",
+			  SYNPROXY_CONN_REUSED_TIMEWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_finwait",
+			  SYNPROXY_CONN_REUSED_FINWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_closewait",
+			  SYNPROXY_CONN_REUSED_CLOSEWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_lastack",
+			  SYNPROXY_CONN_REUSED_LASTACK),
+	IP_VS_ESTATS_ITEM("defence_ip_frag_drop", DEFENCE_IP_FRAG_DROP),
+	IP_VS_ESTATS_ITEM("defence_ip_frag_gather", DEFENCE_IP_FRAG_GATHER),
+	IP_VS_ESTATS_ITEM("defence_tcp_drop", DEFENCE_TCP_DROP),
+	IP_VS_ESTATS_ITEM("defence_udp_drop", DEFENCE_UDP_DROP),
+	IP_VS_ESTATS_ITEM("fast_xmit_reject", FAST_XMIT_REJECT),
+	IP_VS_ESTATS_ITEM("fast_xmit_pass", FAST_XMIT_PASS),
+	IP_VS_ESTATS_ITEM("fast_xmit_skb_copy", FAST_XMIT_SKB_COPY),
+	IP_VS_ESTATS_ITEM("fast_xmit_no_mac", FAST_XMIT_NO_MAC),
+	IP_VS_ESTATS_ITEM("fast_xmit_synproxy_save", FAST_XMIT_SYNPROXY_SAVE),
+	IP_VS_ESTATS_ITEM("fast_xmit_dev_lost", FAST_XMIT_DEV_LOST),
+	IP_VS_ESTATS_ITEM("rst_in_syn_sent", RST_IN_SYN_SENT),
+	IP_VS_ESTATS_ITEM("rst_out_syn_sent", RST_OUT_SYN_SENT),
+	IP_VS_ESTATS_ITEM("rst_in_established", RST_IN_ESTABLISHED),
+	IP_VS_ESTATS_ITEM("rst_out_established", RST_OUT_ESTABLISHED),
+	IP_VS_ESTATS_ITEM("gro_pass", GRO_PASS),
+	IP_VS_ESTATS_ITEM("lro_reject", LRO_REJECT),
+	IP_VS_ESTATS_ITEM("xmit_unexpected_mtu", XMIT_UNEXPECTED_MTU),
+	IP_VS_ESTATS_ITEM("conn_sched_unreach", CONN_SCHED_UNREACH),
+	IP_VS_ESTATS_LAST
+};
 
-		seq_printf(seq, "%3X %8X %8X %8X %16LX %16LX\n",
-			   i, u->ustats.conns, u->ustats.inpkts,
-			   u->ustats.outpkts, (__u64)inbytes,
-			   (__u64)outbytes);
+static int ip_vs_estats_show(struct seq_file *seq, void *v)
+{
+	int i, j;
+
+	/* print CPU first */
+	seq_printf(seq, "                                  ");
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_online(i))
+			seq_printf(seq, "CPU%d       ", i);
+	seq_putc(seq, '\n');
+
+	i = 0;
+	while (NULL != ext_stats[i].name) {
+		seq_printf(seq, "%-25s:", ext_stats[i].name);
+		for (j = 0; j < NR_CPUS; j++) {
+			if (cpu_online(j)) {
+				seq_printf(seq, "%10lu ",
+					   *(((unsigned long *)
+					      per_cpu_ptr(ip_vs_esmib,
+							  j)) +
+					     ext_stats[i].entry));
+			}
+		}
+		seq_putc(seq, '\n');
+		i++;
 	}
-
-	spin_lock_bh(&tot_stats->lock);
-
-	seq_printf(seq, "  ~ %8X %8X %8X %16LX %16LX\n\n",
-		   tot_stats->ustats.conns, tot_stats->ustats.inpkts,
-		   tot_stats->ustats.outpkts,
-		   (unsigned long long) tot_stats->ustats.inbytes,
-		   (unsigned long long) tot_stats->ustats.outbytes);
-
-	ip_vs_read_estimator(&rates, tot_stats);
-
-	spin_unlock_bh(&tot_stats->lock);
-
-/*                 01234567 01234567 01234567 0123456701234567 0123456701234567 */
-	seq_puts(seq,
-		   "     Conns/s   Pkts/s   Pkts/s          Bytes/s          Bytes/s\n");
-	seq_printf(seq, "    %8X %8X %8X %16X %16X\n",
-			rates.cps,
-			rates.inpps,
-			rates.outpps,
-			rates.inbps,
-			rates.outbps);
-
 	return 0;
 }
 
-static int ip_vs_stats_percpu_seq_open(struct inode *inode, struct file *file)
+static int ip_vs_estats_seq_open(struct inode *inode, struct file *file)
 {
-	return single_open_net(inode, file, ip_vs_stats_percpu_show);
+	return single_open(file, ip_vs_estats_show, NULL);
 }
 
-static const struct file_operations ip_vs_stats_percpu_fops = {
+static const struct file_operations ip_vs_estats_fops = {
 	.owner = THIS_MODULE,
-	.open = ip_vs_stats_percpu_seq_open,
+	.open = ip_vs_estats_seq_open,
 	.read = seq_read,
 	.llseek = seq_lseek,
-	.release = single_release_net,
+	.release = single_release,
 };
 #endif
 
 /*
  *	Set timeout values for tcp tcpfin udp in the timeout_table.
  */
-static int ip_vs_set_timeout(struct net *net, struct ip_vs_timeout_user *u)
+static int ip_vs_set_timeout(struct ip_vs_timeout_user *u)
 {
-#if defined(CONFIG_IP_VS_PROTO_TCP) || defined(CONFIG_IP_VS_PROTO_UDP)
-	struct ip_vs_proto_data *pd;
-#endif
-
 	IP_VS_DBG(2, "Setting timeout tcp:%d tcpfin:%d udp:%d\n",
-		  u->tcp_timeout,
-		  u->tcp_fin_timeout,
-		  u->udp_timeout);
+		  u->tcp_timeout, u->tcp_fin_timeout, u->udp_timeout);
 
 #ifdef CONFIG_IP_VS_PROTO_TCP
 	if (u->tcp_timeout) {
-		pd = ip_vs_proto_data_get(net, IPPROTO_TCP);
-		pd->timeout_table[IP_VS_TCP_S_ESTABLISHED]
-			= u->tcp_timeout * HZ;
+		ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_ESTABLISHED]
+		    = u->tcp_timeout * HZ;
 	}
 
 	if (u->tcp_fin_timeout) {
-		pd = ip_vs_proto_data_get(net, IPPROTO_TCP);
-		pd->timeout_table[IP_VS_TCP_S_FIN_WAIT]
-			= u->tcp_fin_timeout * HZ;
+		ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_FIN_WAIT]
+		    = u->tcp_fin_timeout * HZ;
 	}
 #endif
 
 #ifdef CONFIG_IP_VS_PROTO_UDP
 	if (u->udp_timeout) {
-		pd = ip_vs_proto_data_get(net, IPPROTO_UDP);
-		pd->timeout_table[IP_VS_UDP_S_NORMAL]
-			= u->udp_timeout * HZ;
+		ip_vs_protocol_udp.timeout_table[IP_VS_UDP_S_NORMAL]
+		    = u->udp_timeout * HZ;
 	}
 #endif
 	return 0;
 }
 
-
 #define SET_CMDID(cmd)		(cmd - IP_VS_BASE_CTL)
 #define SERVICE_ARG_LEN		(sizeof(struct ip_vs_service_user))
 #define SVCDEST_ARG_LEN		(sizeof(struct ip_vs_service_user) +	\
 				 sizeof(struct ip_vs_dest_user))
+#define SVCLADDR_ARG_LEN	(sizeof(struct ip_vs_service_user) +	\
+				 sizeof(struct ip_vs_laddr_user))
 #define TIMEOUT_ARG_LEN		(sizeof(struct ip_vs_timeout_user))
 #define DAEMON_ARG_LEN		(sizeof(struct ip_vs_daemon_user))
 #define MAX_ARG_LEN		SVCDEST_ARG_LEN
 
-static const unsigned char set_arglen[SET_CMDID(IP_VS_SO_SET_MAX)+1] = {
-	[SET_CMDID(IP_VS_SO_SET_ADD)]		= SERVICE_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_EDIT)]		= SERVICE_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_DEL)]		= SERVICE_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_FLUSH)]		= 0,
-	[SET_CMDID(IP_VS_SO_SET_ADDDEST)]	= SVCDEST_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_DELDEST)]	= SVCDEST_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_EDITDEST)]	= SVCDEST_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_TIMEOUT)]	= TIMEOUT_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_STARTDAEMON)]	= DAEMON_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_STOPDAEMON)]	= DAEMON_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_ZERO)]		= SERVICE_ARG_LEN,
+static const unsigned char set_arglen[SET_CMDID(IP_VS_SO_SET_MAX) + 1] = {
+	[SET_CMDID(IP_VS_SO_SET_ADD)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_EDIT)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DEL)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_FLUSH)] = 0,
+	[SET_CMDID(IP_VS_SO_SET_ADDDEST)] = SVCDEST_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DELDEST)] = SVCDEST_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_EDITDEST)] = SVCDEST_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_TIMEOUT)] = TIMEOUT_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_STARTDAEMON)] = DAEMON_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_STOPDAEMON)] = DAEMON_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_ZERO)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_ADDLADDR)] = SVCLADDR_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DELLADDR)] = SVCLADDR_ARG_LEN,
 };
 
 static void ip_vs_copy_usvc_compat(struct ip_vs_service_user_kern *usvc,
-				  struct ip_vs_service_user *usvc_compat)
+				   struct ip_vs_service_user *usvc_compat)
 {
-	memset(usvc, 0, sizeof(*usvc));
-
-	usvc->af		= AF_INET;
-	usvc->protocol		= usvc_compat->protocol;
-	usvc->addr.ip		= usvc_compat->addr;
-	usvc->port		= usvc_compat->port;
-	usvc->fwmark		= usvc_compat->fwmark;
+	usvc->af = AF_INET;
+	usvc->protocol = usvc_compat->protocol;
+	usvc->addr.ip = usvc_compat->addr;
+	usvc->port = usvc_compat->port;
+	usvc->fwmark = usvc_compat->fwmark;
 
 	/* Deep copy of sched_name is not needed here */
-	usvc->sched_name	= usvc_compat->sched_name;
+	usvc->sched_name = usvc_compat->sched_name;
 
-	usvc->flags		= usvc_compat->flags;
-	usvc->timeout		= usvc_compat->timeout;
-	usvc->netmask		= usvc_compat->netmask;
+	usvc->flags = usvc_compat->flags;
+	usvc->timeout = usvc_compat->timeout;
+	usvc->netmask = usvc_compat->netmask;
 }
 
 static void ip_vs_copy_udest_compat(struct ip_vs_dest_user_kern *udest,
-				   struct ip_vs_dest_user *udest_compat)
+				    struct ip_vs_dest_user *udest_compat)
 {
-	memset(udest, 0, sizeof(*udest));
+	udest->addr.ip = udest_compat->addr;
+	udest->port = udest_compat->port;
+	udest->conn_flags = udest_compat->conn_flags;
+	udest->weight = udest_compat->weight;
+	udest->u_threshold = udest_compat->u_threshold;
+	udest->l_threshold = udest_compat->l_threshold;
+}
 
-	udest->addr.ip		= udest_compat->addr;
-	udest->port		= udest_compat->port;
-	udest->conn_flags	= udest_compat->conn_flags;
-	udest->weight		= udest_compat->weight;
-	udest->u_threshold	= udest_compat->u_threshold;
-	udest->l_threshold	= udest_compat->l_threshold;
+static void ip_vs_copy_uladdr_compat(struct ip_vs_laddr_user_kern *uladdr,
+				     struct ip_vs_laddr_user *uladdr_compat)
+{
+	uladdr->addr.ip = uladdr_compat->addr;
 }
 
 static int
-do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
+do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user * user, unsigned int len)
 {
-	struct net *net = sock_net(sk);
 	int ret;
 	unsigned char arg[MAX_ARG_LEN];
 	struct ip_vs_service_user *usvc_compat;
@@ -2322,15 +2638,12 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 	struct ip_vs_service *svc;
 	struct ip_vs_dest_user *udest_compat;
 	struct ip_vs_dest_user_kern udest;
-	struct netns_ipvs *ipvs = net_ipvs(net);
+	struct ip_vs_laddr_user *uladdr_compat;
+	struct ip_vs_laddr_user_kern uladdr;
 
-	if (!ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))
+	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
 
-	if (cmd < IP_VS_BASE_CTL || cmd > IP_VS_SO_SET_MAX)
-		return -EINVAL;
-	if (len < 0 || len >  MAX_ARG_LEN)
-		return -EINVAL;
 	if (len != set_arglen[SET_CMDID(cmd)]) {
 		pr_err("set_ctl: len %u != %u\n",
 		       len, set_arglen[SET_CMDID(cmd)]);
@@ -2343,24 +2656,6 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 	/* increase the module use count */
 	ip_vs_use_count_inc();
 
-	/* Handle daemons since they have another lock */
-	if (cmd == IP_VS_SO_SET_STARTDAEMON ||
-	    cmd == IP_VS_SO_SET_STOPDAEMON) {
-		struct ip_vs_daemon_user *dm = (struct ip_vs_daemon_user *)arg;
-
-		if (mutex_lock_interruptible(&ipvs->sync_mutex)) {
-			ret = -ERESTARTSYS;
-			goto out_dec;
-		}
-		if (cmd == IP_VS_SO_SET_STARTDAEMON)
-			ret = start_sync_thread(net, dm->state, dm->mcast_ifn,
-						dm->syncid);
-		else
-			ret = stop_sync_thread(net, dm->state);
-		mutex_unlock(&ipvs->sync_mutex);
-		goto out_dec;
-	}
-
 	if (mutex_lock_interruptible(&__ip_vs_mutex)) {
 		ret = -ERESTARTSYS;
 		goto out_dec;
@@ -2368,33 +2663,40 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 
 	if (cmd == IP_VS_SO_SET_FLUSH) {
 		/* Flush the virtual service */
-		ret = ip_vs_flush(net, false);
+		ret = ip_vs_flush();
 		goto out_unlock;
 	} else if (cmd == IP_VS_SO_SET_TIMEOUT) {
 		/* Set timeout values for (tcp tcpfin udp) */
-		ret = ip_vs_set_timeout(net, (struct ip_vs_timeout_user *)arg);
+		ret = ip_vs_set_timeout((struct ip_vs_timeout_user *)arg);
+		goto out_unlock;
+	} else if (cmd == IP_VS_SO_SET_STARTDAEMON) {
+		struct ip_vs_daemon_user *dm = (struct ip_vs_daemon_user *)arg;
+		ret = start_sync_thread(dm->state, dm->mcast_ifn, dm->syncid);
+		goto out_unlock;
+	} else if (cmd == IP_VS_SO_SET_STOPDAEMON) {
+		struct ip_vs_daemon_user *dm = (struct ip_vs_daemon_user *)arg;
+		ret = stop_sync_thread(dm->state);
 		goto out_unlock;
 	}
 
 	usvc_compat = (struct ip_vs_service_user *)arg;
 	udest_compat = (struct ip_vs_dest_user *)(usvc_compat + 1);
+	uladdr_compat = (struct ip_vs_laddr_user *)(usvc_compat + 1);
 
 	/* We only use the new structs internally, so copy userspace compat
 	 * structs to extended internal versions */
 	ip_vs_copy_usvc_compat(&usvc, usvc_compat);
-	ip_vs_copy_udest_compat(&udest, udest_compat);
 
 	if (cmd == IP_VS_SO_SET_ZERO) {
 		/* if no service address is set, zero counters in all */
 		if (!usvc.fwmark && !usvc.addr.ip && !usvc.port) {
-			ret = ip_vs_zero_all(net);
+			ret = ip_vs_zero_all();
 			goto out_unlock;
 		}
 	}
 
-	/* Check for valid protocol: TCP or UDP or SCTP, even for fwmark!=0 */
-	if (usvc.protocol != IPPROTO_TCP && usvc.protocol != IPPROTO_UDP &&
-	    usvc.protocol != IPPROTO_SCTP) {
+	/* Check for valid protocol: TCP or UDP, even for fwmark!=0 */
+	if (usvc.protocol != IPPROTO_TCP && usvc.protocol != IPPROTO_UDP) {
 		pr_err("set_ctl: invalid protocol: %d %pI4:%d %s\n",
 		       usvc.protocol, &usvc.addr.ip,
 		       ntohs(usvc.port), usvc.sched_name);
@@ -2403,13 +2705,11 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 	}
 
 	/* Lookup the exact service by <protocol, addr, port> or fwmark */
-	rcu_read_lock();
 	if (usvc.fwmark == 0)
-		svc = __ip_vs_service_find(net, usvc.af, usvc.protocol,
-					   &usvc.addr, usvc.port);
+		svc = __ip_vs_service_get(usvc.af, usvc.protocol,
+					  &usvc.addr, usvc.port);
 	else
-		svc = __ip_vs_svc_fwm_find(net, usvc.af, usvc.fwmark);
-	rcu_read_unlock();
+		svc = __ip_vs_svc_fwm_get(usvc.af, usvc.fwmark);
 
 	if (cmd != IP_VS_SO_SET_ADD
 	    && (svc == NULL || svc->protocol != usvc.protocol)) {
@@ -2422,7 +2722,7 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 		if (svc != NULL)
 			ret = -EEXIST;
 		else
-			ret = ip_vs_add_service(net, &usvc, &svc);
+			ret = ip_vs_add_service(&usvc, &svc);
 		break;
 	case IP_VS_SO_SET_EDIT:
 		ret = ip_vs_edit_service(svc, &usvc);
@@ -2436,60 +2736,89 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 		ret = ip_vs_zero_service(svc);
 		break;
 	case IP_VS_SO_SET_ADDDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_add_dest(svc, &udest);
 		break;
 	case IP_VS_SO_SET_EDITDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_edit_dest(svc, &udest);
 		break;
 	case IP_VS_SO_SET_DELDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_del_dest(svc, &udest);
 		break;
+	case IP_VS_SO_SET_ADDLADDR:
+		ip_vs_copy_uladdr_compat(&uladdr, uladdr_compat);
+		ret = ip_vs_add_laddr(svc, &uladdr);
+		break;
+	case IP_VS_SO_SET_DELLADDR:
+		ip_vs_copy_uladdr_compat(&uladdr, uladdr_compat);
+		ret = ip_vs_del_laddr(svc, &uladdr);
+		break;
 	default:
 		ret = -EINVAL;
 	}
 
-  out_unlock:
+	if (svc)
+		ip_vs_service_put(svc);
+
+      out_unlock:
 	mutex_unlock(&__ip_vs_mutex);
-  out_dec:
+      out_dec:
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
 
 	return ret;
 }
 
+static void
+ip_vs_copy_stats(struct ip_vs_stats_user *dst, struct ip_vs_stats *src)
+{
+	int i = 0;
+
+	/* Set rate related field as zero due estimator is discard in ipvs kernel */
+	memset(dst, 0x00, sizeof(struct ip_vs_stats_user));
+
+	for_each_online_cpu(i) {
+		dst->conns    += ip_vs_stats_cpu(src, i).conns;
+		dst->inpkts   += ip_vs_stats_cpu(src, i).inpkts;
+		dst->outpkts  += ip_vs_stats_cpu(src, i).outpkts;
+		dst->inbytes  += ip_vs_stats_cpu(src, i).inbytes;
+		dst->outbytes += ip_vs_stats_cpu(src, i).outbytes;
+	}
+
+	return;
+}
 
 static void
 ip_vs_copy_service(struct ip_vs_service_entry *dst, struct ip_vs_service *src)
 {
-	struct ip_vs_scheduler *sched;
-
-	sched = rcu_dereference_protected(src->scheduler, 1);
 	dst->protocol = src->protocol;
 	dst->addr = src->addr.ip;
 	dst->port = src->port;
 	dst->fwmark = src->fwmark;
-	strlcpy(dst->sched_name, sched->name, sizeof(dst->sched_name));
+	strlcpy(dst->sched_name, src->scheduler->name, sizeof(dst->sched_name));
 	dst->flags = src->flags;
 	dst->timeout = src->timeout / HZ;
 	dst->netmask = src->netmask;
 	dst->num_dests = src->num_dests;
-	ip_vs_copy_stats(&dst->stats, &src->stats);
+	dst->num_laddrs = src->num_laddrs;
+	ip_vs_copy_stats(&dst->stats, src->stats);
 }
 
 static inline int
-__ip_vs_get_service_entries(struct net *net,
-			    const struct ip_vs_get_services *get,
-			    struct ip_vs_get_services __user *uptr)
+__ip_vs_get_service_entries(const struct ip_vs_get_services *get,
+			    struct ip_vs_get_services __user * uptr)
 {
-	int idx, count=0;
+	int idx, count = 0;
 	struct ip_vs_service *svc;
 	struct ip_vs_service_entry entry;
 	int ret = 0;
 
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
+		list_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
 			/* Only expose IPv4 entries to old interface */
-			if (svc->af != AF_INET || !net_eq(svc->net, net))
+			if (svc->af != AF_INET)
 				continue;
 
 			if (count >= get->num_services)
@@ -2506,9 +2835,9 @@ __ip_vs_get_service_entries(struct net *net,
 	}
 
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
+		list_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
 			/* Only expose IPv4 entries to old interface */
-			if (svc->af != AF_INET || !net_eq(svc->net, net))
+			if (svc->af != AF_INET)
 				continue;
 
 			if (count >= get->num_services)
@@ -2523,32 +2852,29 @@ __ip_vs_get_service_entries(struct net *net,
 			count++;
 		}
 	}
-out:
+      out:
 	return ret;
 }
 
 static inline int
-__ip_vs_get_dest_entries(struct net *net, const struct ip_vs_get_dests *get,
-			 struct ip_vs_get_dests __user *uptr)
+__ip_vs_get_dest_entries(const struct ip_vs_get_dests *get,
+			 struct ip_vs_get_dests __user * uptr)
 {
 	struct ip_vs_service *svc;
-	union nf_inet_addr addr = { .ip = get->addr };
+	union nf_inet_addr addr = {.ip = get->addr };
 	int ret = 0;
 
-	rcu_read_lock();
 	if (get->fwmark)
-		svc = __ip_vs_svc_fwm_find(net, AF_INET, get->fwmark);
+		svc = __ip_vs_svc_fwm_get(AF_INET, get->fwmark);
 	else
-		svc = __ip_vs_service_find(net, AF_INET, get->protocol, &addr,
-					   get->port);
-	rcu_read_unlock();
+		svc = __ip_vs_service_get(AF_INET, get->protocol, &addr,
+					  get->port);
 
 	if (svc) {
 		int count = 0;
 		struct ip_vs_dest *dest;
 		struct ip_vs_dest_entry entry;
 
-		memset(&entry, 0, sizeof(entry));
 		list_for_each_entry(dest, &svc->destinations, n_list) {
 			if (count >= get->num_dests)
 				break;
@@ -2562,7 +2888,7 @@ __ip_vs_get_dest_entries(struct net *net, const struct ip_vs_get_dests *get,
 			entry.activeconns = atomic_read(&dest->activeconns);
 			entry.inactconns = atomic_read(&dest->inactconns);
 			entry.persistconns = atomic_read(&dest->persistconns);
-			ip_vs_copy_stats(&entry.stats, &dest->stats);
+			ip_vs_copy_stats(&entry.stats, dest->stats);
 			if (copy_to_user(&uptr->entrytable[count],
 					 &entry, sizeof(entry))) {
 				ret = -EFAULT;
@@ -2570,224 +2896,263 @@ __ip_vs_get_dest_entries(struct net *net, const struct ip_vs_get_dests *get,
 			}
 			count++;
 		}
+		ip_vs_service_put(svc);
 	} else
 		ret = -ESRCH;
 	return ret;
 }
 
-static inline void
-__ip_vs_get_timeouts(struct net *net, struct ip_vs_timeout_user *u)
+static inline int
+__ip_vs_get_laddr_entries(const struct ip_vs_get_laddrs *get,
+			  struct ip_vs_get_laddrs __user * uptr)
 {
-#if defined(CONFIG_IP_VS_PROTO_TCP) || defined(CONFIG_IP_VS_PROTO_UDP)
-	struct ip_vs_proto_data *pd;
-#endif
+	struct ip_vs_service *svc;
+	union nf_inet_addr addr = {.ip = get->addr };
+	int ret = 0;
 
-	memset(u, 0, sizeof (*u));
+	if (get->fwmark)
+		svc = __ip_vs_svc_fwm_get(AF_INET, get->fwmark);
+	else
+		svc = __ip_vs_service_get(AF_INET, get->protocol, &addr,
+					  get->port);
+
+	if (svc) {
+		int count = 0;
+		struct ip_vs_laddr *laddr;
+		struct ip_vs_laddr_entry entry;
+
+		list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+			if (count >= get->num_laddrs)
+				break;
 
+			entry.addr = laddr->addr.ip;
+			entry.port_conflict =
+			    atomic64_read(&laddr->port_conflict);
+			entry.conn_counts = atomic_read(&laddr->conn_counts);
+			if (copy_to_user(&uptr->entrytable[count],
+					 &entry, sizeof(entry))) {
+				ret = -EFAULT;
+				break;
+			}
+			count++;
+		}
+		ip_vs_service_put(svc);
+	} else
+		ret = -ESRCH;
+	return ret;
+}
+
+static inline void __ip_vs_get_timeouts(struct ip_vs_timeout_user *u)
+{
 #ifdef CONFIG_IP_VS_PROTO_TCP
-	pd = ip_vs_proto_data_get(net, IPPROTO_TCP);
-	u->tcp_timeout = pd->timeout_table[IP_VS_TCP_S_ESTABLISHED] / HZ;
-	u->tcp_fin_timeout = pd->timeout_table[IP_VS_TCP_S_FIN_WAIT] / HZ;
+	u->tcp_timeout =
+	    ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_ESTABLISHED] / HZ;
+	u->tcp_fin_timeout =
+	    ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_FIN_WAIT] / HZ;
 #endif
 #ifdef CONFIG_IP_VS_PROTO_UDP
-	pd = ip_vs_proto_data_get(net, IPPROTO_UDP);
 	u->udp_timeout =
-			pd->timeout_table[IP_VS_UDP_S_NORMAL] / HZ;
+	    ip_vs_protocol_udp.timeout_table[IP_VS_UDP_S_NORMAL] / HZ;
 #endif
 }
 
-
 #define GET_CMDID(cmd)		(cmd - IP_VS_BASE_CTL)
 #define GET_INFO_ARG_LEN	(sizeof(struct ip_vs_getinfo))
 #define GET_SERVICES_ARG_LEN	(sizeof(struct ip_vs_get_services))
 #define GET_SERVICE_ARG_LEN	(sizeof(struct ip_vs_service_entry))
 #define GET_DESTS_ARG_LEN	(sizeof(struct ip_vs_get_dests))
+#define GET_LADDRS_ARG_LEN	(sizeof(struct ip_vs_get_laddrs))
 #define GET_TIMEOUT_ARG_LEN	(sizeof(struct ip_vs_timeout_user))
 #define GET_DAEMON_ARG_LEN	(sizeof(struct ip_vs_daemon_user) * 2)
 
-static const unsigned char get_arglen[GET_CMDID(IP_VS_SO_GET_MAX)+1] = {
-	[GET_CMDID(IP_VS_SO_GET_VERSION)]	= 64,
-	[GET_CMDID(IP_VS_SO_GET_INFO)]		= GET_INFO_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_SERVICES)]	= GET_SERVICES_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_SERVICE)]	= GET_SERVICE_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_DESTS)]		= GET_DESTS_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_TIMEOUT)]	= GET_TIMEOUT_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_DAEMON)]	= GET_DAEMON_ARG_LEN,
+static const unsigned char get_arglen[GET_CMDID(IP_VS_SO_GET_MAX) + 1] = {
+	[GET_CMDID(IP_VS_SO_GET_VERSION)] = 64,
+	[GET_CMDID(IP_VS_SO_GET_INFO)] = GET_INFO_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_SERVICES)] = GET_SERVICES_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_SERVICE)] = GET_SERVICE_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_DESTS)] = GET_DESTS_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_LADDRS)] = GET_LADDRS_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_TIMEOUT)] = GET_TIMEOUT_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_DAEMON)] = GET_DAEMON_ARG_LEN,
 };
 
 static int
-do_ip_vs_get_ctl(struct sock *sk, int cmd, void __user *user, int *len)
+do_ip_vs_get_ctl(struct sock *sk, int cmd, void __user * user, int *len)
 {
 	unsigned char arg[128];
 	int ret = 0;
-	unsigned int copylen;
-	struct net *net = sock_net(sk);
-	struct netns_ipvs *ipvs = net_ipvs(net);
 
-	BUG_ON(!net);
-	if (!ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))
+	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
 
-	if (cmd < IP_VS_BASE_CTL || cmd > IP_VS_SO_GET_MAX)
-		return -EINVAL;
-
 	if (*len < get_arglen[GET_CMDID(cmd)]) {
 		pr_err("get_ctl: len %u < %u\n",
 		       *len, get_arglen[GET_CMDID(cmd)]);
 		return -EINVAL;
 	}
 
-	copylen = get_arglen[GET_CMDID(cmd)];
-	if (copylen > 128)
-		return -EINVAL;
-
-	if (copy_from_user(arg, user, copylen) != 0)
+	if (copy_from_user(arg, user, get_arglen[GET_CMDID(cmd)]) != 0)
 		return -EFAULT;
-	/*
-	 * Handle daemons first since it has its own locking
-	 */
-	if (cmd == IP_VS_SO_GET_DAEMON) {
-		struct ip_vs_daemon_user d[2];
-
-		memset(&d, 0, sizeof(d));
-		if (mutex_lock_interruptible(&ipvs->sync_mutex))
-			return -ERESTARTSYS;
-
-		if (ipvs->sync_state & IP_VS_STATE_MASTER) {
-			d[0].state = IP_VS_STATE_MASTER;
-			strlcpy(d[0].mcast_ifn, ipvs->master_mcast_ifn,
-				sizeof(d[0].mcast_ifn));
-			d[0].syncid = ipvs->master_syncid;
-		}
-		if (ipvs->sync_state & IP_VS_STATE_BACKUP) {
-			d[1].state = IP_VS_STATE_BACKUP;
-			strlcpy(d[1].mcast_ifn, ipvs->backup_mcast_ifn,
-				sizeof(d[1].mcast_ifn));
-			d[1].syncid = ipvs->backup_syncid;
-		}
-		if (copy_to_user(user, &d, sizeof(d)) != 0)
-			ret = -EFAULT;
-		mutex_unlock(&ipvs->sync_mutex);
-		return ret;
-	}
 
 	if (mutex_lock_interruptible(&__ip_vs_mutex))
 		return -ERESTARTSYS;
 
 	switch (cmd) {
 	case IP_VS_SO_GET_VERSION:
-	{
-		char buf[64];
-
-		sprintf(buf, "IP Virtual Server version %d.%d.%d (size=%d)",
-			NVERSION(IP_VS_VERSION_CODE), ip_vs_conn_tab_size);
-		if (copy_to_user(user, buf, strlen(buf)+1) != 0) {
-			ret = -EFAULT;
-			goto out;
+		{
+			char buf[64];
+
+			sprintf(buf,
+				"IP Virtual Server version %d.%d.%d (size=%d)",
+				NVERSION(IP_VS_VERSION_CODE),
+				IP_VS_CONN_TAB_SIZE);
+			if (copy_to_user(user, buf, strlen(buf) + 1) != 0) {
+				ret = -EFAULT;
+				goto out;
+			}
+			*len = strlen(buf) + 1;
 		}
-		*len = strlen(buf)+1;
-	}
-	break;
+		break;
 
 	case IP_VS_SO_GET_INFO:
-	{
-		struct ip_vs_getinfo info;
-		info.version = IP_VS_VERSION_CODE;
-		info.size = ip_vs_conn_tab_size;
-		info.num_services = ipvs->num_services;
-		if (copy_to_user(user, &info, sizeof(info)) != 0)
-			ret = -EFAULT;
-	}
-	break;
+		{
+			struct ip_vs_getinfo info;
+			info.version = IP_VS_VERSION_CODE;
+			info.size = IP_VS_CONN_TAB_SIZE;
+			info.num_services = ip_vs_num_services;
+			if (copy_to_user(user, &info, sizeof(info)) != 0)
+				ret = -EFAULT;
+		}
+		break;
 
 	case IP_VS_SO_GET_SERVICES:
-	{
-		struct ip_vs_get_services *get;
-		int size;
-
-		get = (struct ip_vs_get_services *)arg;
-		size = sizeof(*get) +
-			sizeof(struct ip_vs_service_entry) * get->num_services;
-		if (*len != size) {
-			pr_err("length: %u != %u\n", *len, size);
-			ret = -EINVAL;
-			goto out;
+		{
+			struct ip_vs_get_services *get;
+			int size;
+
+			get = (struct ip_vs_get_services *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_service_entry) *
+			    get->num_services;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_service_entries(get, user);
 		}
-		ret = __ip_vs_get_service_entries(net, get, user);
-	}
-	break;
+		break;
 
 	case IP_VS_SO_GET_SERVICE:
-	{
-		struct ip_vs_service_entry *entry;
-		struct ip_vs_service *svc;
-		union nf_inet_addr addr;
-
-		entry = (struct ip_vs_service_entry *)arg;
-		addr.ip = entry->addr;
-		rcu_read_lock();
-		if (entry->fwmark)
-			svc = __ip_vs_svc_fwm_find(net, AF_INET, entry->fwmark);
-		else
-			svc = __ip_vs_service_find(net, AF_INET,
-						   entry->protocol, &addr,
-						   entry->port);
-		rcu_read_unlock();
-		if (svc) {
-			ip_vs_copy_service(entry, svc);
-			if (copy_to_user(user, entry, sizeof(*entry)) != 0)
-				ret = -EFAULT;
-		} else
-			ret = -ESRCH;
-	}
-	break;
+		{
+			struct ip_vs_service_entry *entry;
+			struct ip_vs_service *svc;
+			union nf_inet_addr addr;
+
+			entry = (struct ip_vs_service_entry *)arg;
+			addr.ip = entry->addr;
+			if (entry->fwmark)
+				svc =
+				    __ip_vs_svc_fwm_get(AF_INET, entry->fwmark);
+			else
+				svc =
+				    __ip_vs_service_get(AF_INET,
+							entry->protocol, &addr,
+							entry->port);
+			if (svc) {
+				ip_vs_copy_service(entry, svc);
+				if (copy_to_user(user, entry, sizeof(*entry)) !=
+				    0)
+					ret = -EFAULT;
+				ip_vs_service_put(svc);
+			} else
+				ret = -ESRCH;
+		}
+		break;
 
 	case IP_VS_SO_GET_DESTS:
-	{
-		struct ip_vs_get_dests *get;
-		int size;
-
-		get = (struct ip_vs_get_dests *)arg;
-		size = sizeof(*get) +
-			sizeof(struct ip_vs_dest_entry) * get->num_dests;
-		if (*len != size) {
-			pr_err("length: %u != %u\n", *len, size);
-			ret = -EINVAL;
-			goto out;
+		{
+			struct ip_vs_get_dests *get;
+			int size;
+
+			get = (struct ip_vs_get_dests *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_dest_entry) * get->num_dests;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_dest_entries(get, user);
 		}
-		ret = __ip_vs_get_dest_entries(net, get, user);
-	}
-	break;
+		break;
 
+	case IP_VS_SO_GET_LADDRS:
+		{
+			struct ip_vs_get_laddrs *get;
+			int size;
+
+			get = (struct ip_vs_get_laddrs *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_laddr_entry) * get->num_laddrs;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_laddr_entries(get, user);
+		}
+		break;
 	case IP_VS_SO_GET_TIMEOUT:
-	{
-		struct ip_vs_timeout_user t;
+		{
+			struct ip_vs_timeout_user t;
 
-		__ip_vs_get_timeouts(net, &t);
-		if (copy_to_user(user, &t, sizeof(t)) != 0)
-			ret = -EFAULT;
-	}
-	break;
+			__ip_vs_get_timeouts(&t);
+			if (copy_to_user(user, &t, sizeof(t)) != 0)
+				ret = -EFAULT;
+		}
+		break;
+
+	case IP_VS_SO_GET_DAEMON:
+		{
+			struct ip_vs_daemon_user d[2];
+
+			memset(&d, 0, sizeof(d));
+			if (ip_vs_sync_state & IP_VS_STATE_MASTER) {
+				d[0].state = IP_VS_STATE_MASTER;
+				strlcpy(d[0].mcast_ifn, ip_vs_master_mcast_ifn,
+					sizeof(d[0].mcast_ifn));
+				d[0].syncid = ip_vs_master_syncid;
+			}
+			if (ip_vs_sync_state & IP_VS_STATE_BACKUP) {
+				d[1].state = IP_VS_STATE_BACKUP;
+				strlcpy(d[1].mcast_ifn, ip_vs_backup_mcast_ifn,
+					sizeof(d[1].mcast_ifn));
+				d[1].syncid = ip_vs_backup_syncid;
+			}
+			if (copy_to_user(user, &d, sizeof(d)) != 0)
+				ret = -EFAULT;
+		}
+		break;
 
 	default:
 		ret = -EINVAL;
 	}
 
-out:
+      out:
 	mutex_unlock(&__ip_vs_mutex);
 	return ret;
 }
 
-
 static struct nf_sockopt_ops ip_vs_sockopts = {
-	.pf		= PF_INET,
-	.set_optmin	= IP_VS_BASE_CTL,
-	.set_optmax	= IP_VS_SO_SET_MAX+1,
-	.set		= do_ip_vs_set_ctl,
-	.get_optmin	= IP_VS_BASE_CTL,
-	.get_optmax	= IP_VS_SO_GET_MAX+1,
-	.get		= do_ip_vs_get_ctl,
-	.owner		= THIS_MODULE,
+	.pf = PF_INET,
+	.set_optmin = IP_VS_BASE_CTL,
+	.set_optmax = IP_VS_SO_SET_MAX + 1,
+	.set = do_ip_vs_set_ctl,
+	.get_optmin = IP_VS_BASE_CTL,
+	.get_optmax = IP_VS_SO_GET_MAX + 1,
+	.get = do_ip_vs_get_ctl,
+	.owner = THIS_MODULE,
 };
 
 /*
@@ -2796,137 +3161,141 @@ static struct nf_sockopt_ops ip_vs_sockopts = {
 
 /* IPVS genetlink family */
 static struct genl_family ip_vs_genl_family = {
-	.id		= GENL_ID_GENERATE,
-	.hdrsize	= 0,
-	.name		= IPVS_GENL_NAME,
-	.version	= IPVS_GENL_VERSION,
-	.maxattr	= IPVS_CMD_MAX,
-	.netnsok        = true,         /* Make ipvsadm to work on netns */
+	.id = GENL_ID_GENERATE,
+	.hdrsize = 0,
+	.name = IPVS_GENL_NAME,
+	.version = IPVS_GENL_VERSION,
+	.maxattr = IPVS_CMD_MAX,
 };
 
 /* Policy used for first-level command attributes */
 static const struct nla_policy ip_vs_cmd_policy[IPVS_CMD_ATTR_MAX + 1] = {
-	[IPVS_CMD_ATTR_SERVICE]		= { .type = NLA_NESTED },
-	[IPVS_CMD_ATTR_DEST]		= { .type = NLA_NESTED },
-	[IPVS_CMD_ATTR_DAEMON]		= { .type = NLA_NESTED },
-	[IPVS_CMD_ATTR_TIMEOUT_TCP]	= { .type = NLA_U32 },
-	[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]	= { .type = NLA_U32 },
-	[IPVS_CMD_ATTR_TIMEOUT_UDP]	= { .type = NLA_U32 },
+	[IPVS_CMD_ATTR_SERVICE] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_DEST] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_DAEMON] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_TIMEOUT_TCP] = {.type = NLA_U32},
+	[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN] = {.type = NLA_U32},
+	[IPVS_CMD_ATTR_TIMEOUT_UDP] = {.type = NLA_U32},
+	[IPVS_CMD_ATTR_LADDR] = {.type = NLA_NESTED},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_DAEMON */
 static const struct nla_policy ip_vs_daemon_policy[IPVS_DAEMON_ATTR_MAX + 1] = {
-	[IPVS_DAEMON_ATTR_STATE]	= { .type = NLA_U32 },
-	[IPVS_DAEMON_ATTR_MCAST_IFN]	= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_IFNAME_MAXLEN },
-	[IPVS_DAEMON_ATTR_SYNC_ID]	= { .type = NLA_U32 },
+	[IPVS_DAEMON_ATTR_STATE] = {.type = NLA_U32},
+	[IPVS_DAEMON_ATTR_MCAST_IFN] = {.type = NLA_NUL_STRING,
+					.len = IP_VS_IFNAME_MAXLEN},
+	[IPVS_DAEMON_ATTR_SYNC_ID] = {.type = NLA_U32},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_SERVICE */
 static const struct nla_policy ip_vs_svc_policy[IPVS_SVC_ATTR_MAX + 1] = {
-	[IPVS_SVC_ATTR_AF]		= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_PROTOCOL]	= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_ADDR]		= { .type = NLA_BINARY,
-					    .len = sizeof(union nf_inet_addr) },
-	[IPVS_SVC_ATTR_PORT]		= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_FWMARK]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_SCHED_NAME]	= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_SCHEDNAME_MAXLEN },
-	[IPVS_SVC_ATTR_PE_NAME]		= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_PENAME_MAXLEN },
-	[IPVS_SVC_ATTR_FLAGS]		= { .type = NLA_BINARY,
-					    .len = sizeof(struct ip_vs_flags) },
-	[IPVS_SVC_ATTR_TIMEOUT]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_NETMASK]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_STATS]		= { .type = NLA_NESTED },
+	[IPVS_SVC_ATTR_AF] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_PROTOCOL] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_ADDR] = {.type = NLA_BINARY,
+				.len = sizeof(union nf_inet_addr)},
+	[IPVS_SVC_ATTR_PORT] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_FWMARK] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_SCHED_NAME] = {.type = NLA_NUL_STRING,
+				      .len = IP_VS_SCHEDNAME_MAXLEN},
+	[IPVS_SVC_ATTR_FLAGS] = {.type = NLA_BINARY,
+				 .len = sizeof(struct ip_vs_flags)},
+	[IPVS_SVC_ATTR_TIMEOUT] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_NETMASK] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_STATS] = {.type = NLA_NESTED},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_DEST */
 static const struct nla_policy ip_vs_dest_policy[IPVS_DEST_ATTR_MAX + 1] = {
-	[IPVS_DEST_ATTR_ADDR]		= { .type = NLA_BINARY,
-					    .len = sizeof(union nf_inet_addr) },
-	[IPVS_DEST_ATTR_PORT]		= { .type = NLA_U16 },
-	[IPVS_DEST_ATTR_FWD_METHOD]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_WEIGHT]		= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_U_THRESH]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_L_THRESH]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_ACTIVE_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_INACT_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_PERSIST_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_STATS]		= { .type = NLA_NESTED },
+	[IPVS_DEST_ATTR_ADDR] = {.type = NLA_BINARY,
+				 .len = sizeof(union nf_inet_addr)},
+	[IPVS_DEST_ATTR_PORT] = {.type = NLA_U16},
+	[IPVS_DEST_ATTR_FWD_METHOD] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_WEIGHT] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_U_THRESH] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_L_THRESH] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_ACTIVE_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_INACT_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_PERSIST_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_STATS] = {.type = NLA_NESTED},
+};
+
+static const struct nla_policy ip_vs_laddr_policy[IPVS_LADDR_ATTR_MAX + 1] = {
+	[IPVS_LADDR_ATTR_ADDR] = {.type = NLA_BINARY,
+				  .len = sizeof(union nf_inet_addr)},
+	[IPVS_LADDR_ATTR_PORT_CONFLICT] = {.type = NLA_U64},
+	[IPVS_LADDR_ATTR_CONN_COUNTS] = {.type = NLA_U32},
 };
 
 static int ip_vs_genl_fill_stats(struct sk_buff *skb, int container_type,
 				 struct ip_vs_stats *stats)
 {
-	struct ip_vs_stats_user ustats;
 	struct nlattr *nl_stats = nla_nest_start(skb, container_type);
+	struct ip_vs_stats tmp_stats;
+	int i = 0;
+
 	if (!nl_stats)
 		return -EMSGSIZE;
 
-	ip_vs_copy_stats(&ustats, stats);
-
-	if (nla_put_u32(skb, IPVS_STATS_ATTR_CONNS, ustats.conns) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_INPKTS, ustats.inpkts) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_OUTPKTS, ustats.outpkts) ||
-	    nla_put_u64(skb, IPVS_STATS_ATTR_INBYTES, ustats.inbytes) ||
-	    nla_put_u64(skb, IPVS_STATS_ATTR_OUTBYTES, ustats.outbytes) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_CPS, ustats.cps) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_INPPS, ustats.inpps) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_OUTPPS, ustats.outpps) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_INBPS, ustats.inbps) ||
-	    nla_put_u32(skb, IPVS_STATS_ATTR_OUTBPS, ustats.outbps))
-		goto nla_put_failure;
-	nla_nest_end(skb, nl_stats);
+	memset((void*)(&tmp_stats), 0x00, sizeof(struct ip_vs_stats));
+	for_each_online_cpu(i) {
+		tmp_stats.conns    += ip_vs_stats_cpu(stats, i).conns;
+		tmp_stats.inpkts   += ip_vs_stats_cpu(stats, i).inpkts;
+		tmp_stats.outpkts  += ip_vs_stats_cpu(stats, i).outpkts;
+		tmp_stats.inbytes  += ip_vs_stats_cpu(stats, i).inbytes;
+		tmp_stats.outbytes += ip_vs_stats_cpu(stats, i).outbytes;
+	}
 
-	return 0;
+        nla_put_u64(skb, IPVS_STATS_ATTR_CONNS,    tmp_stats.conns);
+        nla_put_u64(skb, IPVS_STATS_ATTR_INPKTS,   tmp_stats.inpkts);
+        nla_put_u64(skb, IPVS_STATS_ATTR_OUTPKTS,  tmp_stats.outpkts);
+        nla_put_u64(skb, IPVS_STATS_ATTR_INBYTES,  tmp_stats.inbytes);
+        nla_put_u64(skb, IPVS_STATS_ATTR_OUTBYTES, tmp_stats.outbytes);
+	nla_put_u32(skb, IPVS_STATS_ATTR_CPS,      0);
+	nla_put_u32(skb, IPVS_STATS_ATTR_INPPS,    0);
+	nla_put_u32(skb, IPVS_STATS_ATTR_OUTPPS,   0);
+	nla_put_u32(skb, IPVS_STATS_ATTR_INBPS,    0);
+	nla_put_u32(skb, IPVS_STATS_ATTR_OUTBPS,   0);
 
-nla_put_failure:
-	nla_nest_cancel(skb, nl_stats);
-	return -EMSGSIZE;
+	nla_nest_end(skb, nl_stats);
+
+	return 0;
 }
 
 static int ip_vs_genl_fill_service(struct sk_buff *skb,
 				   struct ip_vs_service *svc)
 {
-	struct ip_vs_scheduler *sched;
-	struct ip_vs_pe *pe;
 	struct nlattr *nl_service;
-	struct ip_vs_flags flags = { .flags = svc->flags,
-				     .mask = ~0 };
+	struct ip_vs_flags flags = {.flags = svc->flags,
+		.mask = ~0
+	};
 
 	nl_service = nla_nest_start(skb, IPVS_CMD_ATTR_SERVICE);
 	if (!nl_service)
 		return -EMSGSIZE;
 
-	if (nla_put_u16(skb, IPVS_SVC_ATTR_AF, svc->af))
-		goto nla_put_failure;
+	nla_put_u16(skb, IPVS_SVC_ATTR_AF, svc->af);
+
 	if (svc->fwmark) {
-		if (nla_put_u32(skb, IPVS_SVC_ATTR_FWMARK, svc->fwmark))
-			goto nla_put_failure;
+		nla_put_u32(skb, IPVS_SVC_ATTR_FWMARK, svc->fwmark);
 	} else {
-		if (nla_put_u16(skb, IPVS_SVC_ATTR_PROTOCOL, svc->protocol) ||
-		    nla_put(skb, IPVS_SVC_ATTR_ADDR, sizeof(svc->addr), &svc->addr) ||
-		    nla_put_be16(skb, IPVS_SVC_ATTR_PORT, svc->port))
-			goto nla_put_failure;
+		nla_put_u16(skb, IPVS_SVC_ATTR_PROTOCOL, svc->protocol);
+		nla_put(skb, IPVS_SVC_ATTR_ADDR, sizeof(svc->addr), &svc->addr);
+		nla_put_u16(skb, IPVS_SVC_ATTR_PORT, svc->port);
 	}
 
-	sched = rcu_dereference_protected(svc->scheduler, 1);
-	pe = rcu_dereference_protected(svc->pe, 1);
-	if (nla_put_string(skb, IPVS_SVC_ATTR_SCHED_NAME, sched->name) ||
-	    (pe && nla_put_string(skb, IPVS_SVC_ATTR_PE_NAME, pe->name)) ||
-	    nla_put(skb, IPVS_SVC_ATTR_FLAGS, sizeof(flags), &flags) ||
-	    nla_put_u32(skb, IPVS_SVC_ATTR_TIMEOUT, svc->timeout / HZ) ||
-	    nla_put_be32(skb, IPVS_SVC_ATTR_NETMASK, svc->netmask))
-		goto nla_put_failure;
-	if (ip_vs_genl_fill_stats(skb, IPVS_SVC_ATTR_STATS, &svc->stats))
+	nla_put_string(skb, IPVS_SVC_ATTR_SCHED_NAME, svc->scheduler->name);
+	nla_put(skb, IPVS_SVC_ATTR_FLAGS, sizeof(flags), &flags);
+	nla_put_u32(skb, IPVS_SVC_ATTR_TIMEOUT, svc->timeout / HZ);
+	nla_put_u32(skb, IPVS_SVC_ATTR_NETMASK, svc->netmask);
+
+	if (ip_vs_genl_fill_stats(skb, IPVS_SVC_ATTR_STATS, svc->stats))
 		goto nla_put_failure;
 
 	nla_nest_end(skb, nl_service);
 
 	return 0;
 
-nla_put_failure:
+      nla_put_failure:
 	nla_nest_cancel(skb, nl_service);
 	return -EMSGSIZE;
 }
@@ -2948,7 +3317,7 @@ static int ip_vs_genl_dump_service(struct sk_buff *skb,
 
 	return genlmsg_end(skb, hdr);
 
-nla_put_failure:
+      nla_put_failure:
 	genlmsg_cancel(skb, hdr);
 	return -EMSGSIZE;
 }
@@ -2959,12 +3328,11 @@ static int ip_vs_genl_dump_services(struct sk_buff *skb,
 	int idx = 0, i;
 	int start = cb->args[0];
 	struct ip_vs_service *svc;
-	struct net *net = skb_sknet(skb);
 
 	mutex_lock(&__ip_vs_mutex);
 	for (i = 0; i < IP_VS_SVC_TAB_SIZE; i++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_table[i], s_list) {
-			if (++idx <= start || !net_eq(svc->net, net))
+		list_for_each_entry(svc, &ip_vs_svc_table[i], s_list) {
+			if (++idx <= start)
 				continue;
 			if (ip_vs_genl_dump_service(skb, svc, cb) < 0) {
 				idx--;
@@ -2974,8 +3342,8 @@ static int ip_vs_genl_dump_services(struct sk_buff *skb,
 	}
 
 	for (i = 0; i < IP_VS_SVC_TAB_SIZE; i++) {
-		hlist_for_each_entry(svc, &ip_vs_svc_fwm_table[i], f_list) {
-			if (++idx <= start || !net_eq(svc->net, net))
+		list_for_each_entry(svc, &ip_vs_svc_fwm_table[i], f_list) {
+			if (++idx <= start)
 				continue;
 			if (ip_vs_genl_dump_service(skb, svc, cb) < 0) {
 				idx--;
@@ -2984,32 +3352,29 @@ static int ip_vs_genl_dump_services(struct sk_buff *skb,
 		}
 	}
 
-nla_put_failure:
+      nla_put_failure:
 	mutex_unlock(&__ip_vs_mutex);
 	cb->args[0] = idx;
 
 	return skb->len;
 }
 
-static int ip_vs_genl_parse_service(struct net *net,
-				    struct ip_vs_service_user_kern *usvc,
-				    struct nlattr *nla, int full_entry,
-				    struct ip_vs_service **ret_svc)
+static int ip_vs_genl_parse_service(struct ip_vs_service_user_kern *usvc,
+				    struct nlattr *nla, int full_entry)
 {
 	struct nlattr *attrs[IPVS_SVC_ATTR_MAX + 1];
 	struct nlattr *nla_af, *nla_port, *nla_fwmark, *nla_protocol, *nla_addr;
-	struct ip_vs_service *svc;
 
 	/* Parse mandatory identifying service fields first */
 	if (nla == NULL ||
 	    nla_parse_nested(attrs, IPVS_SVC_ATTR_MAX, nla, ip_vs_svc_policy))
 		return -EINVAL;
 
-	nla_af		= attrs[IPVS_SVC_ATTR_AF];
-	nla_protocol	= attrs[IPVS_SVC_ATTR_PROTOCOL];
-	nla_addr	= attrs[IPVS_SVC_ATTR_ADDR];
-	nla_port	= attrs[IPVS_SVC_ATTR_PORT];
-	nla_fwmark	= attrs[IPVS_SVC_ATTR_FWMARK];
+	nla_af = attrs[IPVS_SVC_ATTR_AF];
+	nla_protocol = attrs[IPVS_SVC_ATTR_PROTOCOL];
+	nla_addr = attrs[IPVS_SVC_ATTR_ADDR];
+	nla_port = attrs[IPVS_SVC_ATTR_PORT];
+	nla_fwmark = attrs[IPVS_SVC_ATTR_FWMARK];
 
 	if (!(nla_af && (nla_fwmark || (nla_port && nla_protocol && nla_addr))))
 		return -EINVAL;
@@ -3030,27 +3395,18 @@ static int ip_vs_genl_parse_service(struct net *net,
 	} else {
 		usvc->protocol = nla_get_u16(nla_protocol);
 		nla_memcpy(&usvc->addr, nla_addr, sizeof(usvc->addr));
-		usvc->port = nla_get_be16(nla_port);
+		usvc->port = nla_get_u16(nla_port);
 		usvc->fwmark = 0;
 	}
 
-	rcu_read_lock();
-	if (usvc->fwmark)
-		svc = __ip_vs_svc_fwm_find(net, usvc->af, usvc->fwmark);
-	else
-		svc = __ip_vs_service_find(net, usvc->af, usvc->protocol,
-					   &usvc->addr, usvc->port);
-	rcu_read_unlock();
-	*ret_svc = svc;
-
 	/* If a full entry was requested, check for the additional fields */
 	if (full_entry) {
-		struct nlattr *nla_sched, *nla_flags, *nla_pe, *nla_timeout,
-			      *nla_netmask;
+		struct nlattr *nla_sched, *nla_flags, *nla_timeout,
+		    *nla_netmask;
 		struct ip_vs_flags flags;
+		struct ip_vs_service *svc;
 
 		nla_sched = attrs[IPVS_SVC_ATTR_SCHED_NAME];
-		nla_pe = attrs[IPVS_SVC_ATTR_PE_NAME];
 		nla_flags = attrs[IPVS_SVC_ATTR_FLAGS];
 		nla_timeout = attrs[IPVS_SVC_ATTR_TIMEOUT];
 		nla_netmask = attrs[IPVS_SVC_ATTR_NETMASK];
@@ -3061,30 +3417,42 @@ static int ip_vs_genl_parse_service(struct net *net,
 		nla_memcpy(&flags, nla_flags, sizeof(flags));
 
 		/* prefill flags from service if it already exists */
-		if (svc)
+		if (usvc->fwmark)
+			svc = __ip_vs_svc_fwm_get(usvc->af, usvc->fwmark);
+		else
+			svc = __ip_vs_service_get(usvc->af, usvc->protocol,
+						  &usvc->addr, usvc->port);
+		if (svc) {
 			usvc->flags = svc->flags;
+			ip_vs_service_put(svc);
+		} else
+			usvc->flags = 0;
 
 		/* set new flags from userland */
 		usvc->flags = (usvc->flags & ~flags.mask) |
-			      (flags.flags & flags.mask);
+		    (flags.flags & flags.mask);
 		usvc->sched_name = nla_data(nla_sched);
-		usvc->pe_name = nla_pe ? nla_data(nla_pe) : NULL;
 		usvc->timeout = nla_get_u32(nla_timeout);
-		usvc->netmask = nla_get_be32(nla_netmask);
+		usvc->netmask = nla_get_u32(nla_netmask);
 	}
 
 	return 0;
 }
 
-static struct ip_vs_service *ip_vs_genl_find_service(struct net *net,
-						     struct nlattr *nla)
+static struct ip_vs_service *ip_vs_genl_find_service(struct nlattr *nla)
 {
 	struct ip_vs_service_user_kern usvc;
-	struct ip_vs_service *svc;
 	int ret;
 
-	ret = ip_vs_genl_parse_service(net, &usvc, nla, 0, &svc);
-	return ret ? ERR_PTR(ret) : svc;
+	ret = ip_vs_genl_parse_service(&usvc, nla, 0);
+	if (ret)
+		return ERR_PTR(ret);
+
+	if (usvc.fwmark)
+		return __ip_vs_svc_fwm_get(usvc.af, usvc.fwmark);
+	else
+		return __ip_vs_service_get(usvc.af, usvc.protocol,
+					   &usvc.addr, usvc.port);
 }
 
 static int ip_vs_genl_fill_dest(struct sk_buff *skb, struct ip_vs_dest *dest)
@@ -3095,30 +3463,29 @@ static int ip_vs_genl_fill_dest(struct sk_buff *skb, struct ip_vs_dest *dest)
 	if (!nl_dest)
 		return -EMSGSIZE;
 
-	if (nla_put(skb, IPVS_DEST_ATTR_ADDR, sizeof(dest->addr), &dest->addr) ||
-	    nla_put_be16(skb, IPVS_DEST_ATTR_PORT, dest->port) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_FWD_METHOD,
-			(atomic_read(&dest->conn_flags) &
-			 IP_VS_CONN_F_FWD_MASK)) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_WEIGHT,
-			atomic_read(&dest->weight)) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_U_THRESH, dest->u_threshold) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_L_THRESH, dest->l_threshold) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_ACTIVE_CONNS,
-			atomic_read(&dest->activeconns)) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_INACT_CONNS,
-			atomic_read(&dest->inactconns)) ||
-	    nla_put_u32(skb, IPVS_DEST_ATTR_PERSIST_CONNS,
-			atomic_read(&dest->persistconns)))
-		goto nla_put_failure;
-	if (ip_vs_genl_fill_stats(skb, IPVS_DEST_ATTR_STATS, &dest->stats))
+	nla_put(skb, IPVS_DEST_ATTR_ADDR, sizeof(dest->addr), &dest->addr);
+	nla_put_u16(skb, IPVS_DEST_ATTR_PORT, dest->port);
+
+	nla_put_u32(skb, IPVS_DEST_ATTR_FWD_METHOD,
+		    atomic_read(&dest->conn_flags) & IP_VS_CONN_F_FWD_MASK);
+	nla_put_u32(skb, IPVS_DEST_ATTR_WEIGHT, atomic_read(&dest->weight));
+	nla_put_u32(skb, IPVS_DEST_ATTR_U_THRESH, dest->u_threshold);
+	nla_put_u32(skb, IPVS_DEST_ATTR_L_THRESH, dest->l_threshold);
+	nla_put_u32(skb, IPVS_DEST_ATTR_ACTIVE_CONNS,
+		    atomic_read(&dest->activeconns));
+	nla_put_u32(skb, IPVS_DEST_ATTR_INACT_CONNS,
+		    atomic_read(&dest->inactconns));
+	nla_put_u32(skb, IPVS_DEST_ATTR_PERSIST_CONNS,
+		    atomic_read(&dest->persistconns));
+
+	if (ip_vs_genl_fill_stats(skb, IPVS_DEST_ATTR_STATS, dest->stats))
 		goto nla_put_failure;
 
 	nla_nest_end(skb, nl_dest);
 
 	return 0;
 
-nla_put_failure:
+      nla_put_failure:
 	nla_nest_cancel(skb, nl_dest);
 	return -EMSGSIZE;
 }
@@ -3129,8 +3496,7 @@ static int ip_vs_genl_dump_dest(struct sk_buff *skb, struct ip_vs_dest *dest,
 	void *hdr;
 
 	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,
-			  &ip_vs_genl_family, NLM_F_MULTI,
-			  IPVS_CMD_NEW_DEST);
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_DEST);
 	if (!hdr)
 		return -EMSGSIZE;
 
@@ -3139,7 +3505,7 @@ static int ip_vs_genl_dump_dest(struct sk_buff *skb, struct ip_vs_dest *dest,
 
 	return genlmsg_end(skb, hdr);
 
-nla_put_failure:
+      nla_put_failure:
 	genlmsg_cancel(skb, hdr);
 	return -EMSGSIZE;
 }
@@ -3152,7 +3518,6 @@ static int ip_vs_genl_dump_dests(struct sk_buff *skb,
 	struct ip_vs_service *svc;
 	struct ip_vs_dest *dest;
 	struct nlattr *attrs[IPVS_CMD_ATTR_MAX + 1];
-	struct net *net = skb_sknet(skb);
 
 	mutex_lock(&__ip_vs_mutex);
 
@@ -3161,8 +3526,7 @@ static int ip_vs_genl_dump_dests(struct sk_buff *skb,
 			IPVS_CMD_ATTR_MAX, ip_vs_cmd_policy))
 		goto out_err;
 
-
-	svc = ip_vs_genl_find_service(net, attrs[IPVS_CMD_ATTR_SERVICE]);
+	svc = ip_vs_genl_find_service(attrs[IPVS_CMD_ATTR_SERVICE]);
 	if (IS_ERR(svc) || svc == NULL)
 		goto out_err;
 
@@ -3176,15 +3540,120 @@ static int ip_vs_genl_dump_dests(struct sk_buff *skb,
 		}
 	}
 
-nla_put_failure:
+      nla_put_failure:
 	cb->args[0] = idx;
+	ip_vs_service_put(svc);
 
-out_err:
+      out_err:
 	mutex_unlock(&__ip_vs_mutex);
 
 	return skb->len;
 }
 
+static int ip_vs_genl_fill_laddr(struct sk_buff *skb, struct ip_vs_laddr *laddr)
+{
+	struct nlattr *nl_laddr;
+
+	nl_laddr = nla_nest_start(skb, IPVS_CMD_ATTR_LADDR);
+	if (!nl_laddr)
+		return -EMSGSIZE;
+
+	nla_put(skb, IPVS_LADDR_ATTR_ADDR, sizeof(laddr->addr), &laddr->addr);
+	nla_put_u64(skb, IPVS_LADDR_ATTR_PORT_CONFLICT,
+		    atomic64_read(&laddr->port_conflict));
+	nla_put_u32(skb, IPVS_LADDR_ATTR_CONN_COUNTS,
+		    atomic_read(&laddr->conn_counts));
+
+	nla_nest_end(skb, nl_laddr);
+
+	return 0;
+}
+
+static int ip_vs_genl_dump_laddr(struct sk_buff *skb, struct ip_vs_laddr *laddr,
+				 struct netlink_callback *cb)
+{
+	void *hdr;
+
+	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_LADDR);
+	if (!hdr)
+		return -EMSGSIZE;
+
+	if (ip_vs_genl_fill_laddr(skb, laddr) < 0)
+		goto nla_put_failure;
+
+	return genlmsg_end(skb, hdr);
+
+      nla_put_failure:
+	genlmsg_cancel(skb, hdr);
+	return -EMSGSIZE;
+}
+
+static int ip_vs_genl_dump_laddrs(struct sk_buff *skb,
+				  struct netlink_callback *cb)
+{
+	int idx = 0;
+	int start = cb->args[0];
+	struct ip_vs_service *svc;
+	struct ip_vs_laddr *laddr;
+	struct nlattr *attrs[IPVS_CMD_ATTR_MAX + 1];
+
+	mutex_lock(&__ip_vs_mutex);
+
+	/* Try to find the service for which to dump destinations */
+	if (nlmsg_parse(cb->nlh, GENL_HDRLEN, attrs,
+			IPVS_CMD_ATTR_MAX, ip_vs_cmd_policy))
+		goto out_err;
+
+	svc = ip_vs_genl_find_service(attrs[IPVS_CMD_ATTR_SERVICE]);
+	if (IS_ERR(svc) || svc == NULL)
+		goto out_err;
+
+	IP_VS_DBG_BUF(0, "vip %s:%d get local address \n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port));
+
+	/* Dump the destinations */
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		if (++idx <= start)
+			continue;
+
+		if (ip_vs_genl_dump_laddr(skb, laddr, cb) < 0) {
+			idx--;
+			goto nla_put_failure;
+		}
+	}
+
+      nla_put_failure:
+	cb->args[0] = idx;
+	ip_vs_service_put(svc);
+
+      out_err:
+	mutex_unlock(&__ip_vs_mutex);
+	return skb->len;
+}
+
+static int ip_vs_genl_parse_laddr(struct ip_vs_laddr_user_kern *uladdr,
+				  struct nlattr *nla, int full_entry)
+{
+	struct nlattr *attrs[IPVS_LADDR_ATTR_MAX + 1];
+	struct nlattr *nla_addr;
+
+	/* Parse mandatory identifying destination fields first */
+	if (nla == NULL ||
+	    nla_parse_nested(attrs, IPVS_LADDR_ATTR_MAX, nla,
+			     ip_vs_laddr_policy))
+		return -EINVAL;
+
+	nla_addr = attrs[IPVS_LADDR_ATTR_ADDR];
+	if (!nla_addr)
+		return -EINVAL;
+
+	memset(uladdr, 0, sizeof(*uladdr));
+	nla_memcpy(&uladdr->addr, nla_addr, sizeof(uladdr->addr));
+
+	return 0;
+}
+
 static int ip_vs_genl_parse_dest(struct ip_vs_dest_user_kern *udest,
 				 struct nlattr *nla, int full_entry)
 {
@@ -3196,8 +3665,8 @@ static int ip_vs_genl_parse_dest(struct ip_vs_dest_user_kern *udest,
 	    nla_parse_nested(attrs, IPVS_DEST_ATTR_MAX, nla, ip_vs_dest_policy))
 		return -EINVAL;
 
-	nla_addr	= attrs[IPVS_DEST_ATTR_ADDR];
-	nla_port	= attrs[IPVS_DEST_ATTR_PORT];
+	nla_addr = attrs[IPVS_DEST_ATTR_ADDR];
+	nla_port = attrs[IPVS_DEST_ATTR_PORT];
 
 	if (!(nla_addr && nla_port))
 		return -EINVAL;
@@ -3205,23 +3674,23 @@ static int ip_vs_genl_parse_dest(struct ip_vs_dest_user_kern *udest,
 	memset(udest, 0, sizeof(*udest));
 
 	nla_memcpy(&udest->addr, nla_addr, sizeof(udest->addr));
-	udest->port = nla_get_be16(nla_port);
+	udest->port = nla_get_u16(nla_port);
 
 	/* If a full entry was requested, check for the additional fields */
 	if (full_entry) {
 		struct nlattr *nla_fwd, *nla_weight, *nla_u_thresh,
-			      *nla_l_thresh;
+		    *nla_l_thresh;
 
-		nla_fwd		= attrs[IPVS_DEST_ATTR_FWD_METHOD];
-		nla_weight	= attrs[IPVS_DEST_ATTR_WEIGHT];
-		nla_u_thresh	= attrs[IPVS_DEST_ATTR_U_THRESH];
-		nla_l_thresh	= attrs[IPVS_DEST_ATTR_L_THRESH];
+		nla_fwd = attrs[IPVS_DEST_ATTR_FWD_METHOD];
+		nla_weight = attrs[IPVS_DEST_ATTR_WEIGHT];
+		nla_u_thresh = attrs[IPVS_DEST_ATTR_U_THRESH];
+		nla_l_thresh = attrs[IPVS_DEST_ATTR_L_THRESH];
 
 		if (!(nla_fwd && nla_weight && nla_u_thresh && nla_l_thresh))
 			return -EINVAL;
 
 		udest->conn_flags = nla_get_u32(nla_fwd)
-				    & IP_VS_CONN_F_FWD_MASK;
+		    & IP_VS_CONN_F_FWD_MASK;
 		udest->weight = nla_get_u32(nla_weight);
 		udest->u_threshold = nla_get_u32(nla_u_thresh);
 		udest->l_threshold = nla_get_u32(nla_l_thresh);
@@ -3230,8 +3699,8 @@ static int ip_vs_genl_parse_dest(struct ip_vs_dest_user_kern *udest,
 	return 0;
 }
 
-static int ip_vs_genl_fill_daemon(struct sk_buff *skb, __u32 state,
-				  const char *mcast_ifn, __u32 syncid)
+static int ip_vs_genl_fill_daemon(struct sk_buff *skb, __be32 state,
+				  const char *mcast_ifn, __be32 syncid)
 {
 	struct nlattr *nl_daemon;
 
@@ -3239,27 +3708,22 @@ static int ip_vs_genl_fill_daemon(struct sk_buff *skb, __u32 state,
 	if (!nl_daemon)
 		return -EMSGSIZE;
 
-	if (nla_put_u32(skb, IPVS_DAEMON_ATTR_STATE, state) ||
-	    nla_put_string(skb, IPVS_DAEMON_ATTR_MCAST_IFN, mcast_ifn) ||
-	    nla_put_u32(skb, IPVS_DAEMON_ATTR_SYNC_ID, syncid))
-		goto nla_put_failure;
+	nla_put_u32(skb, IPVS_DAEMON_ATTR_STATE, state);
+	nla_put_string(skb, IPVS_DAEMON_ATTR_MCAST_IFN, mcast_ifn);
+	nla_put_u32(skb, IPVS_DAEMON_ATTR_SYNC_ID, syncid);
+
 	nla_nest_end(skb, nl_daemon);
 
 	return 0;
-
-nla_put_failure:
-	nla_nest_cancel(skb, nl_daemon);
-	return -EMSGSIZE;
 }
 
-static int ip_vs_genl_dump_daemon(struct sk_buff *skb, __u32 state,
-				  const char *mcast_ifn, __u32 syncid,
+static int ip_vs_genl_dump_daemon(struct sk_buff *skb, __be32 state,
+				  const char *mcast_ifn, __be32 syncid,
 				  struct netlink_callback *cb)
 {
 	void *hdr;
 	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,
-			  &ip_vs_genl_family, NLM_F_MULTI,
-			  IPVS_CMD_NEW_DAEMON);
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_DAEMON);
 	if (!hdr)
 		return -EMSGSIZE;
 
@@ -3276,89 +3740,94 @@ nla_put_failure:
 static int ip_vs_genl_dump_daemons(struct sk_buff *skb,
 				   struct netlink_callback *cb)
 {
-	struct net *net = skb_sknet(skb);
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	mutex_lock(&ipvs->sync_mutex);
-	if ((ipvs->sync_state & IP_VS_STATE_MASTER) && !cb->args[0]) {
+	mutex_lock(&__ip_vs_mutex);
+	if ((ip_vs_sync_state & IP_VS_STATE_MASTER) && !cb->args[0]) {
 		if (ip_vs_genl_dump_daemon(skb, IP_VS_STATE_MASTER,
-					   ipvs->master_mcast_ifn,
-					   ipvs->master_syncid, cb) < 0)
+					   ip_vs_master_mcast_ifn,
+					   ip_vs_master_syncid, cb) < 0)
 			goto nla_put_failure;
 
 		cb->args[0] = 1;
 	}
 
-	if ((ipvs->sync_state & IP_VS_STATE_BACKUP) && !cb->args[1]) {
+	if ((ip_vs_sync_state & IP_VS_STATE_BACKUP) && !cb->args[1]) {
 		if (ip_vs_genl_dump_daemon(skb, IP_VS_STATE_BACKUP,
-					   ipvs->backup_mcast_ifn,
-					   ipvs->backup_syncid, cb) < 0)
+					   ip_vs_backup_mcast_ifn,
+					   ip_vs_backup_syncid, cb) < 0)
 			goto nla_put_failure;
 
 		cb->args[1] = 1;
 	}
 
 nla_put_failure:
-	mutex_unlock(&ipvs->sync_mutex);
+	mutex_unlock(&__ip_vs_mutex);
 
 	return skb->len;
 }
 
-static int ip_vs_genl_new_daemon(struct net *net, struct nlattr **attrs)
+static int ip_vs_genl_new_daemon(struct nlattr **attrs)
 {
 	if (!(attrs[IPVS_DAEMON_ATTR_STATE] &&
 	      attrs[IPVS_DAEMON_ATTR_MCAST_IFN] &&
 	      attrs[IPVS_DAEMON_ATTR_SYNC_ID]))
 		return -EINVAL;
 
-	return start_sync_thread(net,
-				 nla_get_u32(attrs[IPVS_DAEMON_ATTR_STATE]),
+	return start_sync_thread(nla_get_u32(attrs[IPVS_DAEMON_ATTR_STATE]),
 				 nla_data(attrs[IPVS_DAEMON_ATTR_MCAST_IFN]),
 				 nla_get_u32(attrs[IPVS_DAEMON_ATTR_SYNC_ID]));
 }
 
-static int ip_vs_genl_del_daemon(struct net *net, struct nlattr **attrs)
+static int ip_vs_genl_del_daemon(struct nlattr **attrs)
 {
 	if (!attrs[IPVS_DAEMON_ATTR_STATE])
 		return -EINVAL;
 
-	return stop_sync_thread(net,
-				nla_get_u32(attrs[IPVS_DAEMON_ATTR_STATE]));
+	return stop_sync_thread(nla_get_u32(attrs[IPVS_DAEMON_ATTR_STATE]));
 }
 
-static int ip_vs_genl_set_config(struct net *net, struct nlattr **attrs)
+static int ip_vs_genl_set_config(struct nlattr **attrs)
 {
 	struct ip_vs_timeout_user t;
 
-	__ip_vs_get_timeouts(net, &t);
+	__ip_vs_get_timeouts(&t);
 
 	if (attrs[IPVS_CMD_ATTR_TIMEOUT_TCP])
 		t.tcp_timeout = nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_TCP]);
 
 	if (attrs[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN])
 		t.tcp_fin_timeout =
-			nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]);
+		    nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]);
 
 	if (attrs[IPVS_CMD_ATTR_TIMEOUT_UDP])
 		t.udp_timeout = nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_UDP]);
 
-	return ip_vs_set_timeout(net, &t);
+	return ip_vs_set_timeout(&t);
 }
 
-static int ip_vs_genl_set_daemon(struct sk_buff *skb, struct genl_info *info)
+static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 {
+	struct ip_vs_service *svc = NULL;
+	struct ip_vs_service_user_kern usvc;
+	struct ip_vs_dest_user_kern udest;
+	struct ip_vs_laddr_user_kern uladdr;
+
 	int ret = 0, cmd;
-	struct net *net;
-	struct netns_ipvs *ipvs;
+	int need_full_svc = 0, need_full_dest = 0;
 
-	net = skb_sknet(skb);
-	ipvs = net_ipvs(net);
 	cmd = info->genlhdr->cmd;
 
-	if (cmd == IPVS_CMD_NEW_DAEMON || cmd == IPVS_CMD_DEL_DAEMON) {
+	mutex_lock(&__ip_vs_mutex);
+
+	if (cmd == IPVS_CMD_FLUSH) {
+		ret = ip_vs_flush();
+		goto out;
+	} else if (cmd == IPVS_CMD_SET_CONFIG) {
+		ret = ip_vs_genl_set_config(info->attrs);
+		goto out;
+	} else if (cmd == IPVS_CMD_NEW_DAEMON || cmd == IPVS_CMD_DEL_DAEMON) {
+
 		struct nlattr *daemon_attrs[IPVS_DAEMON_ATTR_MAX + 1];
 
-		mutex_lock(&ipvs->sync_mutex);
 		if (!info->attrs[IPVS_CMD_ATTR_DAEMON] ||
 		    nla_parse_nested(daemon_attrs, IPVS_DAEMON_ATTR_MAX,
 				     info->attrs[IPVS_CMD_ATTR_DAEMON],
@@ -3368,38 +3837,12 @@ static int ip_vs_genl_set_daemon(struct sk_buff *skb, struct genl_info *info)
 		}
 
 		if (cmd == IPVS_CMD_NEW_DAEMON)
-			ret = ip_vs_genl_new_daemon(net, daemon_attrs);
+			ret = ip_vs_genl_new_daemon(daemon_attrs);
 		else
-			ret = ip_vs_genl_del_daemon(net, daemon_attrs);
-out:
-		mutex_unlock(&ipvs->sync_mutex);
-	}
-	return ret;
-}
-
-static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
-{
-	struct ip_vs_service *svc = NULL;
-	struct ip_vs_service_user_kern usvc;
-	struct ip_vs_dest_user_kern udest;
-	int ret = 0, cmd;
-	int need_full_svc = 0, need_full_dest = 0;
-	struct net *net;
-
-	net = skb_sknet(skb);
-	cmd = info->genlhdr->cmd;
-
-	mutex_lock(&__ip_vs_mutex);
-
-	if (cmd == IPVS_CMD_FLUSH) {
-		ret = ip_vs_flush(net, false);
+			ret = ip_vs_genl_del_daemon(daemon_attrs);
 		goto out;
-	} else if (cmd == IPVS_CMD_SET_CONFIG) {
-		ret = ip_vs_genl_set_config(net, info->attrs);
-		goto out;
-	} else if (cmd == IPVS_CMD_ZERO &&
-		   !info->attrs[IPVS_CMD_ATTR_SERVICE]) {
-		ret = ip_vs_zero_all(net);
+	} else if (cmd == IPVS_CMD_ZERO && !info->attrs[IPVS_CMD_ATTR_SERVICE]) {
+		ret = ip_vs_zero_all();
 		goto out;
 	}
 
@@ -3409,12 +3852,19 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 	if (cmd == IPVS_CMD_NEW_SERVICE || cmd == IPVS_CMD_SET_SERVICE)
 		need_full_svc = 1;
 
-	ret = ip_vs_genl_parse_service(net, &usvc,
+	ret = ip_vs_genl_parse_service(&usvc,
 				       info->attrs[IPVS_CMD_ATTR_SERVICE],
-				       need_full_svc, &svc);
+				       need_full_svc);
 	if (ret)
 		goto out;
 
+	/* Lookup the exact service by <protocol, addr, port> or fwmark */
+	if (usvc.fwmark == 0)
+		svc = __ip_vs_service_get(usvc.af, usvc.protocol,
+					  &usvc.addr, usvc.port);
+	else
+		svc = __ip_vs_svc_fwm_get(usvc.af, usvc.fwmark);
+
 	/* Unless we're adding a new service, the service must already exist */
 	if ((cmd != IPVS_CMD_NEW_SERVICE) && (svc == NULL)) {
 		ret = -ESRCH;
@@ -3436,10 +3886,18 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 			goto out;
 	}
 
+	if (cmd == IPVS_CMD_NEW_LADDR || cmd == IPVS_CMD_DEL_LADDR) {
+		ret = ip_vs_genl_parse_laddr(&uladdr,
+					     info->attrs[IPVS_CMD_ATTR_LADDR],
+					     1);
+		if (ret)
+			goto out;
+	}
+
 	switch (cmd) {
 	case IPVS_CMD_NEW_SERVICE:
 		if (svc == NULL)
-			ret = ip_vs_add_service(net, &usvc, &svc);
+			ret = ip_vs_add_service(&usvc, &svc);
 		else
 			ret = -EEXIST;
 		break;
@@ -3448,7 +3906,6 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 		break;
 	case IPVS_CMD_DEL_SERVICE:
 		ret = ip_vs_del_service(svc);
-		/* do not use svc, it can be freed */
 		break;
 	case IPVS_CMD_NEW_DEST:
 		ret = ip_vs_add_dest(svc, &udest);
@@ -3462,11 +3919,19 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 	case IPVS_CMD_ZERO:
 		ret = ip_vs_zero_service(svc);
 		break;
+	case IPVS_CMD_NEW_LADDR:
+		ret = ip_vs_add_laddr(svc, &uladdr);
+		break;
+	case IPVS_CMD_DEL_LADDR:
+		ret = ip_vs_del_laddr(svc, &uladdr);
+		break;
 	default:
 		ret = -EINVAL;
 	}
 
-out:
+      out:
+	if (svc)
+		ip_vs_service_put(svc);
 	mutex_unlock(&__ip_vs_mutex);
 
 	return ret;
@@ -3477,9 +3942,7 @@ static int ip_vs_genl_get_cmd(struct sk_buff *skb, struct genl_info *info)
 	struct sk_buff *msg;
 	void *reply;
 	int ret, cmd, reply_cmd;
-	struct net *net;
 
-	net = skb_sknet(skb);
 	cmd = info->genlhdr->cmd;
 
 	if (cmd == IPVS_CMD_GET_SERVICE)
@@ -3505,52 +3968,52 @@ static int ip_vs_genl_get_cmd(struct sk_buff *skb, struct genl_info *info)
 
 	switch (cmd) {
 	case IPVS_CMD_GET_SERVICE:
-	{
-		struct ip_vs_service *svc;
+		{
+			struct ip_vs_service *svc;
+
+			svc =
+			    ip_vs_genl_find_service(info->
+						    attrs
+						    [IPVS_CMD_ATTR_SERVICE]);
+			if (IS_ERR(svc)) {
+				ret = PTR_ERR(svc);
+				goto out_err;
+			} else if (svc) {
+				ret = ip_vs_genl_fill_service(msg, svc);
+				ip_vs_service_put(svc);
+				if (ret)
+					goto nla_put_failure;
+			} else {
+				ret = -ESRCH;
+				goto out_err;
+			}
 
-		svc = ip_vs_genl_find_service(net,
-					      info->attrs[IPVS_CMD_ATTR_SERVICE]);
-		if (IS_ERR(svc)) {
-			ret = PTR_ERR(svc);
-			goto out_err;
-		} else if (svc) {
-			ret = ip_vs_genl_fill_service(msg, svc);
-			if (ret)
-				goto nla_put_failure;
-		} else {
-			ret = -ESRCH;
-			goto out_err;
+			break;
 		}
 
-		break;
-	}
-
 	case IPVS_CMD_GET_CONFIG:
-	{
-		struct ip_vs_timeout_user t;
+		{
+			struct ip_vs_timeout_user t;
 
-		__ip_vs_get_timeouts(net, &t);
+			__ip_vs_get_timeouts(&t);
 #ifdef CONFIG_IP_VS_PROTO_TCP
-		if (nla_put_u32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP,
-				t.tcp_timeout) ||
-		    nla_put_u32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,
-				t.tcp_fin_timeout))
-			goto nla_put_failure;
+			nla_put_u32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP,
+				    t.tcp_timeout);
+			nla_put_u32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,
+				    t.tcp_fin_timeout);
 #endif
 #ifdef CONFIG_IP_VS_PROTO_UDP
-		if (nla_put_u32(msg, IPVS_CMD_ATTR_TIMEOUT_UDP, t.udp_timeout))
-			goto nla_put_failure;
+			nla_put_u32(msg, IPVS_CMD_ATTR_TIMEOUT_UDP,
+				    t.udp_timeout);
 #endif
 
-		break;
-	}
+			break;
+		}
 
 	case IPVS_CMD_GET_INFO:
-		if (nla_put_u32(msg, IPVS_INFO_ATTR_VERSION,
-				IP_VS_VERSION_CODE) ||
-		    nla_put_u32(msg, IPVS_INFO_ATTR_CONN_TAB_SIZE,
-				ip_vs_conn_tab_size))
-			goto nla_put_failure;
+		nla_put_u32(msg, IPVS_INFO_ATTR_VERSION, IP_VS_VERSION_CODE);
+		nla_put_u32(msg, IPVS_INFO_ATTR_CONN_TAB_SIZE,
+			    IP_VS_CONN_TAB_SIZE);
 		break;
 	}
 
@@ -3558,113 +4021,130 @@ static int ip_vs_genl_get_cmd(struct sk_buff *skb, struct genl_info *info)
 	ret = genlmsg_reply(msg, info);
 	goto out;
 
-nla_put_failure:
+      nla_put_failure:
 	pr_err("not enough space in Netlink message\n");
 	ret = -EMSGSIZE;
 
-out_err:
+      out_err:
 	nlmsg_free(msg);
-out:
+      out:
 	mutex_unlock(&__ip_vs_mutex);
 
 	return ret;
 }
 
-
-static const struct genl_ops ip_vs_genl_ops[] __read_mostly = {
-	{
-		.cmd	= IPVS_CMD_NEW_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_SET_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_DEL_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_get_cmd,
-		.dumpit	= ip_vs_genl_dump_services,
-		.policy	= ip_vs_cmd_policy,
-	},
-	{
-		.cmd	= IPVS_CMD_NEW_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_SET_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_DEL_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.dumpit	= ip_vs_genl_dump_dests,
-	},
-	{
-		.cmd	= IPVS_CMD_NEW_DAEMON,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_daemon,
-	},
-	{
-		.cmd	= IPVS_CMD_DEL_DAEMON,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_daemon,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_DAEMON,
-		.flags	= GENL_ADMIN_PERM,
-		.dumpit	= ip_vs_genl_dump_daemons,
-	},
-	{
-		.cmd	= IPVS_CMD_SET_CONFIG,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_CONFIG,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_get_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_INFO,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_get_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_ZERO,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_FLUSH,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_set_cmd,
-	},
+static struct genl_ops ip_vs_genl_ops[] __read_mostly = {
+	{
+	 .cmd = IPVS_CMD_NEW_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_SET_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_get_cmd,
+	 .dumpit = ip_vs_genl_dump_services,
+	 .policy = ip_vs_cmd_policy,
+	 },
+	{
+	 .cmd = IPVS_CMD_NEW_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_SET_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .dumpit = ip_vs_genl_dump_dests,
+	 },
+	{
+	 .cmd = IPVS_CMD_NEW_DAEMON,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_DAEMON,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_DAEMON,
+	 .flags = GENL_ADMIN_PERM,
+	 .dumpit = ip_vs_genl_dump_daemons,
+	 },
+	{
+	 .cmd = IPVS_CMD_SET_CONFIG,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_CONFIG,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_get_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_INFO,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_get_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_ZERO,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_FLUSH,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_NEW_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .dumpit = ip_vs_genl_dump_laddrs,
+	 },
 };
 
 static int __init ip_vs_genl_register(void)
@@ -3680,218 +4160,82 @@ static void ip_vs_genl_unregister(void)
 
 /* End of Generic Netlink interface definitions */
 
-/*
- * per netns intit/exit func.
- */
-#ifdef CONFIG_SYSCTL
-static int __net_init ip_vs_control_net_init_sysctl(struct net *net)
-{
-	int idx;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ctl_table *tbl;
-
-	atomic_set(&ipvs->dropentry, 0);
-	spin_lock_init(&ipvs->dropentry_lock);
-	spin_lock_init(&ipvs->droppacket_lock);
-	spin_lock_init(&ipvs->securetcp_lock);
-
-	if (!net_eq(net, &init_net)) {
-		tbl = kmemdup(vs_vars, sizeof(vs_vars), GFP_KERNEL);
-		if (tbl == NULL)
-			return -ENOMEM;
-
-		/* Don't export sysctls to unprivileged users */
-		if (net->user_ns != &init_user_ns)
-			tbl[0].procname = NULL;
-	} else
-		tbl = vs_vars;
-	/* Initialize sysctl defaults */
-	idx = 0;
-	ipvs->sysctl_amemthresh = 1024;
-	tbl[idx++].data = &ipvs->sysctl_amemthresh;
-	ipvs->sysctl_am_droprate = 10;
-	tbl[idx++].data = &ipvs->sysctl_am_droprate;
-	tbl[idx++].data = &ipvs->sysctl_drop_entry;
-	tbl[idx++].data = &ipvs->sysctl_drop_packet;
-#ifdef CONFIG_IP_VS_NFCT
-	tbl[idx++].data = &ipvs->sysctl_conntrack;
-#endif
-	tbl[idx++].data = &ipvs->sysctl_secure_tcp;
-	ipvs->sysctl_snat_reroute = 1;
-	tbl[idx++].data = &ipvs->sysctl_snat_reroute;
-	ipvs->sysctl_sync_ver = 1;
-	tbl[idx++].data = &ipvs->sysctl_sync_ver;
-	ipvs->sysctl_sync_ports = 1;
-	tbl[idx++].data = &ipvs->sysctl_sync_ports;
-	ipvs->sysctl_sync_qlen_max = nr_free_buffer_pages() / 32;
-	tbl[idx++].data = &ipvs->sysctl_sync_qlen_max;
-	ipvs->sysctl_sync_sock_size = 0;
-	tbl[idx++].data = &ipvs->sysctl_sync_sock_size;
-	tbl[idx++].data = &ipvs->sysctl_cache_bypass;
-	tbl[idx++].data = &ipvs->sysctl_expire_nodest_conn;
-	tbl[idx++].data = &ipvs->sysctl_expire_quiescent_template;
-	ipvs->sysctl_sync_threshold[0] = DEFAULT_SYNC_THRESHOLD;
-	ipvs->sysctl_sync_threshold[1] = DEFAULT_SYNC_PERIOD;
-	tbl[idx].data = &ipvs->sysctl_sync_threshold;
-	tbl[idx++].maxlen = sizeof(ipvs->sysctl_sync_threshold);
-	ipvs->sysctl_sync_refresh_period = DEFAULT_SYNC_REFRESH_PERIOD;
-	tbl[idx++].data = &ipvs->sysctl_sync_refresh_period;
-	ipvs->sysctl_sync_retries = clamp_t(int, DEFAULT_SYNC_RETRIES, 0, 3);
-	tbl[idx++].data = &ipvs->sysctl_sync_retries;
-	tbl[idx++].data = &ipvs->sysctl_nat_icmp_send;
-	ipvs->sysctl_pmtu_disc = 1;
-	tbl[idx++].data = &ipvs->sysctl_pmtu_disc;
-	tbl[idx++].data = &ipvs->sysctl_backup_only;
-	ipvs->sysctl_conn_reuse_mode = 1;
-	tbl[idx++].data = &ipvs->sysctl_conn_reuse_mode;
-
-
-	ipvs->sysctl_hdr = register_net_sysctl(net, "net/ipv4/vs", tbl);
-	if (ipvs->sysctl_hdr == NULL) {
-		if (!net_eq(net, &init_net))
-			kfree(tbl);
-		return -ENOMEM;
-	}
-	ip_vs_start_estimator(net, &ipvs->tot_stats);
-	ipvs->sysctl_tbl = tbl;
-	/* Schedule defense work */
-	INIT_DELAYED_WORK(&ipvs->defense_work, defense_work_handler);
-	schedule_delayed_work(&ipvs->defense_work, DEFENSE_TIMER_PERIOD);
-
-	return 0;
-}
-
-static void __net_exit ip_vs_control_net_cleanup_sysctl(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	cancel_delayed_work_sync(&ipvs->defense_work);
-	cancel_work_sync(&ipvs->defense_work.work);
-	unregister_net_sysctl_table(ipvs->sysctl_hdr);
-}
-
-#else
-
-static int __net_init ip_vs_control_net_init_sysctl(struct net *net) { return 0; }
-static void __net_exit ip_vs_control_net_cleanup_sysctl(struct net *net) { }
-
-#endif
-
-static struct notifier_block ip_vs_dst_notifier = {
-	.notifier_call = ip_vs_dst_event,
-};
-
-int __net_init ip_vs_control_net_init(struct net *net)
+int __init ip_vs_control_init(void)
 {
+	int ret;
 	int idx;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	/* Initialize rs_table */
-	for (idx = 0; idx < IP_VS_RTAB_SIZE; idx++)
-		INIT_HLIST_HEAD(&ipvs->rs_table[idx]);
-
-	INIT_LIST_HEAD(&ipvs->dest_trash);
-	spin_lock_init(&ipvs->dest_trash_lock);
-	setup_timer(&ipvs->dest_trash_timer, ip_vs_dest_trash_expire,
-		    (unsigned long) net);
-	atomic_set(&ipvs->ftpsvc_counter, 0);
-	atomic_set(&ipvs->nullsvc_counter, 0);
-
-	/* procfs stats */
-	ipvs->tot_stats.cpustats = alloc_percpu(struct ip_vs_cpu_stats);
-	if (!ipvs->tot_stats.cpustats)
-		return -ENOMEM;
-
-	spin_lock_init(&ipvs->tot_stats.lock);
-
-	proc_create("ip_vs", 0, net->proc_net, &ip_vs_info_fops);
-	proc_create("ip_vs_stats", 0, net->proc_net, &ip_vs_stats_fops);
-	proc_create("ip_vs_stats_percpu", 0, net->proc_net,
-		    &ip_vs_stats_percpu_fops);
-
-	if (ip_vs_control_net_init_sysctl(net))
-		goto err;
-
-	return 0;
-
-err:
-	free_percpu(ipvs->tot_stats.cpustats);
-	return -ENOMEM;
-}
-
-void __net_exit ip_vs_control_net_cleanup(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
 
-	/* Some dest can be in grace period even before cleanup, we have to
-	 * defer ip_vs_trash_cleanup until ip_vs_dest_wait_readers is called.
-	 */
-	rcu_barrier();
-	ip_vs_trash_cleanup(net);
-	ip_vs_stop_estimator(net, &ipvs->tot_stats);
-	ip_vs_control_net_cleanup_sysctl(net);
-	remove_proc_entry("ip_vs_stats_percpu", net->proc_net);
-	remove_proc_entry("ip_vs_stats", net->proc_net);
-	remove_proc_entry("ip_vs", net->proc_net);
-	free_percpu(ipvs->tot_stats.cpustats);
-}
-
-int __init ip_vs_register_nl_ioctl(void)
-{
-	int ret;
+	EnterFunction(2);
 
 	ret = nf_register_sockopt(&ip_vs_sockopts);
 	if (ret) {
 		pr_err("cannot register sockopt.\n");
-		goto err_sock;
+		goto out_err;
 	}
 
 	ret = ip_vs_genl_register();
 	if (ret) {
 		pr_err("cannot register Generic Netlink interface.\n");
-		goto err_genl;
+		goto cleanup_sockopt;
 	}
-	return 0;
 
-err_genl:
-	nf_unregister_sockopt(&ip_vs_sockopts);
-err_sock:
-	return ret;
-}
+	if (NULL == (ip_vs_esmib = alloc_percpu(struct ip_vs_estats_mib))) {
+		pr_err("cannot allocate percpu struct ip_vs_estats_mib.\n");
+		ret = 1;
+		goto cleanup_genl;
+	}
 
-void ip_vs_unregister_nl_ioctl(void)
-{
-	ip_vs_genl_unregister();
-	nf_unregister_sockopt(&ip_vs_sockopts);
-}
+	ret = ip_vs_new_stats(&(ip_vs_stats));
+	if(ret) {
+		pr_err("cannot allocate percpu struct ip_vs_stats.\n");
+		goto cleanup_percpu;
+	}
 
-int __init ip_vs_control_init(void)
-{
-	int idx;
-	int ret;
+	proc_create("ip_vs_ext_stats", 0, init_net.proc_net, &ip_vs_estats_fops);
+	proc_create("ip_vs", 0, init_net.proc_net, &ip_vs_info_fops);
+	proc_create("ip_vs_stats", 0, init_net.proc_net, &ip_vs_stats_fops);
 
-	EnterFunction(2);
+	sysctl_header = register_net_sysctl(&init_net, "net/ipv4/vs", vs_vars);
 
-	/* Initialize svc_table, ip_vs_svc_fwm_table */
+	/* Initialize ip_vs_svc_table, ip_vs_svc_fwm_table, ip_vs_rtable */
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		INIT_HLIST_HEAD(&ip_vs_svc_table[idx]);
-		INIT_HLIST_HEAD(&ip_vs_svc_fwm_table[idx]);
+		INIT_LIST_HEAD(&ip_vs_svc_table[idx]);
+		INIT_LIST_HEAD(&ip_vs_svc_fwm_table[idx]);
+	}
+	for (idx = 0; idx < IP_VS_RTAB_SIZE; idx++) {
+		INIT_LIST_HEAD(&ip_vs_rtable[idx]);
 	}
 
-	smp_wmb();	/* Do we really need it now ? */
 
-	ret = register_netdevice_notifier(&ip_vs_dst_notifier);
-	if (ret < 0)
-		return ret;
+	/* Hook the defense timer */
+	schedule_delayed_work(&defense_work, DEFENSE_TIMER_PERIOD);
 
 	LeaveFunction(2);
 	return 0;
-}
 
+cleanup_percpu:
+	free_percpu(ip_vs_esmib);
+cleanup_genl:
+	ip_vs_genl_unregister();
+cleanup_sockopt:
+	nf_unregister_sockopt(&ip_vs_sockopts);
+out_err:
+	return ret;
+}
 
 void ip_vs_control_cleanup(void)
 {
 	EnterFunction(2);
-	unregister_netdevice_notifier(&ip_vs_dst_notifier);
+	ip_vs_trash_cleanup();
+	cancel_delayed_work_sync(&defense_work);
+	cancel_work_sync(&defense_work.work);
+	ip_vs_del_stats(ip_vs_stats);
+	unregister_sysctl_table(sysctl_header);
+	remove_proc_entry("ip_vs_stats", init_net.proc_net);
+	remove_proc_entry("ip_vs", init_net.proc_net);
+	remove_proc_entry("ip_vs_ext_stats", init_net.proc_net);
+	free_percpu(ip_vs_esmib);
+	ip_vs_genl_unregister();
+	nf_unregister_sockopt(&ip_vs_sockopts);
 	LeaveFunction(2);
 }
diff --git a/net/netfilter/ipvs/ip_vs_dh.c b/net/netfilter/ipvs/ip_vs_dh.c
index ccab120..fe3e188 100644
--- a/net/netfilter/ipvs/ip_vs_dh.c
+++ b/net/netfilter/ipvs/ip_vs_dh.c
@@ -39,7 +39,6 @@
 #define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
 
 #include <linux/ip.h>
-#include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/skbuff.h>
@@ -51,7 +50,7 @@
  *      IPVS DH bucket
  */
 struct ip_vs_dh_bucket {
-	struct ip_vs_dest __rcu	*dest;	/* real server (cache) */
+	struct ip_vs_dest       *dest;          /* real server (cache) */
 };
 
 /*
@@ -64,15 +63,11 @@ struct ip_vs_dh_bucket {
 #define IP_VS_DH_TAB_SIZE               (1 << IP_VS_DH_TAB_BITS)
 #define IP_VS_DH_TAB_MASK               (IP_VS_DH_TAB_SIZE - 1)
 
-struct ip_vs_dh_state {
-	struct ip_vs_dh_bucket		buckets[IP_VS_DH_TAB_SIZE];
-	struct rcu_head			rcu_head;
-};
 
 /*
  *	Returns hash value for IPVS DH entry
  */
-static inline unsigned int ip_vs_dh_hashkey(int af, const union nf_inet_addr *addr)
+static inline unsigned ip_vs_dh_hashkey(int af, const union nf_inet_addr *addr)
 {
 	__be32 addr_fold = addr->ip;
 
@@ -89,9 +84,10 @@ static inline unsigned int ip_vs_dh_hashkey(int af, const union nf_inet_addr *ad
  *      Get ip_vs_dest associated with supplied parameters.
  */
 static inline struct ip_vs_dest *
-ip_vs_dh_get(int af, struct ip_vs_dh_state *s, const union nf_inet_addr *addr)
+ip_vs_dh_get(int af, struct ip_vs_dh_bucket *tbl,
+	     const union nf_inet_addr *addr)
 {
-	return rcu_dereference(s->buckets[ip_vs_dh_hashkey(af, addr)].dest);
+	return (tbl[ip_vs_dh_hashkey(af, addr)]).dest;
 }
 
 
@@ -99,30 +95,25 @@ ip_vs_dh_get(int af, struct ip_vs_dh_state *s, const union nf_inet_addr *addr)
  *      Assign all the hash buckets of the specified table with the service.
  */
 static int
-ip_vs_dh_reassign(struct ip_vs_dh_state *s, struct ip_vs_service *svc)
+ip_vs_dh_assign(struct ip_vs_dh_bucket *tbl, struct ip_vs_service *svc)
 {
 	int i;
 	struct ip_vs_dh_bucket *b;
 	struct list_head *p;
 	struct ip_vs_dest *dest;
-	bool empty;
 
-	b = &s->buckets[0];
+	b = tbl;
 	p = &svc->destinations;
-	empty = list_empty(p);
 	for (i=0; i<IP_VS_DH_TAB_SIZE; i++) {
-		dest = rcu_dereference_protected(b->dest, 1);
-		if (dest)
-			ip_vs_dest_put(dest);
-		if (empty)
-			RCU_INIT_POINTER(b->dest, NULL);
-		else {
+		if (list_empty(p)) {
+			b->dest = NULL;
+		} else {
 			if (p == &svc->destinations)
 				p = p->next;
 
 			dest = list_entry(p, struct ip_vs_dest, n_list);
-			ip_vs_dest_hold(dest);
-			RCU_INIT_POINTER(b->dest, dest);
+			atomic_inc(&dest->refcnt);
+			b->dest = dest;
 
 			p = p->next;
 		}
@@ -135,18 +126,16 @@ ip_vs_dh_reassign(struct ip_vs_dh_state *s, struct ip_vs_service *svc)
 /*
  *      Flush all the hash buckets of the specified table.
  */
-static void ip_vs_dh_flush(struct ip_vs_dh_state *s)
+static void ip_vs_dh_flush(struct ip_vs_dh_bucket *tbl)
 {
 	int i;
 	struct ip_vs_dh_bucket *b;
-	struct ip_vs_dest *dest;
 
-	b = &s->buckets[0];
+	b = tbl;
 	for (i=0; i<IP_VS_DH_TAB_SIZE; i++) {
-		dest = rcu_dereference_protected(b->dest, 1);
-		if (dest) {
-			ip_vs_dest_put(dest);
-			RCU_INIT_POINTER(b->dest, NULL);
+		if (b->dest) {
+			atomic_dec(&b->dest->refcnt);
+			b->dest = NULL;
 		}
 		b++;
 	}
@@ -155,46 +144,52 @@ static void ip_vs_dh_flush(struct ip_vs_dh_state *s)
 
 static int ip_vs_dh_init_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_dh_state *s;
+	struct ip_vs_dh_bucket *tbl;
 
 	/* allocate the DH table for this service */
-	s = kzalloc(sizeof(struct ip_vs_dh_state), GFP_KERNEL);
-	if (s == NULL)
+	tbl = kmalloc(sizeof(struct ip_vs_dh_bucket)*IP_VS_DH_TAB_SIZE,
+		      GFP_ATOMIC);
+	if (tbl == NULL) {
+		pr_err("%s(): no memory\n", __func__);
 		return -ENOMEM;
-
-	svc->sched_data = s;
+	}
+	svc->sched_data = tbl;
 	IP_VS_DBG(6, "DH hash table (memory=%Zdbytes) allocated for "
 		  "current service\n",
 		  sizeof(struct ip_vs_dh_bucket)*IP_VS_DH_TAB_SIZE);
 
-	/* assign the hash buckets with current dests */
-	ip_vs_dh_reassign(s, svc);
+	/* assign the hash buckets with the updated service */
+	ip_vs_dh_assign(tbl, svc);
 
 	return 0;
 }
 
 
-static void ip_vs_dh_done_svc(struct ip_vs_service *svc)
+static int ip_vs_dh_done_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_dh_state *s = svc->sched_data;
+	struct ip_vs_dh_bucket *tbl = svc->sched_data;
 
 	/* got to clean up hash buckets here */
-	ip_vs_dh_flush(s);
+	ip_vs_dh_flush(tbl);
 
 	/* release the table itself */
-	kfree_rcu(s, rcu_head);
+	kfree(svc->sched_data);
 	IP_VS_DBG(6, "DH hash table (memory=%Zdbytes) released\n",
 		  sizeof(struct ip_vs_dh_bucket)*IP_VS_DH_TAB_SIZE);
+
+	return 0;
 }
 
 
-static int ip_vs_dh_dest_changed(struct ip_vs_service *svc,
-				 struct ip_vs_dest *dest)
+static int ip_vs_dh_update_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_dh_state *s = svc->sched_data;
+	struct ip_vs_dh_bucket *tbl = svc->sched_data;
+
+	/* got to clean up hash buckets here */
+	ip_vs_dh_flush(tbl);
 
 	/* assign the hash buckets with the updated service */
-	ip_vs_dh_reassign(s, svc);
+	ip_vs_dh_assign(tbl, svc);
 
 	return 0;
 }
@@ -217,20 +212,19 @@ static struct ip_vs_dest *
 ip_vs_dh_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest;
-	struct ip_vs_dh_state *s;
+	struct ip_vs_dh_bucket *tbl;
 	struct ip_vs_iphdr iph;
 
-	ip_vs_fill_iph_addr_only(svc->af, skb, &iph);
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
 
 	IP_VS_DBG(6, "%s(): Scheduling...\n", __func__);
 
-	s = (struct ip_vs_dh_state *) svc->sched_data;
-	dest = ip_vs_dh_get(svc->af, s, &iph.daddr);
+	tbl = (struct ip_vs_dh_bucket *)svc->sched_data;
+	dest = ip_vs_dh_get(svc->af, tbl, &iph.daddr);
 	if (!dest
 	    || !(dest->flags & IP_VS_DEST_F_AVAILABLE)
 	    || atomic_read(&dest->weight) <= 0
 	    || is_overloaded(dest)) {
-		ip_vs_scheduler_err(svc, "no destination available");
 		return NULL;
 	}
 
@@ -254,8 +248,7 @@ static struct ip_vs_scheduler ip_vs_dh_scheduler =
 	.n_list =		LIST_HEAD_INIT(ip_vs_dh_scheduler.n_list),
 	.init_service =		ip_vs_dh_init_svc,
 	.done_service =		ip_vs_dh_done_svc,
-	.add_dest =		ip_vs_dh_dest_changed,
-	.del_dest =		ip_vs_dh_dest_changed,
+	.update_service =	ip_vs_dh_update_svc,
 	.schedule =		ip_vs_dh_schedule,
 };
 
@@ -269,7 +262,6 @@ static int __init ip_vs_dh_init(void)
 static void __exit ip_vs_dh_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_dh_scheduler);
-	synchronize_rcu();
 }
 
 
diff --git a/net/netfilter/ipvs/ip_vs_est.c b/net/netfilter/ipvs/ip_vs_est.c
index 6bee6d0..61e12fd 100644
--- a/net/netfilter/ipvs/ip_vs_est.c
+++ b/net/netfilter/ipvs/ip_vs_est.c
@@ -8,12 +8,8 @@
  *              as published by the Free Software Foundation; either version
  *              2 of the License, or (at your option) any later version.
  *
- * Changes:     Hans Schillstrom <hans.schillstrom@ericsson.com>
- *              Network name space (netns) aware.
- *              Global data moved to netns i.e struct netns_ipvs
- *              Affected data: est_list and est_lock.
- *              estimation_timer() runs with timer per netns.
- *              get_stats()) do the per cpu summing.
+ * Changes:
+ *
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -21,6 +17,7 @@
 
 #include <linux/kernel.h>
 #include <linux/jiffies.h>
+#include <linux/slab.h>
 #include <linux/types.h>
 #include <linux/interrupt.h>
 #include <linux/sysctl.h>
@@ -51,62 +48,26 @@
   * A lot code is taken from net/sched/estimator.c
  */
 
+static void estimation_timer(unsigned long arg);
 
-/*
- * Make a summary from each cpu
- */
-static void ip_vs_read_cpu_stats(struct ip_vs_stats_user *sum,
-				 struct ip_vs_cpu_stats __percpu *stats)
-{
-	int i;
-
-	for_each_possible_cpu(i) {
-		struct ip_vs_cpu_stats *s = per_cpu_ptr(stats, i);
-		unsigned int start;
-		__u64 inbytes, outbytes;
-		if (i) {
-			sum->conns += s->ustats.conns;
-			sum->inpkts += s->ustats.inpkts;
-			sum->outpkts += s->ustats.outpkts;
-			do {
-				start = u64_stats_fetch_begin(&s->syncp);
-				inbytes = s->ustats.inbytes;
-				outbytes = s->ustats.outbytes;
-			} while (u64_stats_fetch_retry(&s->syncp, start));
-			sum->inbytes += inbytes;
-			sum->outbytes += outbytes;
-		} else {
-			sum->conns = s->ustats.conns;
-			sum->inpkts = s->ustats.inpkts;
-			sum->outpkts = s->ustats.outpkts;
-			do {
-				start = u64_stats_fetch_begin(&s->syncp);
-				sum->inbytes = s->ustats.inbytes;
-				sum->outbytes = s->ustats.outbytes;
-			} while (u64_stats_fetch_retry(&s->syncp, start));
-		}
-	}
-}
-
+static LIST_HEAD(est_list);
+static DEFINE_SPINLOCK(est_lock);
+static DEFINE_TIMER(est_timer, estimation_timer, 0, 0);
 
 static void estimation_timer(unsigned long arg)
 {
 	struct ip_vs_estimator *e;
 	struct ip_vs_stats *s;
-	u32 n_conns;
-	u32 n_inpkts, n_outpkts;
+	u64 n_conns;
+	u64 n_inpkts, n_outpkts;
 	u64 n_inbytes, n_outbytes;
-	u32 rate;
-	struct net *net = (struct net *)arg;
-	struct netns_ipvs *ipvs;
+	u64 rate;
 
-	ipvs = net_ipvs(net);
-	spin_lock(&ipvs->est_lock);
-	list_for_each_entry(e, &ipvs->est_list, list) {
+	spin_lock(&est_lock);
+	list_for_each_entry(e, &est_list, list) {
 		s = container_of(e, struct ip_vs_stats, est);
 
 		spin_lock(&s->lock);
-		ip_vs_read_cpu_stats(&s->ustats, s->cpustats);
 		n_conns = s->ustats.conns;
 		n_inpkts = s->ustats.inpkts;
 		n_outpkts = s->ustats.outpkts;
@@ -117,61 +78,78 @@ static void estimation_timer(unsigned long arg)
 		rate = (n_conns - e->last_conns) << 9;
 		e->last_conns = n_conns;
 		e->cps += ((long)rate - (long)e->cps) >> 2;
+		s->ustats.cps = (e->cps + 0x1FF) >> 10;
 
 		rate = (n_inpkts - e->last_inpkts) << 9;
 		e->last_inpkts = n_inpkts;
 		e->inpps += ((long)rate - (long)e->inpps) >> 2;
+		s->ustats.inpps = (e->inpps + 0x1FF) >> 10;
 
 		rate = (n_outpkts - e->last_outpkts) << 9;
 		e->last_outpkts = n_outpkts;
 		e->outpps += ((long)rate - (long)e->outpps) >> 2;
+		s->ustats.outpps = (e->outpps + 0x1FF) >> 10;
 
 		rate = (n_inbytes - e->last_inbytes) << 4;
 		e->last_inbytes = n_inbytes;
 		e->inbps += ((long)rate - (long)e->inbps) >> 2;
+		s->ustats.inbps = (e->inbps + 0xF) >> 5;
 
 		rate = (n_outbytes - e->last_outbytes) << 4;
 		e->last_outbytes = n_outbytes;
 		e->outbps += ((long)rate - (long)e->outbps) >> 2;
+		s->ustats.outbps = (e->outbps + 0xF) >> 5;
 		spin_unlock(&s->lock);
 	}
-	spin_unlock(&ipvs->est_lock);
-	mod_timer(&ipvs->est_timer, jiffies + 2*HZ);
+	spin_unlock(&est_lock);
+	mod_timer(&est_timer, jiffies + 2 * HZ);
 }
 
-void ip_vs_start_estimator(struct net *net, struct ip_vs_stats *stats)
+void ip_vs_new_estimator(struct ip_vs_stats *stats)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
 	struct ip_vs_estimator *est = &stats->est;
 
 	INIT_LIST_HEAD(&est->list);
 
-	spin_lock_bh(&ipvs->est_lock);
-	list_add(&est->list, &ipvs->est_list);
-	spin_unlock_bh(&ipvs->est_lock);
+	est->last_conns = stats->ustats.conns;
+	est->cps = stats->ustats.cps << 10;
+
+	est->last_inpkts = stats->ustats.inpkts;
+	est->inpps = stats->ustats.inpps << 10;
+
+	est->last_outpkts = stats->ustats.outpkts;
+	est->outpps = stats->ustats.outpps << 10;
+
+	est->last_inbytes = stats->ustats.inbytes;
+	est->inbps = (u64) (stats->ustats.inbps) << 5;
+
+	est->last_outbytes = stats->ustats.outbytes;
+	est->outbps = (u64) (stats->ustats.outbps) << 5;
+
+	spin_lock_bh(&est_lock);
+	list_add(&est->list, &est_list);
+	spin_unlock_bh(&est_lock);
 }
 
-void ip_vs_stop_estimator(struct net *net, struct ip_vs_stats *stats)
+void ip_vs_kill_estimator(struct ip_vs_stats *stats)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
 	struct ip_vs_estimator *est = &stats->est;
 
-	spin_lock_bh(&ipvs->est_lock);
+	spin_lock_bh(&est_lock);
 	list_del(&est->list);
-	spin_unlock_bh(&ipvs->est_lock);
+	spin_unlock_bh(&est_lock);
 }
 
 void ip_vs_zero_estimator(struct ip_vs_stats *stats)
 {
 	struct ip_vs_estimator *est = &stats->est;
-	struct ip_vs_stats_user *u = &stats->ustats;
-
-	/* reset counters, caller must hold the stats->lock lock */
-	est->last_inbytes = u->inbytes;
-	est->last_outbytes = u->outbytes;
-	est->last_conns = u->conns;
-	est->last_inpkts = u->inpkts;
-	est->last_outpkts = u->outpkts;
+
+	/* set counters zero, caller must hold the stats->lock lock */
+	est->last_inbytes = 0;
+	est->last_outbytes = 0;
+	est->last_conns = 0;
+	est->last_inpkts = 0;
+	est->last_outpkts = 0;
 	est->cps = 0;
 	est->inpps = 0;
 	est->outpps = 0;
@@ -179,31 +157,13 @@ void ip_vs_zero_estimator(struct ip_vs_stats *stats)
 	est->outbps = 0;
 }
 
-/* Get decoded rates */
-void ip_vs_read_estimator(struct ip_vs_stats_user *dst,
-			  struct ip_vs_stats *stats)
+int __init ip_vs_estimator_init(void)
 {
-	struct ip_vs_estimator *e = &stats->est;
-
-	dst->cps = (e->cps + 0x1FF) >> 10;
-	dst->inpps = (e->inpps + 0x1FF) >> 10;
-	dst->outpps = (e->outpps + 0x1FF) >> 10;
-	dst->inbps = (e->inbps + 0xF) >> 5;
-	dst->outbps = (e->outbps + 0xF) >> 5;
-}
-
-int __net_init ip_vs_estimator_net_init(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	INIT_LIST_HEAD(&ipvs->est_list);
-	spin_lock_init(&ipvs->est_lock);
-	setup_timer(&ipvs->est_timer, estimation_timer, (unsigned long)net);
-	mod_timer(&ipvs->est_timer, jiffies + 2 * HZ);
+	mod_timer(&est_timer, jiffies + 2 * HZ);
 	return 0;
 }
 
-void __net_exit ip_vs_estimator_net_cleanup(struct net *net)
+void ip_vs_estimator_cleanup(void)
 {
-	del_timer_sync(&net_ipvs(net)->est_timer);
+	del_timer_sync(&est_timer);
 }
diff --git a/net/netfilter/ipvs/ip_vs_ftp.c b/net/netfilter/ipvs/ip_vs_ftp.c
index 77c1732..c885542 100644
--- a/net/netfilter/ipvs/ip_vs_ftp.c
+++ b/net/netfilter/ipvs/ip_vs_ftp.c
@@ -32,65 +32,47 @@
 #include <linux/in.h>
 #include <linux/ip.h>
 #include <linux/netfilter.h>
-#include <net/netfilter/nf_conntrack.h>
-#include <net/netfilter/nf_conntrack_expect.h>
-#include <net/netfilter/nf_nat.h>
-#include <net/netfilter/nf_nat_helper.h>
-#include <linux/gfp.h>
 #include <net/protocol.h>
 #include <net/tcp.h>
 #include <asm/unaligned.h>
 
 #include <net/ip_vs.h>
 
-
-#define SERVER_STRING "227 "
-#define CLIENT_STRING "PORT"
-
+#define SERVER_STRING "227 Entering Passive Mode ("
+#define CLIENT_STRING "PORT "
 
 /*
  * List of ports (up to IP_VS_APP_MAX_PORTS) to be handled by helper
  * First port is set to the default port.
  */
-static unsigned int ports_count = 1;
-static unsigned short ports[IP_VS_APP_MAX_PORTS] = {21, 0};
-module_param_array(ports, ushort, &ports_count, 0444);
-MODULE_PARM_DESC(ports, "Ports to monitor for FTP control commands");
+static unsigned short ports[IP_VS_APP_MAX_PORTS] = { 21, 0 };
 
+module_param_array(ports, ushort, NULL, 0);
+MODULE_PARM_DESC(ports, "Ports to monitor for FTP control commands");
 
 /*	Dummy variable */
 static int ip_vs_ftp_pasv;
 
-
-static int
-ip_vs_ftp_init_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
+static int ip_vs_ftp_init_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
 {
-	/* We use connection tracking for the command connection */
-	cp->flags |= IP_VS_CONN_F_NFCT;
 	return 0;
 }
 
-
-static int
-ip_vs_ftp_done_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
+static int ip_vs_ftp_done_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
 {
 	return 0;
 }
 
-
 /*
  * Get <addr,port> from the string "xxx.xxx.xxx.xxx,ppp,ppp", started
- * with the "pattern", ignoring before "skip" and terminated with
- * the "term" character.
+ * with the "pattern" and terminated with the "term" character.
  * <addr,port> is in network order.
  */
 static int ip_vs_ftp_get_addrport(char *data, char *data_limit,
-				  const char *pattern, size_t plen,
-				  char skip, char term,
-				  __be32 *addr, __be16 *port,
+				  const char *pattern, size_t plen, char term,
+				  __be32 * addr, __be16 * port,
 				  char **start, char **end)
 {
-	char *s, c;
 	unsigned char p[6];
 	int i = 0;
 
@@ -105,38 +87,19 @@ static int ip_vs_ftp_get_addrport(char *data, char *data_limit,
 	if (strnicmp(data, pattern, plen) != 0) {
 		return 0;
 	}
-	s = data + plen;
-	if (skip) {
-		int found = 0;
-
-		for (;; s++) {
-			if (s == data_limit)
-				return -1;
-			if (!found) {
-				if (*s == skip)
-					found = 1;
-			} else if (*s != skip) {
-				break;
-			}
-		}
-	}
+	*start = data + plen;
 
-	for (data = s; ; data++) {
+	for (data = *start; *data != term; data++) {
 		if (data == data_limit)
 			return -1;
-		if (*data == term)
-			break;
 	}
 	*end = data;
 
 	memset(p, 0, sizeof(p));
-	for (data = s; ; data++) {
-		c = *data;
-		if (c == term)
-			break;
-		if (c >= '0' && c <= '9') {
-			p[i] = p[i]*10 + c - '0';
-		} else if (c == ',' && i < 5) {
+	for (data = *start; data != *end; data++) {
+		if (*data >= '0' && *data <= '9') {
+			p[i] = p[i] * 10 + *data - '0';
+		} else if (*data == ',' && i < 5) {
 			i++;
 		} else {
 			/* unexpected character */
@@ -147,7 +110,6 @@ static int ip_vs_ftp_get_addrport(char *data, char *data_limit,
 	if (i != 5)
 		return -1;
 
-	*start = s;
 	*addr = get_unaligned((__be32 *) p);
 	*port = get_unaligned((__be16 *) (p + 4));
 	return 1;
@@ -177,11 +139,8 @@ static int ip_vs_ftp_out(struct ip_vs_app *app, struct ip_vs_conn *cp,
 	__be16 port;
 	struct ip_vs_conn *n_cp;
 	char buf[24];		/* xxx.xxx.xxx.xxx,ppp,ppp\000 */
-	unsigned int buf_len;
-	int ret = 0;
-	enum ip_conntrack_info ctinfo;
-	struct nf_conn *ct;
-	struct net *net;
+	unsigned buf_len;
+	int ret, res_dir;
 
 #ifdef CONFIG_IP_VS_IPV6
 	/* This application helper doesn't work with IPv6 yet,
@@ -203,16 +162,14 @@ static int ip_vs_ftp_out(struct ip_vs_app *app, struct ip_vs_conn *cp,
 
 	if (cp->app_data == &ip_vs_ftp_pasv) {
 		iph = ip_hdr(skb);
-		th = (struct tcphdr *)&(((char *)iph)[iph->ihl*4]);
+		th = (struct tcphdr *)&(((char *)iph)[iph->ihl * 4]);
 		data = (char *)th + (th->doff << 2);
 		data_limit = skb_tail_pointer(skb);
 
 		if (ip_vs_ftp_get_addrport(data, data_limit,
 					   SERVER_STRING,
-					   sizeof(SERVER_STRING)-1,
-					   '(', ')',
-					   &from.ip, &port,
-					   &start, &end) != 1)
+					   sizeof(SERVER_STRING) - 1, ')',
+					   &from.ip, &port, &start, &end) != 1)
 			return 1;
 
 		IP_VS_DBG(7, "PASV response (%pI4:%d) -> %pI4:%d detected\n",
@@ -221,22 +178,15 @@ static int ip_vs_ftp_out(struct ip_vs_app *app, struct ip_vs_conn *cp,
 		/*
 		 * Now update or create an connection entry for it
 		 */
-		{
-			struct ip_vs_conn_param p;
-			ip_vs_conn_fill_param(ip_vs_conn_net(cp), AF_INET,
-					      iph->protocol, &from, port,
-					      &cp->caddr, 0, &p);
-			n_cp = ip_vs_conn_out_get(&p);
-		}
+		n_cp = ip_vs_conn_get(AF_INET, iph->protocol, &from, port,
+				      &cp->caddr, 0, &res_dir);
 		if (!n_cp) {
-			struct ip_vs_conn_param p;
-			ip_vs_conn_fill_param(ip_vs_conn_net(cp),
-					      AF_INET, IPPROTO_TCP, &cp->caddr,
-					      0, &cp->vaddr, port, &p);
-			n_cp = ip_vs_conn_new(&p, &from, port,
-					      IP_VS_CONN_F_NO_CPORT |
-					      IP_VS_CONN_F_NFCT,
-					      cp->dest, skb->mark);
+			n_cp = ip_vs_conn_new(AF_INET, IPPROTO_TCP,
+					      &cp->caddr, 0,
+					      &cp->vaddr, port,
+					      &from, port,
+					      IP_VS_CONN_F_NO_CPORT,
+					      cp->dest, NULL, 0);
 			if (!n_cp)
 				return 0;
 
@@ -249,56 +199,33 @@ static int ip_vs_ftp_out(struct ip_vs_app *app, struct ip_vs_conn *cp,
 		 */
 		from.ip = n_cp->vaddr.ip;
 		port = n_cp->vport;
-		snprintf(buf, sizeof(buf), "%u,%u,%u,%u,%u,%u",
-			 ((unsigned char *)&from.ip)[0],
-			 ((unsigned char *)&from.ip)[1],
-			 ((unsigned char *)&from.ip)[2],
-			 ((unsigned char *)&from.ip)[3],
-			 ntohs(port) >> 8,
-			 ntohs(port) & 0xFF);
-
+		sprintf(buf, "%pI4,%d,%d", &from.ip,
+			(ntohs(port) >> 8) & 255, ntohs(port) & 255);
 		buf_len = strlen(buf);
 
-		ct = nf_ct_get(skb, &ctinfo);
-		if (ct && !nf_ct_is_untracked(ct) && nfct_nat(ct)) {
-			/* If mangling fails this function will return 0
-			 * which will cause the packet to be dropped.
-			 * Mangling can only fail under memory pressure,
-			 * hopefully it will succeed on the retransmitted
-			 * packet.
-			 */
-			rcu_read_lock();
-			ret = nf_nat_mangle_tcp_packet(skb, ct, ctinfo,
-						       iph->ihl * 4,
-						       start-data, end-start,
-						       buf, buf_len);
-			rcu_read_unlock();
-			if (ret) {
-				ip_vs_nfct_expect_related(skb, ct, n_cp,
-							  IPPROTO_TCP, 0, 0);
-				if (skb->ip_summed == CHECKSUM_COMPLETE)
-					skb->ip_summed = CHECKSUM_UNNECESSARY;
-				/* csum is updated */
-				ret = 1;
-			}
-		}
-
 		/*
-		 * Not setting 'diff' is intentional, otherwise the sequence
-		 * would be adjusted twice.
+		 * Calculate required delta-offset to keep TCP happy
 		 */
+		*diff = buf_len - (end - start);
+
+		if (*diff == 0) {
+			/* simply replace it with new passive address */
+			memcpy(start, buf, buf_len);
+			ret = 1;
+		} else {
+			ret = !ip_vs_skb_replace(skb, GFP_ATOMIC, start,
+						 end - start, buf, buf_len);
+		}
 
-		net = skb_net(skb);
 		cp->app_data = NULL;
-		ip_vs_tcp_conn_listen(net, n_cp);
+		ip_vs_tcp_conn_listen(n_cp);
 		ip_vs_conn_put(n_cp);
 		return ret;
 	}
 	return 1;
 }
 
-
-/*
+/*/
  * Look at incoming ftp packets to catch the PASV/PORT command
  * (outside-to-inside).
  *
@@ -319,7 +246,7 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 	union nf_inet_addr to;
 	__be16 port;
 	struct ip_vs_conn *n_cp;
-	struct net *net;
+	int res_dir;
 
 #ifdef CONFIG_IP_VS_IPV6
 	/* This application helper doesn't work with IPv6 yet,
@@ -344,7 +271,7 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 	 * Detecting whether it is passive
 	 */
 	iph = ip_hdr(skb);
-	th = (struct tcphdr *)&(((char *)iph)[iph->ihl*4]);
+	th = (struct tcphdr *)&(((char *)iph)[iph->ihl * 4]);
 
 	/* Since there may be OPTIONS in the TCP packet and the HLEN is
 	   the length of the header in 32-bit multiples, it is accurate
@@ -356,8 +283,7 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 		if (strnicmp(data, "PASV\r\n", 6) == 0) {
 			/* Passive mode on */
 			IP_VS_DBG(7, "got PASV at %td of %td\n",
-				  data - data_start,
-				  data_limit - data_start);
+				  data - data_start, data_limit - data_start);
 			cp->app_data = &ip_vs_ftp_pasv;
 			return 1;
 		}
@@ -372,9 +298,8 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 	 * connection.
 	 */
 	if (ip_vs_ftp_get_addrport(data_start, data_limit,
-				   CLIENT_STRING, sizeof(CLIENT_STRING)-1,
-				   ' ', '\r', &to.ip, &port,
-				   &start, &end) != 1)
+				   CLIENT_STRING, sizeof(CLIENT_STRING) - 1,
+				   '\r', &to.ip, &port, &start, &end) != 1)
 		return 1;
 
 	IP_VS_DBG(7, "PORT %pI4:%d detected\n", &to.ip, ntohs(port));
@@ -389,101 +314,72 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 		  ip_vs_proto_name(iph->protocol),
 		  &to.ip, ntohs(port), &cp->vaddr.ip, 0);
 
-	{
-		struct ip_vs_conn_param p;
-		ip_vs_conn_fill_param(ip_vs_conn_net(cp), AF_INET,
-				      iph->protocol, &to, port, &cp->vaddr,
-				      htons(ntohs(cp->vport)-1), &p);
-		n_cp = ip_vs_conn_in_get(&p);
-		if (!n_cp) {
-			n_cp = ip_vs_conn_new(&p, &cp->daddr,
-					      htons(ntohs(cp->dport)-1),
-					      IP_VS_CONN_F_NFCT, cp->dest,
-					      skb->mark);
-			if (!n_cp)
-				return 0;
+	n_cp = ip_vs_conn_get(AF_INET, iph->protocol,
+			      &to, port,
+			      &cp->vaddr, htons(ntohs(cp->vport) - 1),
+			      &res_dir);
+	if (!n_cp) {
+		n_cp = ip_vs_conn_new(AF_INET, IPPROTO_TCP,
+				      &to, port,
+				      &cp->vaddr, htons(ntohs(cp->vport) - 1),
+				      &cp->daddr, htons(ntohs(cp->dport) - 1),
+				      0, cp->dest, NULL, 0);
+		if (!n_cp)
+			return 0;
 
-			/* add its controller */
-			ip_vs_control_add(n_cp, cp);
-		}
+		/* add its controller */
+		ip_vs_control_add(n_cp, cp);
 	}
 
 	/*
-	 *	Move tunnel to listen state
+	 *      Move tunnel to listen state
 	 */
-	net = skb_net(skb);
-	ip_vs_tcp_conn_listen(net, n_cp);
+	ip_vs_tcp_conn_listen(n_cp);
 	ip_vs_conn_put(n_cp);
 
 	return 1;
 }
 
-
 static struct ip_vs_app ip_vs_ftp = {
-	.name =		"ftp",
-	.type =		IP_VS_APP_TYPE_FTP,
-	.protocol =	IPPROTO_TCP,
-	.module =	THIS_MODULE,
-	.incs_list =	LIST_HEAD_INIT(ip_vs_ftp.incs_list),
-	.init_conn =	ip_vs_ftp_init_conn,
-	.done_conn =	ip_vs_ftp_done_conn,
-	.bind_conn =	NULL,
-	.unbind_conn =	NULL,
-	.pkt_out =	ip_vs_ftp_out,
-	.pkt_in =	ip_vs_ftp_in,
+	.name = "ftp",
+	.type = IP_VS_APP_TYPE_FTP,
+	.protocol = IPPROTO_TCP,
+	.module = THIS_MODULE,
+	.incs_list = LIST_HEAD_INIT(ip_vs_ftp.incs_list),
+	.init_conn = ip_vs_ftp_init_conn,
+	.done_conn = ip_vs_ftp_done_conn,
+	.bind_conn = NULL,
+	.unbind_conn = NULL,
+	.pkt_out = ip_vs_ftp_out,
+	.pkt_in = ip_vs_ftp_in,
 };
 
 /*
- *	per netns ip_vs_ftp initialization
+ *	ip_vs_ftp initialization
  */
-static int __net_init __ip_vs_ftp_init(struct net *net)
+static int __init ip_vs_ftp_init(void)
 {
 	int i, ret;
-	struct ip_vs_app *app;
-	struct netns_ipvs *ipvs = net_ipvs(net);
+	struct ip_vs_app *app = &ip_vs_ftp;
 
-	if (!ipvs)
-		return -ENOENT;
-
-	app = register_ip_vs_app(net, &ip_vs_ftp);
-	if (IS_ERR(app))
-		return PTR_ERR(app);
+	ret = register_ip_vs_app(app);
+	if (ret)
+		return ret;
 
-	for (i = 0; i < ports_count; i++) {
+	for (i = 0; i < IP_VS_APP_MAX_PORTS; i++) {
 		if (!ports[i])
 			continue;
-		ret = register_ip_vs_app_inc(net, app, app->protocol, ports[i]);
+		ret = register_ip_vs_app_inc(app, app->protocol, ports[i]);
 		if (ret)
-			goto err_unreg;
+			break;
 		pr_info("%s: loaded support on port[%d] = %d\n",
 			app->name, i, ports[i]);
 	}
-	return 0;
 
-err_unreg:
-	unregister_ip_vs_app(net, &ip_vs_ftp);
-	return ret;
-}
-/*
- *	netns exit
- */
-static void __ip_vs_ftp_exit(struct net *net)
-{
-	unregister_ip_vs_app(net, &ip_vs_ftp);
-}
-
-static struct pernet_operations ip_vs_ftp_ops = {
-	.init = __ip_vs_ftp_init,
-	.exit = __ip_vs_ftp_exit,
-};
+	if (ret)
+		unregister_ip_vs_app(app);
 
-static int __init ip_vs_ftp_init(void)
-{
-	int rv;
-
-	rv = register_pernet_subsys(&ip_vs_ftp_ops);
-	/* rcu_barrier() is called by netns on error */
-	return rv;
+	return ret;
 }
 
 /*
@@ -491,11 +387,9 @@ static int __init ip_vs_ftp_init(void)
  */
 static void __exit ip_vs_ftp_exit(void)
 {
-	unregister_pernet_subsys(&ip_vs_ftp_ops);
-	/* rcu_barrier() is called by netns */
+	unregister_ip_vs_app(&ip_vs_ftp);
 }
 
-
 module_init(ip_vs_ftp_init);
 module_exit(ip_vs_ftp_exit);
 MODULE_LICENSE("GPL");
diff --git a/net/netfilter/ipvs/ip_vs_lblc.c b/net/netfilter/ipvs/ip_vs_lblc.c
index 44595b8..302a196 100644
--- a/net/netfilter/ipvs/ip_vs_lblc.c
+++ b/net/netfilter/ipvs/ip_vs_lblc.c
@@ -11,7 +11,7 @@
  * Changes:
  *     Martin Hamilton         :    fixed the terrible locking bugs
  *                                   *lock(tbl->lock) ==> *lock(&tbl->lock)
- *     Wensong Zhang           :    fixed the uninitialized tbl->lock bug
+ *     Wensong Zhang           :    fixed the uninitilized tbl->lock bug
  *     Wensong Zhang           :    added doing full expiration check to
  *                                   collect stale entries of 24+ hours when
  *                                   no partial expire check in a half hour
@@ -43,7 +43,6 @@
 #define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
 
 #include <linux/ip.h>
-#include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/skbuff.h>
@@ -63,8 +62,6 @@
 #define CHECK_EXPIRE_INTERVAL   (60*HZ)
 #define ENTRY_TIMEOUT           (6*60*HZ)
 
-#define DEFAULT_EXPIRATION	(24*60*60*HZ)
-
 /*
  *    It is for full expiration check.
  *    When there is no partial expiration check (garbage collection)
@@ -72,6 +69,7 @@
  *    entries that haven't been touched for a day.
  */
 #define COUNT_FOR_FULL_EXPIRATION   30
+static int sysctl_ip_vs_lblc_expiration = 24*60*60*HZ;
 
 
 /*
@@ -90,12 +88,11 @@
  *      IP address and its destination server
  */
 struct ip_vs_lblc_entry {
-	struct hlist_node	list;
+	struct list_head        list;
 	int			af;		/* address family */
 	union nf_inet_addr      addr;           /* destination IP address */
-	struct ip_vs_dest __rcu	*dest;          /* real server (cache) */
+	struct ip_vs_dest       *dest;          /* real server (cache) */
 	unsigned long           lastuse;        /* last used time */
-	struct rcu_head		rcu_head;
 };
 
 
@@ -103,52 +100,48 @@ struct ip_vs_lblc_entry {
  *      IPVS lblc hash table
  */
 struct ip_vs_lblc_table {
-	struct rcu_head		rcu_head;
-	struct hlist_head	bucket[IP_VS_LBLC_TAB_SIZE];  /* hash bucket */
-	struct timer_list       periodic_timer; /* collect stale entries */
+	struct list_head        bucket[IP_VS_LBLC_TAB_SIZE];  /* hash bucket */
 	atomic_t                entries;        /* number of entries */
 	int                     max_size;       /* maximum size of entries */
+	struct timer_list       periodic_timer; /* collect stale entries */
 	int                     rover;          /* rover for expire check */
 	int                     counter;        /* counter for no expire */
-	bool			dead;
 };
 
 
 /*
  *      IPVS LBLC sysctl table
  */
-#ifdef CONFIG_SYSCTL
-static struct ctl_table vs_vars_table[] = {
+
+static ctl_table vs_vars_table[] = {
 	{
 		.procname	= "lblc_expiration",
-		.data		= NULL,
+		.data		= &sysctl_ip_vs_lblc_expiration,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec_jiffies,
 	},
-	{ }
+	{}
 };
-#endif
+
+static struct ctl_table_header * sysctl_header;
 
 static inline void ip_vs_lblc_free(struct ip_vs_lblc_entry *en)
 {
-	struct ip_vs_dest *dest;
-
-	hlist_del_rcu(&en->list);
+	list_del(&en->list);
 	/*
-	 * We don't kfree dest because it is referred either by its service
+	 * We don't kfree dest because it is refered either by its service
 	 * or the trash dest list.
 	 */
-	dest = rcu_dereference_protected(en->dest, 1);
-	ip_vs_dest_put(dest);
-	kfree_rcu(en, rcu_head);
+	atomic_dec(&en->dest->refcnt);
+	kfree(en);
 }
 
 
 /*
  *	Returns hash value for IPVS LBLC entry
  */
-static inline unsigned int
+static inline unsigned
 ip_vs_lblc_hashkey(int af, const union nf_inet_addr *addr)
 {
 	__be32 addr_fold = addr->ip;
@@ -169,22 +162,25 @@ ip_vs_lblc_hashkey(int af, const union nf_inet_addr *addr)
 static void
 ip_vs_lblc_hash(struct ip_vs_lblc_table *tbl, struct ip_vs_lblc_entry *en)
 {
-	unsigned int hash = ip_vs_lblc_hashkey(en->af, &en->addr);
+	unsigned hash = ip_vs_lblc_hashkey(en->af, &en->addr);
 
-	hlist_add_head_rcu(&en->list, &tbl->bucket[hash]);
+	list_add(&en->list, &tbl->bucket[hash]);
 	atomic_inc(&tbl->entries);
 }
 
 
-/* Get ip_vs_lblc_entry associated with supplied parameters. */
+/*
+ *  Get ip_vs_lblc_entry associated with supplied parameters. Called under read
+ *  lock
+ */
 static inline struct ip_vs_lblc_entry *
 ip_vs_lblc_get(int af, struct ip_vs_lblc_table *tbl,
 	       const union nf_inet_addr *addr)
 {
-	unsigned int hash = ip_vs_lblc_hashkey(af, addr);
+	unsigned hash = ip_vs_lblc_hashkey(af, addr);
 	struct ip_vs_lblc_entry *en;
 
-	hlist_for_each_entry_rcu(en, &tbl->bucket[hash], list)
+	list_for_each_entry(en, &tbl->bucket[hash], list)
 		if (ip_vs_addr_equal(af, &en->addr, addr))
 			return en;
 
@@ -194,7 +190,7 @@ ip_vs_lblc_get(int af, struct ip_vs_lblc_table *tbl,
 
 /*
  * Create or update an ip_vs_lblc_entry, which is a mapping of a destination IP
- * address to a server. Called under spin lock.
+ * address to a server. Called under write lock.
  */
 static inline struct ip_vs_lblc_entry *
 ip_vs_lblc_new(struct ip_vs_lblc_table *tbl, const union nf_inet_addr *daddr,
@@ -205,27 +201,23 @@ ip_vs_lblc_new(struct ip_vs_lblc_table *tbl, const union nf_inet_addr *daddr,
 	en = ip_vs_lblc_get(dest->af, tbl, daddr);
 	if (!en) {
 		en = kmalloc(sizeof(*en), GFP_ATOMIC);
-		if (!en)
+		if (!en) {
+			pr_err("%s(): no memory\n", __func__);
 			return NULL;
+		}
 
 		en->af = dest->af;
 		ip_vs_addr_copy(dest->af, &en->addr, daddr);
 		en->lastuse = jiffies;
 
-		ip_vs_dest_hold(dest);
-		RCU_INIT_POINTER(en->dest, dest);
+		atomic_inc(&dest->refcnt);
+		en->dest = dest;
 
 		ip_vs_lblc_hash(tbl, en);
-	} else {
-		struct ip_vs_dest *old_dest;
-
-		old_dest = rcu_dereference_protected(en->dest, 1);
-		if (old_dest != dest) {
-			ip_vs_dest_put(old_dest);
-			ip_vs_dest_hold(dest);
-			/* No ordering constraints for refcnt */
-			RCU_INIT_POINTER(en->dest, dest);
-		}
+	} else if (en->dest != dest) {
+		atomic_dec(&en->dest->refcnt);
+		atomic_inc(&dest->refcnt);
+		en->dest = dest;
 	}
 
 	return en;
@@ -235,56 +227,40 @@ ip_vs_lblc_new(struct ip_vs_lblc_table *tbl, const union nf_inet_addr *daddr,
 /*
  *      Flush all the entries of the specified table.
  */
-static void ip_vs_lblc_flush(struct ip_vs_service *svc)
+static void ip_vs_lblc_flush(struct ip_vs_lblc_table *tbl)
 {
-	struct ip_vs_lblc_table *tbl = svc->sched_data;
-	struct ip_vs_lblc_entry *en;
-	struct hlist_node *next;
+	struct ip_vs_lblc_entry *en, *nxt;
 	int i;
 
-	spin_lock_bh(&svc->sched_lock);
-	tbl->dead = 1;
 	for (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {
-		hlist_for_each_entry_safe(en, next, &tbl->bucket[i], list) {
+		list_for_each_entry_safe(en, nxt, &tbl->bucket[i], list) {
 			ip_vs_lblc_free(en);
 			atomic_dec(&tbl->entries);
 		}
 	}
-	spin_unlock_bh(&svc->sched_lock);
 }
 
-static int sysctl_lblc_expiration(struct ip_vs_service *svc)
-{
-#ifdef CONFIG_SYSCTL
-	struct netns_ipvs *ipvs = net_ipvs(svc->net);
-	return ipvs->sysctl_lblc_expiration;
-#else
-	return DEFAULT_EXPIRATION;
-#endif
-}
 
 static inline void ip_vs_lblc_full_check(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblc_table *tbl = svc->sched_data;
-	struct ip_vs_lblc_entry *en;
-	struct hlist_node *next;
+	struct ip_vs_lblc_entry *en, *nxt;
 	unsigned long now = jiffies;
 	int i, j;
 
 	for (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLC_TAB_MASK;
 
-		spin_lock(&svc->sched_lock);
-		hlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {
+		write_lock(&svc->sched_lock);
+		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
 			if (time_before(now,
-					en->lastuse +
-					sysctl_lblc_expiration(svc)))
+					en->lastuse + sysctl_ip_vs_lblc_expiration))
 				continue;
 
 			ip_vs_lblc_free(en);
 			atomic_dec(&tbl->entries);
 		}
-		spin_unlock(&svc->sched_lock);
+		write_unlock(&svc->sched_lock);
 	}
 	tbl->rover = j;
 }
@@ -308,8 +284,7 @@ static void ip_vs_lblc_check_expire(unsigned long data)
 	unsigned long now = jiffies;
 	int goal;
 	int i, j;
-	struct ip_vs_lblc_entry *en;
-	struct hlist_node *next;
+	struct ip_vs_lblc_entry *en, *nxt;
 
 	if ((tbl->counter % COUNT_FOR_FULL_EXPIRATION) == 0) {
 		/* do full expiration check */
@@ -330,8 +305,8 @@ static void ip_vs_lblc_check_expire(unsigned long data)
 	for (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLC_TAB_MASK;
 
-		spin_lock(&svc->sched_lock);
-		hlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {
+		write_lock(&svc->sched_lock);
+		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
 			if (time_before(now, en->lastuse + ENTRY_TIMEOUT))
 				continue;
 
@@ -339,7 +314,7 @@ static void ip_vs_lblc_check_expire(unsigned long data)
 			atomic_dec(&tbl->entries);
 			goal--;
 		}
-		spin_unlock(&svc->sched_lock);
+		write_unlock(&svc->sched_lock);
 		if (goal <= 0)
 			break;
 	}
@@ -358,10 +333,11 @@ static int ip_vs_lblc_init_svc(struct ip_vs_service *svc)
 	/*
 	 *    Allocate the ip_vs_lblc_table for this service
 	 */
-	tbl = kmalloc(sizeof(*tbl), GFP_KERNEL);
-	if (tbl == NULL)
+	tbl = kmalloc(sizeof(*tbl), GFP_ATOMIC);
+	if (tbl == NULL) {
+		pr_err("%s(): no memory\n", __func__);
 		return -ENOMEM;
-
+	}
 	svc->sched_data = tbl;
 	IP_VS_DBG(6, "LBLC hash table (memory=%Zdbytes) allocated for "
 		  "current service\n", sizeof(*tbl));
@@ -370,12 +346,11 @@ static int ip_vs_lblc_init_svc(struct ip_vs_service *svc)
 	 *    Initialize the hash buckets
 	 */
 	for (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {
-		INIT_HLIST_HEAD(&tbl->bucket[i]);
+		INIT_LIST_HEAD(&tbl->bucket[i]);
 	}
 	tbl->max_size = IP_VS_LBLC_TAB_SIZE*16;
 	tbl->rover = 0;
 	tbl->counter = 1;
-	tbl->dead = 0;
 
 	/*
 	 *    Hook periodic timer for garbage collection
@@ -388,7 +363,7 @@ static int ip_vs_lblc_init_svc(struct ip_vs_service *svc)
 }
 
 
-static void ip_vs_lblc_done_svc(struct ip_vs_service *svc)
+static int ip_vs_lblc_done_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblc_table *tbl = svc->sched_data;
 
@@ -396,12 +371,14 @@ static void ip_vs_lblc_done_svc(struct ip_vs_service *svc)
 	del_timer_sync(&tbl->periodic_timer);
 
 	/* got to clean up table entries here */
-	ip_vs_lblc_flush(svc);
+	ip_vs_lblc_flush(tbl);
 
 	/* release the table itself */
-	kfree_rcu(tbl, rcu_head);
+	kfree(tbl);
 	IP_VS_DBG(6, "LBLC hash table (memory=%Zdbytes) released\n",
 		  sizeof(*tbl));
+
+	return 0;
 }
 
 
@@ -412,7 +389,12 @@ __ip_vs_lblc_schedule(struct ip_vs_service *svc)
 	int loh, doh;
 
 	/*
-	 * We use the following formula to estimate the load:
+	 * We think the overhead of processing active connections is fifty
+	 * times higher than that of inactive connections in average. (This
+	 * fifty times might not be accurate, we will change it later.) We
+	 * use the following formula to estimate the overhead:
+	 *                dest->activeconns*50 + dest->inactconns
+	 * and the load:
 	 *                (dest overhead) / dest->weight
 	 *
 	 * Remember -- no floats in kernel mode!!!
@@ -423,12 +405,13 @@ __ip_vs_lblc_schedule(struct ip_vs_service *svc)
 	 * The server with weight=0 is quiesced and will not receive any
 	 * new connection.
 	 */
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 		if (atomic_read(&dest->weight) > 0) {
 			least = dest;
-			loh = ip_vs_dest_conn_overhead(least);
+			loh = atomic_read(&least->activeconns) * 50
+				+ atomic_read(&least->inactconns);
 			goto nextstage;
 		}
 	}
@@ -438,11 +421,12 @@ __ip_vs_lblc_schedule(struct ip_vs_service *svc)
 	 *    Find the destination with the least load.
 	 */
   nextstage:
-	list_for_each_entry_continue_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
-		doh = ip_vs_dest_conn_overhead(dest);
+		doh = atomic_read(&dest->activeconns) * 50
+			+ atomic_read(&dest->inactconns);
 		if (loh * atomic_read(&dest->weight) >
 		    doh * atomic_read(&least->weight)) {
 			least = dest;
@@ -472,7 +456,7 @@ is_overloaded(struct ip_vs_dest *dest, struct ip_vs_service *svc)
 	if (atomic_read(&dest->activeconns) > atomic_read(&dest->weight)) {
 		struct ip_vs_dest *d;
 
-		list_for_each_entry_rcu(d, &svc->destinations, n_list) {
+		list_for_each_entry(d, &svc->destinations, n_list) {
 			if (atomic_read(&d->activeconns)*2
 			    < atomic_read(&d->weight)) {
 				return 1;
@@ -494,11 +478,12 @@ ip_vs_lblc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	struct ip_vs_dest *dest = NULL;
 	struct ip_vs_lblc_entry *en;
 
-	ip_vs_fill_iph_addr_only(svc->af, skb, &iph);
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
 
 	IP_VS_DBG(6, "%s(): Scheduling...\n", __func__);
 
 	/* First look in our cache */
+	read_lock(&svc->sched_lock);
 	en = ip_vs_lblc_get(svc->af, tbl, &iph.daddr);
 	if (en) {
 		/* We only hold a read lock, but this is atomic */
@@ -513,24 +498,26 @@ ip_vs_lblc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 		 * free up entries from the trash at any time.
 		 */
 
-		dest = rcu_dereference(en->dest);
-		if ((dest->flags & IP_VS_DEST_F_AVAILABLE) &&
-		    atomic_read(&dest->weight) > 0 && !is_overloaded(dest, svc))
-			goto out;
+		if (en->dest->flags & IP_VS_DEST_F_AVAILABLE)
+			dest = en->dest;
 	}
+	read_unlock(&svc->sched_lock);
+
+	/* If the destination has a weight and is not overloaded, use it */
+	if (dest && atomic_read(&dest->weight) > 0 && !is_overloaded(dest, svc))
+		goto out;
 
 	/* No cache entry or it is invalid, time to schedule */
 	dest = __ip_vs_lblc_schedule(svc);
 	if (!dest) {
-		ip_vs_scheduler_err(svc, "no destination available");
+		IP_VS_ERR_RL("LBLC: no destination available\n");
 		return NULL;
 	}
 
 	/* If we fail to create a cache entry, we'll just use the valid dest */
-	spin_lock_bh(&svc->sched_lock);
-	if (!tbl->dead)
-		ip_vs_lblc_new(tbl, &iph.daddr, dest);
-	spin_unlock_bh(&svc->sched_lock);
+	write_lock(&svc->sched_lock);
+	ip_vs_lblc_new(tbl, &iph.daddr, dest);
+	write_unlock(&svc->sched_lock);
 
 out:
 	IP_VS_DBG_BUF(6, "LBLC: destination IP address %s --> server %s:%d\n",
@@ -555,85 +542,23 @@ static struct ip_vs_scheduler ip_vs_lblc_scheduler =
 	.schedule =		ip_vs_lblc_schedule,
 };
 
-/*
- *  per netns init.
- */
-#ifdef CONFIG_SYSCTL
-static int __net_init __ip_vs_lblc_init(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	if (!ipvs)
-		return -ENOENT;
-
-	if (!net_eq(net, &init_net)) {
-		ipvs->lblc_ctl_table = kmemdup(vs_vars_table,
-						sizeof(vs_vars_table),
-						GFP_KERNEL);
-		if (ipvs->lblc_ctl_table == NULL)
-			return -ENOMEM;
-
-		/* Don't export sysctls to unprivileged users */
-		if (net->user_ns != &init_user_ns)
-			ipvs->lblc_ctl_table[0].procname = NULL;
-
-	} else
-		ipvs->lblc_ctl_table = vs_vars_table;
-	ipvs->sysctl_lblc_expiration = DEFAULT_EXPIRATION;
-	ipvs->lblc_ctl_table[0].data = &ipvs->sysctl_lblc_expiration;
-
-	ipvs->lblc_ctl_header =
-		register_net_sysctl(net, "net/ipv4/vs", ipvs->lblc_ctl_table);
-	if (!ipvs->lblc_ctl_header) {
-		if (!net_eq(net, &init_net))
-			kfree(ipvs->lblc_ctl_table);
-		return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static void __net_exit __ip_vs_lblc_exit(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	unregister_net_sysctl_table(ipvs->lblc_ctl_header);
-
-	if (!net_eq(net, &init_net))
-		kfree(ipvs->lblc_ctl_table);
-}
-
-#else
-
-static int __net_init __ip_vs_lblc_init(struct net *net) { return 0; }
-static void __net_exit __ip_vs_lblc_exit(struct net *net) { }
-
-#endif
-
-static struct pernet_operations ip_vs_lblc_ops = {
-	.init = __ip_vs_lblc_init,
-	.exit = __ip_vs_lblc_exit,
-};
 
 static int __init ip_vs_lblc_init(void)
 {
 	int ret;
 
-	ret = register_pernet_subsys(&ip_vs_lblc_ops);
-	if (ret)
-		return ret;
-
+	sysctl_header = register_net_sysctl(&init_net, "net/ipv4/vs", vs_vars_table);
 	ret = register_ip_vs_scheduler(&ip_vs_lblc_scheduler);
 	if (ret)
-		unregister_pernet_subsys(&ip_vs_lblc_ops);
+		unregister_sysctl_table(sysctl_header);
 	return ret;
 }
 
+
 static void __exit ip_vs_lblc_cleanup(void)
 {
+	unregister_sysctl_table(sysctl_header);
 	unregister_ip_vs_scheduler(&ip_vs_lblc_scheduler);
-	unregister_pernet_subsys(&ip_vs_lblc_ops);
-	synchronize_rcu();
 }
 
 
diff --git a/net/netfilter/ipvs/ip_vs_lblcr.c b/net/netfilter/ipvs/ip_vs_lblcr.c
index 876937d..9fe3d9b 100644
--- a/net/netfilter/ipvs/ip_vs_lblcr.c
+++ b/net/netfilter/ipvs/ip_vs_lblcr.c
@@ -45,8 +45,6 @@
 #include <linux/kernel.h>
 #include <linux/skbuff.h>
 #include <linux/jiffies.h>
-#include <linux/list.h>
-#include <linux/slab.h>
 
 /* for sysctl */
 #include <linux/fs.h>
@@ -63,8 +61,6 @@
 #define CHECK_EXPIRE_INTERVAL   (60*HZ)
 #define ENTRY_TIMEOUT           (6*60*HZ)
 
-#define DEFAULT_EXPIRATION	(24*60*60*HZ)
-
 /*
  *    It is for full expiration check.
  *    When there is no partial expiration check (garbage collection)
@@ -72,6 +68,8 @@
  *    entries that haven't been touched for a day.
  */
 #define COUNT_FOR_FULL_EXPIRATION   30
+static int sysctl_ip_vs_lblcr_expiration = 24*60*60*HZ;
+
 
 /*
  *     for IPVS lblcr entry hash table
@@ -87,91 +85,88 @@
 /*
  *      IPVS destination set structure and operations
  */
-struct ip_vs_dest_set_elem {
-	struct list_head	list;          /* list link */
-	struct ip_vs_dest __rcu *dest;         /* destination server */
-	struct rcu_head		rcu_head;
+struct ip_vs_dest_list {
+	struct ip_vs_dest_list  *next;          /* list link */
+	struct ip_vs_dest       *dest;          /* destination server */
 };
 
 struct ip_vs_dest_set {
 	atomic_t                size;           /* set size */
 	unsigned long           lastmod;        /* last modified time */
-	struct list_head	list;           /* destination list */
+	struct ip_vs_dest_list  *list;          /* destination list */
+	rwlock_t	        lock;           /* lock for this list */
 };
 
 
-static void ip_vs_dest_set_insert(struct ip_vs_dest_set *set,
-				  struct ip_vs_dest *dest, bool check)
+static struct ip_vs_dest_list *
+ip_vs_dest_set_insert(struct ip_vs_dest_set *set, struct ip_vs_dest *dest)
 {
-	struct ip_vs_dest_set_elem *e;
+	struct ip_vs_dest_list *e;
 
-	if (check) {
-		list_for_each_entry(e, &set->list, list) {
-			struct ip_vs_dest *d;
-
-			d = rcu_dereference_protected(e->dest, 1);
-			if (d == dest)
-				/* already existed */
-				return;
-		}
+	for (e=set->list; e!=NULL; e=e->next) {
+		if (e->dest == dest)
+			/* already existed */
+			return NULL;
 	}
 
 	e = kmalloc(sizeof(*e), GFP_ATOMIC);
-	if (e == NULL)
-		return;
+	if (e == NULL) {
+		pr_err("%s(): no memory\n", __func__);
+		return NULL;
+	}
 
-	ip_vs_dest_hold(dest);
-	RCU_INIT_POINTER(e->dest, dest);
+	atomic_inc(&dest->refcnt);
+	e->dest = dest;
 
-	list_add_rcu(&e->list, &set->list);
+	/* link it to the list */
+	e->next = set->list;
+	set->list = e;
 	atomic_inc(&set->size);
 
 	set->lastmod = jiffies;
+	return e;
 }
 
 static void
 ip_vs_dest_set_erase(struct ip_vs_dest_set *set, struct ip_vs_dest *dest)
 {
-	struct ip_vs_dest_set_elem *e;
+	struct ip_vs_dest_list *e, **ep;
 
-	list_for_each_entry(e, &set->list, list) {
-		struct ip_vs_dest *d;
-
-		d = rcu_dereference_protected(e->dest, 1);
-		if (d == dest) {
+	for (ep=&set->list, e=*ep; e!=NULL; e=*ep) {
+		if (e->dest == dest) {
 			/* HIT */
+			*ep = e->next;
 			atomic_dec(&set->size);
 			set->lastmod = jiffies;
-			ip_vs_dest_put(dest);
-			list_del_rcu(&e->list);
-			kfree_rcu(e, rcu_head);
+			atomic_dec(&e->dest->refcnt);
+			kfree(e);
 			break;
 		}
+		ep = &e->next;
 	}
 }
 
 static void ip_vs_dest_set_eraseall(struct ip_vs_dest_set *set)
 {
-	struct ip_vs_dest_set_elem *e, *ep;
-
-	list_for_each_entry_safe(e, ep, &set->list, list) {
-		struct ip_vs_dest *d;
+	struct ip_vs_dest_list *e, **ep;
 
-		d = rcu_dereference_protected(e->dest, 1);
+	write_lock(&set->lock);
+	for (ep=&set->list, e=*ep; e!=NULL; e=*ep) {
+		*ep = e->next;
 		/*
-		 * We don't kfree dest because it is referred either
+		 * We don't kfree dest because it is refered either
 		 * by its service or by the trash dest list.
 		 */
-		ip_vs_dest_put(d);
-		list_del_rcu(&e->list);
-		kfree_rcu(e, rcu_head);
+		atomic_dec(&e->dest->refcnt);
+		kfree(e);
 	}
+	write_unlock(&set->lock);
 }
 
 /* get weighted least-connection node in the destination set */
 static inline struct ip_vs_dest *ip_vs_dest_set_min(struct ip_vs_dest_set *set)
 {
-	register struct ip_vs_dest_set_elem *e;
+	register struct ip_vs_dest_list *e;
 	struct ip_vs_dest *dest, *least;
 	int loh, doh;
 
@@ -179,14 +174,15 @@ static inline struct ip_vs_dest *ip_vs_dest_set_min(struct ip_vs_dest_set *set)
 		return NULL;
 
 	/* select the first destination server, whose weight > 0 */
-	list_for_each_entry_rcu(e, &set->list, list) {
-		least = rcu_dereference(e->dest);
+	for (e=set->list; e!=NULL; e=e->next) {
+		least = e->dest;
 		if (least->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
 		if ((atomic_read(&least->weight) > 0)
 		    && (least->flags & IP_VS_DEST_F_AVAILABLE)) {
-			loh = ip_vs_dest_conn_overhead(least);
+			loh = atomic_read(&least->activeconns) * 50
+				+ atomic_read(&least->inactconns);
 			goto nextstage;
 		}
 	}
@@ -194,12 +190,13 @@ static inline struct ip_vs_dest *ip_vs_dest_set_min(struct ip_vs_dest_set *set)
 
 	/* find the destination with the weighted least load */
   nextstage:
-	list_for_each_entry_continue_rcu(e, &set->list, list) {
-		dest = rcu_dereference(e->dest);
+	for (e=e->next; e!=NULL; e=e->next) {
+		dest = e->dest;
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
-		doh = ip_vs_dest_conn_overhead(dest);
+		doh = atomic_read(&dest->activeconns) * 50
+			+ atomic_read(&dest->inactconns);
 		if ((loh * atomic_read(&dest->weight) >
 		     doh * atomic_read(&least->weight))
 		    && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
@@ -223,7 +220,7 @@ static inline struct ip_vs_dest *ip_vs_dest_set_min(struct ip_vs_dest_set *set)
 /* get weighted most-connection node in the destination set */
 static inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)
 {
-	register struct ip_vs_dest_set_elem *e;
+	register struct ip_vs_dest_list *e;
 	struct ip_vs_dest *dest, *most;
 	int moh, doh;
 
@@ -231,10 +228,11 @@ static inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)
 		return NULL;
 
 	/* select the first destination server, whose weight > 0 */
-	list_for_each_entry(e, &set->list, list) {
-		most = rcu_dereference_protected(e->dest, 1);
+	for (e=set->list; e!=NULL; e=e->next) {
+		most = e->dest;
 		if (atomic_read(&most->weight) > 0) {
-			moh = ip_vs_dest_conn_overhead(most);
+			moh = atomic_read(&most->activeconns) * 50
+				+ atomic_read(&most->inactconns);
 			goto nextstage;
 		}
 	}
@@ -242,9 +240,10 @@ static inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)
 
 	/* find the destination with the weighted most load */
   nextstage:
-	list_for_each_entry_continue(e, &set->list, list) {
-		dest = rcu_dereference_protected(e->dest, 1);
-		doh = ip_vs_dest_conn_overhead(dest);
+	for (e=e->next; e!=NULL; e=e->next) {
+		dest = e->dest;
+		doh = atomic_read(&dest->activeconns) * 50
+			+ atomic_read(&dest->inactconns);
 		/* moh/mw < doh/dw ==> moh*dw < doh*mw, where mw,dw>0 */
 		if ((moh * atomic_read(&dest->weight) <
 		     doh * atomic_read(&most->weight))
@@ -270,12 +269,11 @@ static inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)
  *      IP address and its destination server set
  */
 struct ip_vs_lblcr_entry {
-	struct hlist_node       list;
+	struct list_head        list;
 	int			af;		/* address family */
 	union nf_inet_addr      addr;           /* destination IP address */
 	struct ip_vs_dest_set   set;            /* destination server set */
 	unsigned long           lastuse;        /* last used time */
-	struct rcu_head		rcu_head;
 };
 
 
@@ -283,46 +281,44 @@ struct ip_vs_lblcr_entry {
  *      IPVS lblcr hash table
  */
 struct ip_vs_lblcr_table {
-	struct rcu_head		rcu_head;
-	struct hlist_head	bucket[IP_VS_LBLCR_TAB_SIZE];  /* hash bucket */
+	struct list_head        bucket[IP_VS_LBLCR_TAB_SIZE];  /* hash bucket */
 	atomic_t                entries;        /* number of entries */
 	int                     max_size;       /* maximum size of entries */
 	struct timer_list       periodic_timer; /* collect stale entries */
 	int                     rover;          /* rover for expire check */
 	int                     counter;        /* counter for no expire */
-	bool			dead;
 };
 
 
-#ifdef CONFIG_SYSCTL
 /*
  *      IPVS LBLCR sysctl table
  */
 
-static struct ctl_table vs_vars_table[] = {
+static ctl_table vs_vars_table[] = {
 	{
 		.procname	= "lblcr_expiration",
-		.data		= NULL,
+		.data		= &sysctl_ip_vs_lblcr_expiration,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec_jiffies,
 	},
-	{ }
+	{}
 };
-#endif
+
+static struct ctl_table_header * sysctl_header;
 
 static inline void ip_vs_lblcr_free(struct ip_vs_lblcr_entry *en)
 {
-	hlist_del_rcu(&en->list);
+	list_del(&en->list);
 	ip_vs_dest_set_eraseall(&en->set);
-	kfree_rcu(en, rcu_head);
+	kfree(en);
 }
 
 
 /*
  *	Returns hash value for IPVS LBLCR entry
  */
-static inline unsigned int
+static inline unsigned
 ip_vs_lblcr_hashkey(int af, const union nf_inet_addr *addr)
 {
 	__be32 addr_fold = addr->ip;
@@ -343,22 +339,25 @@ ip_vs_lblcr_hashkey(int af, const union nf_inet_addr *addr)
 static void
 ip_vs_lblcr_hash(struct ip_vs_lblcr_table *tbl, struct ip_vs_lblcr_entry *en)
 {
-	unsigned int hash = ip_vs_lblcr_hashkey(en->af, &en->addr);
+	unsigned hash = ip_vs_lblcr_hashkey(en->af, &en->addr);
 
-	hlist_add_head_rcu(&en->list, &tbl->bucket[hash]);
+	list_add(&en->list, &tbl->bucket[hash]);
 	atomic_inc(&tbl->entries);
 }
 
 
-/* Get ip_vs_lblcr_entry associated with supplied parameters. */
+/*
+ *  Get ip_vs_lblcr_entry associated with supplied parameters. Called under
+ *  read lock.
+ */
 static inline struct ip_vs_lblcr_entry *
 ip_vs_lblcr_get(int af, struct ip_vs_lblcr_table *tbl,
 		const union nf_inet_addr *addr)
 {
-	unsigned int hash = ip_vs_lblcr_hashkey(af, addr);
+	unsigned hash = ip_vs_lblcr_hashkey(af, addr);
 	struct ip_vs_lblcr_entry *en;
 
-	hlist_for_each_entry_rcu(en, &tbl->bucket[hash], list)
+	list_for_each_entry(en, &tbl->bucket[hash], list)
 		if (ip_vs_addr_equal(af, &en->addr, addr))
 			return en;
 
@@ -368,7 +367,7 @@ ip_vs_lblcr_get(int af, struct ip_vs_lblcr_table *tbl,
 
 /*
  * Create or update an ip_vs_lblcr_entry, which is a mapping of a destination
- * IP address to a server. Called under spin lock.
+ * IP address to a server. Called under write lock.
  */
 static inline struct ip_vs_lblcr_entry *
 ip_vs_lblcr_new(struct ip_vs_lblcr_table *tbl, const union nf_inet_addr *daddr,
@@ -379,24 +378,26 @@ ip_vs_lblcr_new(struct ip_vs_lblcr_table *tbl, const union nf_inet_addr *daddr,
 	en = ip_vs_lblcr_get(dest->af, tbl, daddr);
 	if (!en) {
 		en = kmalloc(sizeof(*en), GFP_ATOMIC);
-		if (!en)
+		if (!en) {
+			pr_err("%s(): no memory\n", __func__);
 			return NULL;
+		}
 
 		en->af = dest->af;
 		ip_vs_addr_copy(dest->af, &en->addr, daddr);
 		en->lastuse = jiffies;
 
-		/* initialize its dest set */
+		/* initilize its dest set */
 		atomic_set(&(en->set.size), 0);
-		INIT_LIST_HEAD(&en->set.list);
-
-		ip_vs_dest_set_insert(&en->set, dest, false);
+		en->set.list = NULL;
+		rwlock_init(&en->set.lock);
 
 		ip_vs_lblcr_hash(tbl, en);
-		return en;
 	}
 
-	ip_vs_dest_set_insert(&en->set, dest, true);
+	write_lock(&en->set.lock);
+	ip_vs_dest_set_insert(&en->set, dest);
+	write_unlock(&en->set.lock);
 
 	return en;
 }
@@ -405,54 +406,40 @@ ip_vs_lblcr_new(struct ip_vs_lblcr_table *tbl, const union nf_inet_addr *daddr,
 /*
  *      Flush all the entries of the specified table.
  */
-static void ip_vs_lblcr_flush(struct ip_vs_service *svc)
+static void ip_vs_lblcr_flush(struct ip_vs_lblcr_table *tbl)
 {
-	struct ip_vs_lblcr_table *tbl = svc->sched_data;
 	int i;
-	struct ip_vs_lblcr_entry *en;
-	struct hlist_node *next;
+	struct ip_vs_lblcr_entry *en, *nxt;
 
-	spin_lock_bh(&svc->sched_lock);
-	tbl->dead = 1;
+	/* No locking required, only called during cleanup. */
 	for (i=0; i<IP_VS_LBLCR_TAB_SIZE; i++) {
-		hlist_for_each_entry_safe(en, next, &tbl->bucket[i], list) {
+		list_for_each_entry_safe(en, nxt, &tbl->bucket[i], list) {
 			ip_vs_lblcr_free(en);
 		}
 	}
-	spin_unlock_bh(&svc->sched_lock);
 }
 
-static int sysctl_lblcr_expiration(struct ip_vs_service *svc)
-{
-#ifdef CONFIG_SYSCTL
-	struct netns_ipvs *ipvs = net_ipvs(svc->net);
-	return ipvs->sysctl_lblcr_expiration;
-#else
-	return DEFAULT_EXPIRATION;
-#endif
-}
 
 static inline void ip_vs_lblcr_full_check(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
 	unsigned long now = jiffies;
 	int i, j;
-	struct ip_vs_lblcr_entry *en;
-	struct hlist_node *next;
+	struct ip_vs_lblcr_entry *en, *nxt;
 
 	for (i=0, j=tbl->rover; i<IP_VS_LBLCR_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLCR_TAB_MASK;
 
-		spin_lock(&svc->sched_lock);
-		hlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {
-			if (time_after(en->lastuse +
-				       sysctl_lblcr_expiration(svc), now))
+		write_lock(&svc->sched_lock);
+		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
+			if (time_after(en->lastuse+sysctl_ip_vs_lblcr_expiration,
+				       now))
 				continue;
 
 			ip_vs_lblcr_free(en);
 			atomic_dec(&tbl->entries);
 		}
-		spin_unlock(&svc->sched_lock);
+		write_unlock(&svc->sched_lock);
 	}
 	tbl->rover = j;
 }
@@ -476,8 +463,7 @@ static void ip_vs_lblcr_check_expire(unsigned long data)
 	unsigned long now = jiffies;
 	int goal;
 	int i, j;
-	struct ip_vs_lblcr_entry *en;
-	struct hlist_node *next;
+	struct ip_vs_lblcr_entry *en, *nxt;
 
 	if ((tbl->counter % COUNT_FOR_FULL_EXPIRATION) == 0) {
 		/* do full expiration check */
@@ -498,8 +484,8 @@ static void ip_vs_lblcr_check_expire(unsigned long data)
 	for (i=0, j=tbl->rover; i<IP_VS_LBLCR_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLCR_TAB_MASK;
 
-		spin_lock(&svc->sched_lock);
-		hlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {
+		write_lock(&svc->sched_lock);
+		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
 			if (time_before(now, en->lastuse+ENTRY_TIMEOUT))
 				continue;
 
@@ -507,7 +493,7 @@ static void ip_vs_lblcr_check_expire(unsigned long data)
 			atomic_dec(&tbl->entries);
 			goal--;
 		}
-		spin_unlock(&svc->sched_lock);
+		write_unlock(&svc->sched_lock);
 		if (goal <= 0)
 			break;
 	}
@@ -525,10 +511,11 @@ static int ip_vs_lblcr_init_svc(struct ip_vs_service *svc)
 	/*
 	 *    Allocate the ip_vs_lblcr_table for this service
 	 */
-	tbl = kmalloc(sizeof(*tbl), GFP_KERNEL);
-	if (tbl == NULL)
+	tbl = kmalloc(sizeof(*tbl), GFP_ATOMIC);
+	if (tbl == NULL) {
+		pr_err("%s(): no memory\n", __func__);
 		return -ENOMEM;
-
+	}
 	svc->sched_data = tbl;
 	IP_VS_DBG(6, "LBLCR hash table (memory=%Zdbytes) allocated for "
 		  "current service\n", sizeof(*tbl));
@@ -537,12 +524,11 @@ static int ip_vs_lblcr_init_svc(struct ip_vs_service *svc)
 	 *    Initialize the hash buckets
 	 */
 	for (i=0; i<IP_VS_LBLCR_TAB_SIZE; i++) {
-		INIT_HLIST_HEAD(&tbl->bucket[i]);
+		INIT_LIST_HEAD(&tbl->bucket[i]);
 	}
 	tbl->max_size = IP_VS_LBLCR_TAB_SIZE*16;
 	tbl->rover = 0;
 	tbl->counter = 1;
-	tbl->dead = 0;
 
 	/*
 	 *    Hook periodic timer for garbage collection
@@ -555,7 +541,7 @@ static int ip_vs_lblcr_init_svc(struct ip_vs_service *svc)
 }
 
 
-static void ip_vs_lblcr_done_svc(struct ip_vs_service *svc)
+static int ip_vs_lblcr_done_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
 
@@ -563,12 +549,14 @@ static void ip_vs_lblcr_done_svc(struct ip_vs_service *svc)
 	del_timer_sync(&tbl->periodic_timer);
 
 	/* got to clean up table entries here */
-	ip_vs_lblcr_flush(svc);
+	ip_vs_lblcr_flush(tbl);
 
 	/* release the table itself */
-	kfree_rcu(tbl, rcu_head);
+	kfree(tbl);
 	IP_VS_DBG(6, "LBLCR hash table (memory=%Zdbytes) released\n",
 		  sizeof(*tbl));
+
+	return 0;
 }
 
 
@@ -579,7 +567,12 @@ __ip_vs_lblcr_schedule(struct ip_vs_service *svc)
 	int loh, doh;
 
 	/*
-	 * We use the following formula to estimate the load:
+	 * We think the overhead of processing active connections is fifty
+	 * times higher than that of inactive connections in average. (This
+	 * fifty times might not be accurate, we will change it later.) We
+	 * use the following formula to estimate the overhead:
+	 *                dest->activeconns*50 + dest->inactconns
+	 * and the load:
 	 *                (dest overhead) / dest->weight
 	 *
 	 * Remember -- no floats in kernel mode!!!
@@ -590,13 +583,14 @@ __ip_vs_lblcr_schedule(struct ip_vs_service *svc)
 	 * The server with weight=0 is quiesced and will not receive any
 	 * new connection.
 	 */
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
 		if (atomic_read(&dest->weight) > 0) {
 			least = dest;
-			loh = ip_vs_dest_conn_overhead(least);
+			loh = atomic_read(&least->activeconns) * 50
+				+ atomic_read(&least->inactconns);
 			goto nextstage;
 		}
 	}
@@ -606,11 +600,12 @@ __ip_vs_lblcr_schedule(struct ip_vs_service *svc)
 	 *    Find the destination with the least load.
 	 */
   nextstage:
-	list_for_each_entry_continue_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
-		doh = ip_vs_dest_conn_overhead(dest);
+		doh = atomic_read(&dest->activeconns) * 50
+			+ atomic_read(&dest->inactconns);
 		if (loh * atomic_read(&dest->weight) >
 		    doh * atomic_read(&least->weight)) {
 			least = dest;
@@ -640,7 +635,7 @@ is_overloaded(struct ip_vs_dest *dest, struct ip_vs_service *svc)
 	if (atomic_read(&dest->activeconns) > atomic_read(&dest->weight)) {
 		struct ip_vs_dest *d;
 
-		list_for_each_entry_rcu(d, &svc->destinations, n_list) {
+		list_for_each_entry(d, &svc->destinations, n_list) {
 			if (atomic_read(&d->activeconns)*2
 			    < atomic_read(&d->weight)) {
 				return 1;
@@ -659,54 +654,61 @@ ip_vs_lblcr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 {
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
 	struct ip_vs_iphdr iph;
-	struct ip_vs_dest *dest;
+	struct ip_vs_dest *dest = NULL;
 	struct ip_vs_lblcr_entry *en;
 
-	ip_vs_fill_iph_addr_only(svc->af, skb, &iph);
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
 
 	IP_VS_DBG(6, "%s(): Scheduling...\n", __func__);
 
 	/* First look in our cache */
+	read_lock(&svc->sched_lock);
 	en = ip_vs_lblcr_get(svc->af, tbl, &iph.daddr);
 	if (en) {
+		/* We only hold a read lock, but this is atomic */
 		en->lastuse = jiffies;
 
 		/* Get the least loaded destination */
+		read_lock(&en->set.lock);
 		dest = ip_vs_dest_set_min(&en->set);
+		read_unlock(&en->set.lock);
 
 		/* More than one destination + enough time passed by, cleanup */
 		if (atomic_read(&en->set.size) > 1 &&
-		    time_after(jiffies, en->set.lastmod +
-				sysctl_lblcr_expiration(svc))) {
-			spin_lock_bh(&svc->sched_lock);
-			if (atomic_read(&en->set.size) > 1) {
-				struct ip_vs_dest *m;
-
-				m = ip_vs_dest_set_max(&en->set);
-				if (m)
-					ip_vs_dest_set_erase(&en->set, m);
-			}
-			spin_unlock_bh(&svc->sched_lock);
+				time_after(jiffies, en->set.lastmod +
+				sysctl_ip_vs_lblcr_expiration)) {
+			struct ip_vs_dest *m;
+
+			write_lock(&en->set.lock);
+			m = ip_vs_dest_set_max(&en->set);
+			if (m)
+				ip_vs_dest_set_erase(&en->set, m);
+			write_unlock(&en->set.lock);
 		}
 
 		/* If the destination is not overloaded, use it */
-		if (dest && !is_overloaded(dest, svc))
+		if (dest && !is_overloaded(dest, svc)) {
+			read_unlock(&svc->sched_lock);
 			goto out;
+		}
 
 		/* The cache entry is invalid, time to schedule */
 		dest = __ip_vs_lblcr_schedule(svc);
 		if (!dest) {
-			ip_vs_scheduler_err(svc, "no destination available");
+			IP_VS_ERR_RL("LBLCR: no destination available\n");
+			read_unlock(&svc->sched_lock);
 			return NULL;
 		}
 
 		/* Update our cache entry */
-		spin_lock_bh(&svc->sched_lock);
-		if (!tbl->dead)
-			ip_vs_dest_set_insert(&en->set, dest, true);
-		spin_unlock_bh(&svc->sched_lock);
-		goto out;
+		write_lock(&en->set.lock);
+		ip_vs_dest_set_insert(&en->set, dest);
+		write_unlock(&en->set.lock);
 	}
+	read_unlock(&svc->sched_lock);
+
+	if (dest)
+		goto out;
 
 	/* No cache entry, time to schedule */
 	dest = __ip_vs_lblcr_schedule(svc);
@@ -716,10 +718,9 @@ ip_vs_lblcr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	}
 
 	/* If we fail to create a cache entry, we'll just use the valid dest */
-	spin_lock_bh(&svc->sched_lock);
-	if (!tbl->dead)
-		ip_vs_lblcr_new(tbl, &iph.daddr, dest);
-	spin_unlock_bh(&svc->sched_lock);
+	write_lock(&svc->sched_lock);
+	ip_vs_lblcr_new(tbl, &iph.daddr, dest);
+	write_unlock(&svc->sched_lock);
 
 out:
 	IP_VS_DBG_BUF(6, "LBLCR: destination IP address %s --> server %s:%d\n",
@@ -744,84 +745,23 @@ static struct ip_vs_scheduler ip_vs_lblcr_scheduler =
 	.schedule =		ip_vs_lblcr_schedule,
 };
 
-/*
- *  per netns init.
- */
-#ifdef CONFIG_SYSCTL
-static int __net_init __ip_vs_lblcr_init(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	if (!ipvs)
-		return -ENOENT;
-
-	if (!net_eq(net, &init_net)) {
-		ipvs->lblcr_ctl_table = kmemdup(vs_vars_table,
-						sizeof(vs_vars_table),
-						GFP_KERNEL);
-		if (ipvs->lblcr_ctl_table == NULL)
-			return -ENOMEM;
-
-		/* Don't export sysctls to unprivileged users */
-		if (net->user_ns != &init_user_ns)
-			ipvs->lblcr_ctl_table[0].procname = NULL;
-	} else
-		ipvs->lblcr_ctl_table = vs_vars_table;
-	ipvs->sysctl_lblcr_expiration = DEFAULT_EXPIRATION;
-	ipvs->lblcr_ctl_table[0].data = &ipvs->sysctl_lblcr_expiration;
-
-	ipvs->lblcr_ctl_header =
-		register_net_sysctl(net, "net/ipv4/vs", ipvs->lblcr_ctl_table);
-	if (!ipvs->lblcr_ctl_header) {
-		if (!net_eq(net, &init_net))
-			kfree(ipvs->lblcr_ctl_table);
-		return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static void __net_exit __ip_vs_lblcr_exit(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	unregister_net_sysctl_table(ipvs->lblcr_ctl_header);
-
-	if (!net_eq(net, &init_net))
-		kfree(ipvs->lblcr_ctl_table);
-}
-
-#else
-
-static int __net_init __ip_vs_lblcr_init(struct net *net) { return 0; }
-static void __net_exit __ip_vs_lblcr_exit(struct net *net) { }
-
-#endif
-
-static struct pernet_operations ip_vs_lblcr_ops = {
-	.init = __ip_vs_lblcr_init,
-	.exit = __ip_vs_lblcr_exit,
-};
 
 static int __init ip_vs_lblcr_init(void)
 {
 	int ret;
 
-	ret = register_pernet_subsys(&ip_vs_lblcr_ops);
-	if (ret)
-		return ret;
-
+	sysctl_header = register_net_sysctl(&init_net, "net/ipv4/vs", vs_vars_table);
 	ret = register_ip_vs_scheduler(&ip_vs_lblcr_scheduler);
 	if (ret)
-		unregister_pernet_subsys(&ip_vs_lblcr_ops);
+		unregister_sysctl_table(sysctl_header);
 	return ret;
 }
 
+
 static void __exit ip_vs_lblcr_cleanup(void)
 {
+	unregister_sysctl_table(sysctl_header);
 	unregister_ip_vs_scheduler(&ip_vs_lblcr_scheduler);
-	unregister_pernet_subsys(&ip_vs_lblcr_ops);
-	synchronize_rcu();
 }
 
 
diff --git a/net/netfilter/ipvs/ip_vs_lc.c b/net/netfilter/ipvs/ip_vs_lc.c
index 5128e33..4f69db1 100644
--- a/net/netfilter/ipvs/ip_vs_lc.c
+++ b/net/netfilter/ipvs/ip_vs_lc.c
@@ -22,6 +22,22 @@
 
 #include <net/ip_vs.h>
 
+
+static inline unsigned int
+ip_vs_lc_dest_overhead(struct ip_vs_dest *dest)
+{
+	/*
+	 * We think the overhead of processing active connections is 256
+	 * times higher than that of inactive connections in average. (This
+	 * 256 times might not be accurate, we will change it later) We
+	 * use the following formula to estimate the overhead now:
+	 *		  dest->activeconns*256 + dest->inactconns
+	 */
+	return (atomic_read(&dest->activeconns) << 8) +
+		atomic_read(&dest->inactconns);
+}
+
+
 /*
  *	Least Connection scheduling
  */
@@ -42,11 +58,11 @@ ip_vs_lc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	 * served, but no new connection is assigned to the server.
 	 */
 
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 		if ((dest->flags & IP_VS_DEST_F_OVERLOAD) ||
 		    atomic_read(&dest->weight) == 0)
 			continue;
-		doh = ip_vs_dest_conn_overhead(dest);
+		doh = ip_vs_lc_dest_overhead(dest);
 		if (!least || doh < loh) {
 			least = dest;
 			loh = doh;
@@ -54,7 +70,7 @@ ip_vs_lc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	}
 
 	if (!least)
-		ip_vs_scheduler_err(svc, "no destination available");
+		IP_VS_ERR_RL("LC: no destination available\n");
 	else
 		IP_VS_DBG_BUF(6, "LC: server %s:%u activeconns %d "
 			      "inactconns %d\n",
@@ -84,7 +100,6 @@ static int __init ip_vs_lc_init(void)
 static void __exit ip_vs_lc_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_lc_scheduler);
-	synchronize_rcu();
 }
 
 module_init(ip_vs_lc_init);
diff --git a/net/netfilter/ipvs/ip_vs_nfct.c b/net/netfilter/ipvs/ip_vs_nfct.c
deleted file mode 100644
index 5a355a4..0000000
--- a/net/netfilter/ipvs/ip_vs_nfct.c
+++ /dev/null
@@ -1,300 +0,0 @@
-/*
- * ip_vs_nfct.c:	Netfilter connection tracking support for IPVS
- *
- * Portions Copyright (C) 2001-2002
- * Antefacto Ltd, 181 Parnell St, Dublin 1, Ireland.
- *
- * Portions Copyright (C) 2003-2010
- * Julian Anastasov
- *
- *
- * This code is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
- *
- *
- * Authors:
- * Ben North <ben@redfrontdoor.org>
- * Julian Anastasov <ja@ssi.bg>		Reorganize and sync with latest kernels
- * Hannes Eder <heder@google.com>	Extend NFCT support for FTP, ipvs match
- *
- *
- * Current status:
- *
- * - provide conntrack confirmation for new and related connections, by
- * this way we can see their proper conntrack state in all hooks
- * - support for all forwarding methods, not only NAT
- * - FTP support (NAT), ability to support other NAT apps with expectations
- * - to correctly create expectations for related NAT connections the proper
- * NF conntrack support must be already installed, eg. ip_vs_ftp requires
- * nf_conntrack_ftp ... iptables_nat for the same ports (but no iptables
- * NAT rules are needed)
- * - alter reply for NAT when forwarding packet in original direction:
- * conntrack from client in NEW or RELATED (Passive FTP DATA) state or
- * when RELATED conntrack is created from real server (Active FTP DATA)
- * - if iptables_nat is not loaded the Passive FTP will not work (the
- * PASV response can not be NAT-ed) but Active FTP should work
- *
- */
-
-#define KMSG_COMPONENT "IPVS"
-#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
-
-#include <linux/module.h>
-#include <linux/types.h>
-#include <linux/kernel.h>
-#include <linux/errno.h>
-#include <linux/compiler.h>
-#include <linux/vmalloc.h>
-#include <linux/skbuff.h>
-#include <net/ip.h>
-#include <linux/netfilter.h>
-#include <linux/netfilter_ipv4.h>
-#include <net/ip_vs.h>
-#include <net/netfilter/nf_conntrack_core.h>
-#include <net/netfilter/nf_conntrack_expect.h>
-#include <net/netfilter/nf_conntrack_seqadj.h>
-#include <net/netfilter/nf_conntrack_helper.h>
-#include <net/netfilter/nf_conntrack_zones.h>
-
-
-#define FMT_TUPLE	"%pI4:%u->%pI4:%u/%u"
-#define ARG_TUPLE(T)	&(T)->src.u3.ip, ntohs((T)->src.u.all), \
-			&(T)->dst.u3.ip, ntohs((T)->dst.u.all), \
-			(T)->dst.protonum
-
-#define FMT_CONN	"%pI4:%u->%pI4:%u->%pI4:%u/%u:%u"
-#define ARG_CONN(C)	&((C)->caddr.ip), ntohs((C)->cport), \
-			&((C)->vaddr.ip), ntohs((C)->vport), \
-			&((C)->daddr.ip), ntohs((C)->dport), \
-			(C)->protocol, (C)->state
-
-void
-ip_vs_update_conntrack(struct sk_buff *skb, struct ip_vs_conn *cp, int outin)
-{
-	enum ip_conntrack_info ctinfo;
-	struct nf_conn *ct = nf_ct_get(skb, &ctinfo);
-	struct nf_conntrack_tuple new_tuple;
-
-	if (ct == NULL || nf_ct_is_confirmed(ct) || nf_ct_is_untracked(ct) ||
-	    nf_ct_is_dying(ct))
-		return;
-
-	/* Never alter conntrack for non-NAT conns */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)
-		return;
-
-	/* Alter reply only in original direction */
-	if (CTINFO2DIR(ctinfo) != IP_CT_DIR_ORIGINAL)
-		return;
-
-	/* Applications may adjust TCP seqs */
-	if (cp->app && nf_ct_protonum(ct) == IPPROTO_TCP &&
-	    !nfct_seqadj(ct) && !nfct_seqadj_ext_add(ct))
-		return;
-
-	/*
-	 * The connection is not yet in the hashtable, so we update it.
-	 * CIP->VIP will remain the same, so leave the tuple in
-	 * IP_CT_DIR_ORIGINAL untouched.  When the reply comes back from the
-	 * real-server we will see RIP->DIP.
-	 */
-	new_tuple = ct->tuplehash[IP_CT_DIR_REPLY].tuple;
-	/*
-	 * This will also take care of UDP and other protocols.
-	 */
-	if (outin) {
-		new_tuple.src.u3 = cp->daddr;
-		if (new_tuple.dst.protonum != IPPROTO_ICMP &&
-		    new_tuple.dst.protonum != IPPROTO_ICMPV6)
-			new_tuple.src.u.tcp.port = cp->dport;
-	} else {
-		new_tuple.dst.u3 = cp->vaddr;
-		if (new_tuple.dst.protonum != IPPROTO_ICMP &&
-		    new_tuple.dst.protonum != IPPROTO_ICMPV6)
-			new_tuple.dst.u.tcp.port = cp->vport;
-	}
-	IP_VS_DBG(7, "%s: Updating conntrack ct=%p, status=0x%lX, "
-		  "ctinfo=%d, old reply=" FMT_TUPLE
-		  ", new reply=" FMT_TUPLE ", cp=" FMT_CONN "\n",
-		  __func__, ct, ct->status, ctinfo,
-		  ARG_TUPLE(&ct->tuplehash[IP_CT_DIR_REPLY].tuple),
-		  ARG_TUPLE(&new_tuple), ARG_CONN(cp));
-	nf_conntrack_alter_reply(ct, &new_tuple);
-}
-
-int ip_vs_confirm_conntrack(struct sk_buff *skb)
-{
-	return nf_conntrack_confirm(skb);
-}
-
-/*
- * Called from init_conntrack() as expectfn handler.
- */
-static void ip_vs_nfct_expect_callback(struct nf_conn *ct,
-	struct nf_conntrack_expect *exp)
-{
-	struct nf_conntrack_tuple *orig, new_reply;
-	struct ip_vs_conn *cp;
-	struct ip_vs_conn_param p;
-	struct net *net = nf_ct_net(ct);
-
-	if (exp->tuple.src.l3num != PF_INET)
-		return;
-
-	/*
-	 * We assume that no NF locks are held before this callback.
-	 * ip_vs_conn_out_get and ip_vs_conn_in_get should match their
-	 * expectations even if they use wildcard values, now we provide the
-	 * actual values from the newly created original conntrack direction.
-	 * The conntrack is confirmed when packet reaches IPVS hooks.
-	 */
-
-	/* RS->CLIENT */
-	orig = &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple;
-	ip_vs_conn_fill_param(net, exp->tuple.src.l3num, orig->dst.protonum,
-			      &orig->src.u3, orig->src.u.tcp.port,
-			      &orig->dst.u3, orig->dst.u.tcp.port, &p);
-	cp = ip_vs_conn_out_get(&p);
-	if (cp) {
-		/* Change reply CLIENT->RS to CLIENT->VS */
-		new_reply = ct->tuplehash[IP_CT_DIR_REPLY].tuple;
-		IP_VS_DBG(7, "%s: ct=%p, status=0x%lX, tuples=" FMT_TUPLE ", "
-			  FMT_TUPLE ", found inout cp=" FMT_CONN "\n",
-			  __func__, ct, ct->status,
-			  ARG_TUPLE(orig), ARG_TUPLE(&new_reply),
-			  ARG_CONN(cp));
-		new_reply.dst.u3 = cp->vaddr;
-		new_reply.dst.u.tcp.port = cp->vport;
-		IP_VS_DBG(7, "%s: ct=%p, new tuples=" FMT_TUPLE ", " FMT_TUPLE
-			  ", inout cp=" FMT_CONN "\n",
-			  __func__, ct,
-			  ARG_TUPLE(orig), ARG_TUPLE(&new_reply),
-			  ARG_CONN(cp));
-		goto alter;
-	}
-
-	/* CLIENT->VS */
-	cp = ip_vs_conn_in_get(&p);
-	if (cp) {
-		/* Change reply VS->CLIENT to RS->CLIENT */
-		new_reply = ct->tuplehash[IP_CT_DIR_REPLY].tuple;
-		IP_VS_DBG(7, "%s: ct=%p, status=0x%lX, tuples=" FMT_TUPLE ", "
-			  FMT_TUPLE ", found outin cp=" FMT_CONN "\n",
-			  __func__, ct, ct->status,
-			  ARG_TUPLE(orig), ARG_TUPLE(&new_reply),
-			  ARG_CONN(cp));
-		new_reply.src.u3 = cp->daddr;
-		new_reply.src.u.tcp.port = cp->dport;
-		IP_VS_DBG(7, "%s: ct=%p, new tuples=" FMT_TUPLE ", "
-			  FMT_TUPLE ", outin cp=" FMT_CONN "\n",
-			  __func__, ct,
-			  ARG_TUPLE(orig), ARG_TUPLE(&new_reply),
-			  ARG_CONN(cp));
-		goto alter;
-	}
-
-	IP_VS_DBG(7, "%s: ct=%p, status=0x%lX, tuple=" FMT_TUPLE
-		  " - unknown expect\n",
-		  __func__, ct, ct->status, ARG_TUPLE(orig));
-	return;
-
-alter:
-	/* Never alter conntrack for non-NAT conns */
-	if (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_MASQ)
-		nf_conntrack_alter_reply(ct, &new_reply);
-	ip_vs_conn_put(cp);
-	return;
-}
-
-/*
- * Create NF conntrack expectation with wildcard (optional) source port.
- * Then the default callback function will alter the reply and will confirm
- * the conntrack entry when the first packet comes.
- * Use port 0 to expect connection from any port.
- */
-void ip_vs_nfct_expect_related(struct sk_buff *skb, struct nf_conn *ct,
-			       struct ip_vs_conn *cp, u_int8_t proto,
-			       const __be16 port, int from_rs)
-{
-	struct nf_conntrack_expect *exp;
-
-	if (ct == NULL || nf_ct_is_untracked(ct))
-		return;
-
-	exp = nf_ct_expect_alloc(ct);
-	if (!exp)
-		return;
-
-	nf_ct_expect_init(exp, NF_CT_EXPECT_CLASS_DEFAULT, nf_ct_l3num(ct),
-			from_rs ? &cp->daddr : &cp->caddr,
-			from_rs ? &cp->caddr : &cp->vaddr,
-			proto, port ? &port : NULL,
-			from_rs ? &cp->cport : &cp->vport);
-
-	exp->expectfn = ip_vs_nfct_expect_callback;
-
-	IP_VS_DBG(7, "%s: ct=%p, expect tuple=" FMT_TUPLE "\n",
-		__func__, ct, ARG_TUPLE(&exp->tuple));
-	nf_ct_expect_related(exp);
-	nf_ct_expect_put(exp);
-}
-EXPORT_SYMBOL(ip_vs_nfct_expect_related);
-
-/*
- * Our connection was terminated, try to drop the conntrack immediately
- */
-void ip_vs_conn_drop_conntrack(struct ip_vs_conn *cp)
-{
-	struct nf_conntrack_tuple_hash *h;
-	struct nf_conn *ct;
-	struct nf_conntrack_tuple tuple;
-
-	if (!cp->cport)
-		return;
-
-	tuple = (struct nf_conntrack_tuple) {
-		.dst = { .protonum = cp->protocol, .dir = IP_CT_DIR_ORIGINAL } };
-	tuple.src.u3 = cp->caddr;
-	tuple.src.u.all = cp->cport;
-	tuple.src.l3num = cp->af;
-	tuple.dst.u3 = cp->vaddr;
-	tuple.dst.u.all = cp->vport;
-
-	IP_VS_DBG(7, "%s: dropping conntrack with tuple=" FMT_TUPLE
-		" for conn " FMT_CONN "\n",
-		__func__, ARG_TUPLE(&tuple), ARG_CONN(cp));
-
-	h = nf_conntrack_find_get(ip_vs_conn_net(cp), NF_CT_DEFAULT_ZONE,
-				  &tuple);
-	if (h) {
-		ct = nf_ct_tuplehash_to_ctrack(h);
-		/* Show what happens instead of calling nf_ct_kill() */
-		if (del_timer(&ct->timeout)) {
-			IP_VS_DBG(7, "%s: ct=%p, deleted conntrack timer for tuple="
-				FMT_TUPLE "\n",
-				__func__, ct, ARG_TUPLE(&tuple));
-			if (ct->timeout.function)
-				ct->timeout.function(ct->timeout.data);
-		} else {
-			IP_VS_DBG(7, "%s: ct=%p, no conntrack timer for tuple="
-				FMT_TUPLE "\n",
-				__func__, ct, ARG_TUPLE(&tuple));
-		}
-		nf_ct_put(ct);
-	} else {
-		IP_VS_DBG(7, "%s: no conntrack for tuple=" FMT_TUPLE "\n",
-			__func__, ARG_TUPLE(&tuple));
-	}
-}
-
diff --git a/net/netfilter/ipvs/ip_vs_nq.c b/net/netfilter/ipvs/ip_vs_nq.c
index 646cfd4..c413e18 100644
--- a/net/netfilter/ipvs/ip_vs_nq.c
+++ b/net/netfilter/ipvs/ip_vs_nq.c
@@ -75,7 +75,7 @@ ip_vs_nq_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	 * new connections.
 	 */
 
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD ||
 		    !atomic_read(&dest->weight))
@@ -99,7 +99,7 @@ ip_vs_nq_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	}
 
 	if (!least) {
-		ip_vs_scheduler_err(svc, "no destination available");
+		IP_VS_ERR_RL("NQ: no destination available\n");
 		return NULL;
 	}
 
@@ -133,7 +133,6 @@ static int __init ip_vs_nq_init(void)
 static void __exit ip_vs_nq_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_nq_scheduler);
-	synchronize_rcu();
 }
 
 module_init(ip_vs_nq_init);
diff --git a/net/netfilter/ipvs/ip_vs_pe.c b/net/netfilter/ipvs/ip_vs_pe.c
deleted file mode 100644
index 1a82b29..0000000
--- a/net/netfilter/ipvs/ip_vs_pe.c
+++ /dev/null
@@ -1,111 +0,0 @@
-#define KMSG_COMPONENT "IPVS"
-#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
-
-#include <linux/module.h>
-#include <linux/spinlock.h>
-#include <linux/interrupt.h>
-#include <asm/string.h>
-#include <linux/kmod.h>
-#include <linux/sysctl.h>
-
-#include <net/ip_vs.h>
-
-/* IPVS pe list */
-static LIST_HEAD(ip_vs_pe);
-
-/* semaphore for IPVS PEs. */
-static DEFINE_MUTEX(ip_vs_pe_mutex);
-
-/* Get pe in the pe list by name */
-struct ip_vs_pe *__ip_vs_pe_getbyname(const char *pe_name)
-{
-	struct ip_vs_pe *pe;
-
-	IP_VS_DBG(10, "%s(): pe_name \"%s\"\n", __func__,
-		  pe_name);
-
-	rcu_read_lock();
-	list_for_each_entry_rcu(pe, &ip_vs_pe, n_list) {
-		/* Test and get the modules atomically */
-		if (pe->module &&
-		    !try_module_get(pe->module)) {
-			/* This pe is just deleted */
-			continue;
-		}
-		if (strcmp(pe_name, pe->name)==0) {
-			/* HIT */
-			rcu_read_unlock();
-			return pe;
-		}
-		if (pe->module)
-			module_put(pe->module);
-	}
-	rcu_read_unlock();
-
-	return NULL;
-}
-
-/* Lookup pe and try to load it if it doesn't exist */
-struct ip_vs_pe *ip_vs_pe_getbyname(const char *name)
-{
-	struct ip_vs_pe *pe;
-
-	/* Search for the pe by name */
-	pe = __ip_vs_pe_getbyname(name);
-
-	/* If pe not found, load the module and search again */
-	if (!pe) {
-		request_module("ip_vs_pe_%s", name);
-		pe = __ip_vs_pe_getbyname(name);
-	}
-
-	return pe;
-}
-
-/* Register a pe in the pe list */
-int register_ip_vs_pe(struct ip_vs_pe *pe)
-{
-	struct ip_vs_pe *tmp;
-
-	/* increase the module use count */
-	ip_vs_use_count_inc();
-
-	mutex_lock(&ip_vs_pe_mutex);
-	/* Make sure that the pe with this name doesn't exist
-	 * in the pe list.
-	 */
-	list_for_each_entry(tmp, &ip_vs_pe, n_list) {
-		if (strcmp(tmp->name, pe->name) == 0) {
-			mutex_unlock(&ip_vs_pe_mutex);
-			ip_vs_use_count_dec();
-			pr_err("%s(): [%s] pe already existed "
-			       "in the system\n", __func__, pe->name);
-			return -EINVAL;
-		}
-	}
-	/* Add it into the d-linked pe list */
-	list_add_rcu(&pe->n_list, &ip_vs_pe);
-	mutex_unlock(&ip_vs_pe_mutex);
-
-	pr_info("[%s] pe registered.\n", pe->name);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(register_ip_vs_pe);
-
-/* Unregister a pe from the pe list */
-int unregister_ip_vs_pe(struct ip_vs_pe *pe)
-{
-	mutex_lock(&ip_vs_pe_mutex);
-	/* Remove it from the d-linked pe list */
-	list_del_rcu(&pe->n_list);
-	mutex_unlock(&ip_vs_pe_mutex);
-
-	/* decrease the module use count */
-	ip_vs_use_count_dec();
-
-	pr_info("[%s] pe unregistered.\n", pe->name);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(unregister_ip_vs_pe);
diff --git a/net/netfilter/ipvs/ip_vs_pe_sip.c b/net/netfilter/ipvs/ip_vs_pe_sip.c
deleted file mode 100644
index bed5f70..0000000
--- a/net/netfilter/ipvs/ip_vs_pe_sip.c
+++ /dev/null
@@ -1,171 +0,0 @@
-#define KMSG_COMPONENT "IPVS"
-#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
-
-#include <linux/module.h>
-#include <linux/kernel.h>
-
-#include <net/ip_vs.h>
-#include <net/netfilter/nf_conntrack.h>
-#include <linux/netfilter/nf_conntrack_sip.h>
-
-#ifdef CONFIG_IP_VS_DEBUG
-static const char *ip_vs_dbg_callid(char *buf, size_t buf_len,
-				    const char *callid, size_t callid_len,
-				    int *idx)
-{
-	size_t max_len = 64;
-	size_t len = min3(max_len, callid_len, buf_len - *idx - 1);
-	memcpy(buf + *idx, callid, len);
-	buf[*idx+len] = '\0';
-	*idx += len + 1;
-	return buf + *idx - len;
-}
-
-#define IP_VS_DEBUG_CALLID(callid, len)					\
-	ip_vs_dbg_callid(ip_vs_dbg_buf, sizeof(ip_vs_dbg_buf),		\
-			 callid, len, &ip_vs_dbg_idx)
-#endif
-
-static int get_callid(const char *dptr, unsigned int dataoff,
-		      unsigned int datalen,
-		      unsigned int *matchoff, unsigned int *matchlen)
-{
-	/* Find callid */
-	while (1) {
-		int ret = ct_sip_get_header(NULL, dptr, dataoff, datalen,
-					    SIP_HDR_CALL_ID, matchoff,
-					    matchlen);
-		if (ret > 0)
-			break;
-		if (!ret)
-			return -EINVAL;
-		dataoff += *matchoff;
-	}
-
-	/* Too large is useless */
-	if (*matchlen > IP_VS_PEDATA_MAXLEN)
-		return -EINVAL;
-
-	/* SIP headers are always followed by a line terminator */
-	if (*matchoff + *matchlen == datalen)
-		return -EINVAL;
-
-	/* RFC 2543 allows lines to be terminated with CR, LF or CRLF,
-	 * RFC 3261 allows only CRLF, we support both. */
-	if (*(dptr + *matchoff + *matchlen) != '\r' &&
-	    *(dptr + *matchoff + *matchlen) != '\n')
-		return -EINVAL;
-
-	IP_VS_DBG_BUF(9, "SIP callid %s (%d bytes)\n",
-		      IP_VS_DEBUG_CALLID(dptr + *matchoff, *matchlen),
-		      *matchlen);
-	return 0;
-}
-
-static int
-ip_vs_sip_fill_param(struct ip_vs_conn_param *p, struct sk_buff *skb)
-{
-	struct ip_vs_iphdr iph;
-	unsigned int dataoff, datalen, matchoff, matchlen;
-	const char *dptr;
-	int retc;
-
-	ip_vs_fill_iph_skb(p->af, skb, &iph);
-
-	/* Only useful with UDP */
-	if (iph.protocol != IPPROTO_UDP)
-		return -EINVAL;
-	/* todo: IPv6 fragments:
-	 *       I think this only should be done for the first fragment. /HS
-	 */
-	dataoff = iph.len + sizeof(struct udphdr);
-
-	if (dataoff >= skb->len)
-		return -EINVAL;
-	retc = skb_linearize(skb);
-	if (retc < 0)
-		return retc;
-	dptr = skb->data + dataoff;
-	datalen = skb->len - dataoff;
-
-	if (get_callid(dptr, dataoff, datalen, &matchoff, &matchlen))
-		return -EINVAL;
-
-	/* N.B: pe_data is only set on success,
-	 * this allows fallback to the default persistence logic on failure
-	 */
-	p->pe_data = kmemdup(dptr + matchoff, matchlen, GFP_ATOMIC);
-	if (!p->pe_data)
-		return -ENOMEM;
-
-	p->pe_data_len = matchlen;
-
-	return 0;
-}
-
-static bool ip_vs_sip_ct_match(const struct ip_vs_conn_param *p,
-				  struct ip_vs_conn *ct)
-
-{
-	bool ret = false;
-
-	if (ct->af == p->af &&
-	    ip_vs_addr_equal(p->af, p->caddr, &ct->caddr) &&
-	    /* protocol should only be IPPROTO_IP if
-	     * d_addr is a fwmark */
-	    ip_vs_addr_equal(p->protocol == IPPROTO_IP ? AF_UNSPEC : p->af,
-			     p->vaddr, &ct->vaddr) &&
-	    ct->vport == p->vport &&
-	    ct->flags & IP_VS_CONN_F_TEMPLATE &&
-	    ct->protocol == p->protocol &&
-	    ct->pe_data && ct->pe_data_len == p->pe_data_len &&
-	    !memcmp(ct->pe_data, p->pe_data, p->pe_data_len))
-		ret = true;
-
-	IP_VS_DBG_BUF(9, "SIP template match %s %s->%s:%d %s\n",
-		      ip_vs_proto_name(p->protocol),
-		      IP_VS_DEBUG_CALLID(p->pe_data, p->pe_data_len),
-		      IP_VS_DBG_ADDR(p->af, p->vaddr), ntohs(p->vport),
-		      ret ? "hit" : "not hit");
-
-	return ret;
-}
-
-static u32 ip_vs_sip_hashkey_raw(const struct ip_vs_conn_param *p,
-				 u32 initval, bool inverse)
-{
-	return jhash(p->pe_data, p->pe_data_len, initval);
-}
-
-static int ip_vs_sip_show_pe_data(const struct ip_vs_conn *cp, char *buf)
-{
-	memcpy(buf, cp->pe_data, cp->pe_data_len);
-	return cp->pe_data_len;
-}
-
-static struct ip_vs_pe ip_vs_sip_pe =
-{
-	.name =			"sip",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_sip_pe.n_list),
-	.fill_param =		ip_vs_sip_fill_param,
-	.ct_match =		ip_vs_sip_ct_match,
-	.hashkey_raw =		ip_vs_sip_hashkey_raw,
-	.show_pe_data =		ip_vs_sip_show_pe_data,
-};
-
-static int __init ip_vs_sip_init(void)
-{
-	return register_ip_vs_pe(&ip_vs_sip_pe);
-}
-
-static void __exit ip_vs_sip_cleanup(void)
-{
-	unregister_ip_vs_pe(&ip_vs_sip_pe);
-	synchronize_rcu();
-}
-
-module_init(ip_vs_sip_init);
-module_exit(ip_vs_sip_cleanup);
-MODULE_LICENSE("GPL");
diff --git a/net/netfilter/ipvs/ip_vs_proto.c b/net/netfilter/ipvs/ip_vs_proto.c
index 939f7fb..71faee4 100644
--- a/net/netfilter/ipvs/ip_vs_proto.c
+++ b/net/netfilter/ipvs/ip_vs_proto.c
@@ -19,7 +19,6 @@
 #include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/skbuff.h>
-#include <linux/gfp.h>
 #include <linux/in.h>
 #include <linux/ip.h>
 #include <net/protocol.h>
@@ -48,7 +47,7 @@ static struct ip_vs_protocol *ip_vs_proto_table[IP_VS_PROTO_TAB_SIZE];
  */
 static int __used __init register_ip_vs_protocol(struct ip_vs_protocol *pp)
 {
-	unsigned int hash = IP_VS_PROTO_HASH(pp->protocol);
+	unsigned hash = IP_VS_PROTO_HASH(pp->protocol);
 
 	pp->next = ip_vs_proto_table[hash];
 	ip_vs_proto_table[hash] = pp;
@@ -59,37 +58,6 @@ static int __used __init register_ip_vs_protocol(struct ip_vs_protocol *pp)
 	return 0;
 }
 
-/*
- *	register an ipvs protocols netns related data
- */
-static int
-register_ip_vs_proto_netns(struct net *net, struct ip_vs_protocol *pp)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	unsigned int hash = IP_VS_PROTO_HASH(pp->protocol);
-	struct ip_vs_proto_data *pd =
-			kzalloc(sizeof(struct ip_vs_proto_data), GFP_KERNEL);
-
-	if (!pd)
-		return -ENOMEM;
-
-	pd->pp = pp;	/* For speed issues */
-	pd->next = ipvs->proto_data_table[hash];
-	ipvs->proto_data_table[hash] = pd;
-	atomic_set(&pd->appcnt, 0);	/* Init app counter */
-
-	if (pp->init_netns != NULL) {
-		int ret = pp->init_netns(net, pd);
-		if (ret) {
-			/* unlink an free proto data */
-			ipvs->proto_data_table[hash] = pd->next;
-			kfree(pd);
-			return ret;
-		}
-	}
-
-	return 0;
-}
 
 /*
  *	unregister an ipvs protocol
@@ -97,7 +65,7 @@ register_ip_vs_proto_netns(struct net *net, struct ip_vs_protocol *pp)
 static int unregister_ip_vs_protocol(struct ip_vs_protocol *pp)
 {
 	struct ip_vs_protocol **pp_p;
-	unsigned int hash = IP_VS_PROTO_HASH(pp->protocol);
+	unsigned hash = IP_VS_PROTO_HASH(pp->protocol);
 
 	pp_p = &ip_vs_proto_table[hash];
 	for (; *pp_p; pp_p = &(*pp_p)->next) {
@@ -112,29 +80,6 @@ static int unregister_ip_vs_protocol(struct ip_vs_protocol *pp)
 	return -ESRCH;
 }
 
-/*
- *	unregister an ipvs protocols netns data
- */
-static int
-unregister_ip_vs_proto_netns(struct net *net, struct ip_vs_proto_data *pd)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_proto_data **pd_p;
-	unsigned int hash = IP_VS_PROTO_HASH(pd->pp->protocol);
-
-	pd_p = &ipvs->proto_data_table[hash];
-	for (; *pd_p; pd_p = &(*pd_p)->next) {
-		if (*pd_p == pd) {
-			*pd_p = pd->next;
-			if (pd->pp->exit_netns != NULL)
-				pd->pp->exit_netns(net, pd);
-			kfree(pd);
-			return 0;
-		}
-	}
-
-	return -ESRCH;
-}
 
 /*
  *	get ip_vs_protocol object by its proto.
@@ -142,7 +87,7 @@ unregister_ip_vs_proto_netns(struct net *net, struct ip_vs_proto_data *pd)
 struct ip_vs_protocol * ip_vs_proto_get(unsigned short proto)
 {
 	struct ip_vs_protocol *pp;
-	unsigned int hash = IP_VS_PROTO_HASH(proto);
+	unsigned hash = IP_VS_PROTO_HASH(proto);
 
 	for (pp = ip_vs_proto_table[hash]; pp; pp = pp->next) {
 		if (pp->protocol == proto)
@@ -151,46 +96,20 @@ struct ip_vs_protocol * ip_vs_proto_get(unsigned short proto)
 
 	return NULL;
 }
-EXPORT_SYMBOL(ip_vs_proto_get);
-
-/*
- *	get ip_vs_protocol object data by netns and proto
- */
-static struct ip_vs_proto_data *
-__ipvs_proto_data_get(struct netns_ipvs *ipvs, unsigned short proto)
-{
-	struct ip_vs_proto_data *pd;
-	unsigned int hash = IP_VS_PROTO_HASH(proto);
-
-	for (pd = ipvs->proto_data_table[hash]; pd; pd = pd->next) {
-		if (pd->pp->protocol == proto)
-			return pd;
-	}
-
-	return NULL;
-}
 
-struct ip_vs_proto_data *
-ip_vs_proto_data_get(struct net *net, unsigned short proto)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	return __ipvs_proto_data_get(ipvs, proto);
-}
-EXPORT_SYMBOL(ip_vs_proto_data_get);
 
 /*
  *	Propagate event for state change to all protocols
  */
-void ip_vs_protocol_timeout_change(struct netns_ipvs *ipvs, int flags)
+void ip_vs_protocol_timeout_change(int flags)
 {
-	struct ip_vs_proto_data *pd;
+	struct ip_vs_protocol *pp;
 	int i;
 
 	for (i = 0; i < IP_VS_PROTO_TAB_SIZE; i++) {
-		for (pd = ipvs->proto_data_table[i]; pd; pd = pd->next) {
-			if (pd->pp->timeout_change)
-				pd->pp->timeout_change(pd, flags);
+		for (pp = ip_vs_proto_table[i]; pp; pp = pp->next) {
+			if (pp->timeout_change)
+				pp->timeout_change(pp, flags);
 		}
 	}
 }
@@ -199,7 +118,7 @@ void ip_vs_protocol_timeout_change(struct netns_ipvs *ipvs, int flags)
 int *
 ip_vs_create_timeout_table(int *table, int size)
 {
-	return kmemdup(table, size, GFP_KERNEL);
+	return kmemdup(table, size, GFP_ATOMIC);
 }
 
 
@@ -246,24 +165,26 @@ ip_vs_tcpudp_debug_packet_v4(struct ip_vs_protocol *pp,
 
 	ih = skb_header_pointer(skb, offset, sizeof(_iph), &_iph);
 	if (ih == NULL)
-		sprintf(buf, "TRUNCATED");
+		sprintf(buf, "%s TRUNCATED", pp->name);
 	else if (ih->frag_off & htons(IP_OFFSET))
-		sprintf(buf, "%pI4->%pI4 frag", &ih->saddr, &ih->daddr);
+		sprintf(buf, "%s %pI4->%pI4 frag",
+			pp->name, &ih->saddr, &ih->daddr);
 	else {
-		__be16 _ports[2], *pptr;
-
+		__be16 _ports[2], *pptr
+;
 		pptr = skb_header_pointer(skb, offset + ih->ihl*4,
 					  sizeof(_ports), _ports);
 		if (pptr == NULL)
-			sprintf(buf, "TRUNCATED %pI4->%pI4",
-				&ih->saddr, &ih->daddr);
+			sprintf(buf, "%s TRUNCATED %pI4->%pI4",
+				pp->name, &ih->saddr, &ih->daddr);
 		else
-			sprintf(buf, "%pI4:%u->%pI4:%u",
+			sprintf(buf, "%s %pI4:%u->%pI4:%u",
+				pp->name,
 				&ih->saddr, ntohs(pptr[0]),
 				&ih->daddr, ntohs(pptr[1]));
 	}
 
-	pr_debug("%s: %s %s\n", msg, pp->name, buf);
+	pr_debug("%s: %s\n", msg, buf);
 }
 
 #ifdef CONFIG_IP_VS_IPV6
@@ -278,90 +199,44 @@ ip_vs_tcpudp_debug_packet_v6(struct ip_vs_protocol *pp,
 
 	ih = skb_header_pointer(skb, offset, sizeof(_iph), &_iph);
 	if (ih == NULL)
-		sprintf(buf, "TRUNCATED");
+		sprintf(buf, "%s TRUNCATED", pp->name);
 	else if (ih->nexthdr == IPPROTO_FRAGMENT)
-		sprintf(buf, "%pI6c->%pI6c frag", &ih->saddr, &ih->daddr);
+		sprintf(buf, "%s %pI6->%pI6 frag",
+			pp->name, &ih->saddr, &ih->daddr);
 	else {
 		__be16 _ports[2], *pptr;
 
 		pptr = skb_header_pointer(skb, offset + sizeof(struct ipv6hdr),
 					  sizeof(_ports), _ports);
 		if (pptr == NULL)
-			sprintf(buf, "TRUNCATED %pI6c->%pI6c",
-				&ih->saddr, &ih->daddr);
+			sprintf(buf, "%s TRUNCATED %pI6->%pI6",
+				pp->name, &ih->saddr, &ih->daddr);
 		else
-			sprintf(buf, "%pI6c:%u->%pI6c:%u",
+			sprintf(buf, "%s %pI6:%u->%pI6:%u",
+				pp->name,
 				&ih->saddr, ntohs(pptr[0]),
 				&ih->daddr, ntohs(pptr[1]));
 	}
 
-	pr_debug("%s: %s %s\n", msg, pp->name, buf);
+	pr_debug("%s: %s\n", msg, buf);
 }
 #endif
 
 
 void
-ip_vs_tcpudp_debug_packet(int af, struct ip_vs_protocol *pp,
+ip_vs_tcpudp_debug_packet(struct ip_vs_protocol *pp,
 			  const struct sk_buff *skb,
 			  int offset,
 			  const char *msg)
 {
 #ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
+	if (skb->protocol == htons(ETH_P_IPV6))
 		ip_vs_tcpudp_debug_packet_v6(pp, skb, offset, msg);
 	else
 #endif
 		ip_vs_tcpudp_debug_packet_v4(pp, skb, offset, msg);
 }
 
-/*
- * per network name-space init
- */
-int __net_init ip_vs_protocol_net_init(struct net *net)
-{
-	int i, ret;
-	static struct ip_vs_protocol *protos[] = {
-#ifdef CONFIG_IP_VS_PROTO_TCP
-        &ip_vs_protocol_tcp,
-#endif
-#ifdef CONFIG_IP_VS_PROTO_UDP
-	&ip_vs_protocol_udp,
-#endif
-#ifdef CONFIG_IP_VS_PROTO_SCTP
-	&ip_vs_protocol_sctp,
-#endif
-#ifdef CONFIG_IP_VS_PROTO_AH
-	&ip_vs_protocol_ah,
-#endif
-#ifdef CONFIG_IP_VS_PROTO_ESP
-	&ip_vs_protocol_esp,
-#endif
-	};
-
-	for (i = 0; i < ARRAY_SIZE(protos); i++) {
-		ret = register_ip_vs_proto_netns(net, protos[i]);
-		if (ret < 0)
-			goto cleanup;
-	}
-	return 0;
-
-cleanup:
-	ip_vs_protocol_net_cleanup(net);
-	return ret;
-}
-
-void __net_exit ip_vs_protocol_net_cleanup(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_proto_data *pd;
-	int i;
-
-	/* unregister all the ipvs proto data for this netns */
-	for (i = 0; i < IP_VS_PROTO_TAB_SIZE; i++) {
-		while ((pd = ipvs->proto_data_table[i]) != NULL)
-			unregister_ip_vs_proto_netns(net, pd);
-	}
-}
 
 int __init ip_vs_protocol_init(void)
 {
@@ -381,9 +256,6 @@ int __init ip_vs_protocol_init(void)
 #ifdef CONFIG_IP_VS_PROTO_UDP
 	REGISTER_PROTOCOL(&ip_vs_protocol_udp);
 #endif
-#ifdef CONFIG_IP_VS_PROTO_SCTP
-	REGISTER_PROTOCOL(&ip_vs_protocol_sctp);
-#endif
 #ifdef CONFIG_IP_VS_PROTO_AH
 	REGISTER_PROTOCOL(&ip_vs_protocol_ah);
 #endif
diff --git a/net/netfilter/ipvs/ip_vs_proto_ah_esp.c b/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
index 5de3dd3..3306e7f 100644
--- a/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
+++ b/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
@@ -40,32 +40,26 @@ struct isakmp_hdr {
 
 #define PORT_ISAKMP	500
 
-static void
-ah_esp_conn_fill_param_proto(struct net *net, int af,
-			     const struct ip_vs_iphdr *iph, int inverse,
-			     struct ip_vs_conn_param *p)
-{
-	if (likely(!inverse))
-		ip_vs_conn_fill_param(net, af, IPPROTO_UDP,
-				      &iph->saddr, htons(PORT_ISAKMP),
-				      &iph->daddr, htons(PORT_ISAKMP), p);
-	else
-		ip_vs_conn_fill_param(net, af, IPPROTO_UDP,
-				      &iph->daddr, htons(PORT_ISAKMP),
-				      &iph->saddr, htons(PORT_ISAKMP), p);
-}
-
-static struct ip_vs_conn *
-ah_esp_conn_in_get(int af, const struct sk_buff *skb,
-		   const struct ip_vs_iphdr *iph,
-		   int inverse)
+static struct ip_vs_conn *ah_esp_conn_in_get(int af, const struct sk_buff *skb,
+					     struct ip_vs_protocol *pp,
+					     const struct ip_vs_iphdr *iph,
+					     unsigned int proto_off,
+					     int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
-	struct ip_vs_conn_param p;
-	struct net *net = skb_net(skb);
 
-	ah_esp_conn_fill_param_proto(net, af, iph, inverse, &p);
-	cp = ip_vs_conn_in_get(&p);
+	if (likely(!inverse)) {
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->saddr,
+				    htons(PORT_ISAKMP),
+				    &iph->daddr, htons(PORT_ISAKMP), res_dir);
+	} else {
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->daddr,
+				    htons(PORT_ISAKMP),
+				    &iph->saddr, htons(PORT_ISAKMP), res_dir);
+	}
+
 	if (!cp) {
 		/*
 		 * We are not sure if the packet is from our
@@ -74,7 +68,7 @@ ah_esp_conn_in_get(int af, const struct sk_buff *skb,
 		IP_VS_DBG_BUF(12, "Unknown ISAKMP entry for outin packet "
 			      "%s%s %s->%s\n",
 			      inverse ? "ICMP+" : "",
-			      ip_vs_proto_get(iph->protocol)->name,
+			      pp->name,
 			      IP_VS_DBG_ADDR(af, &iph->saddr),
 			      IP_VS_DBG_ADDR(af, &iph->daddr));
 	}
@@ -82,22 +76,31 @@ ah_esp_conn_in_get(int af, const struct sk_buff *skb,
 	return cp;
 }
 
-
-static struct ip_vs_conn *
-ah_esp_conn_out_get(int af, const struct sk_buff *skb,
-		    const struct ip_vs_iphdr *iph, int inverse)
+static struct ip_vs_conn *ah_esp_conn_out_get(int af, const struct sk_buff *skb,
+					      struct ip_vs_protocol *pp,
+					      const struct ip_vs_iphdr *iph,
+					      unsigned int proto_off,
+					      int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
-	struct ip_vs_conn_param p;
-	struct net *net = skb_net(skb);
 
-	ah_esp_conn_fill_param_proto(net, af, iph, inverse, &p);
-	cp = ip_vs_conn_out_get(&p);
+	if (likely(!inverse)) {
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->saddr,
+				    htons(PORT_ISAKMP),
+				    &iph->daddr, htons(PORT_ISAKMP), res_dir);
+	} else {
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->daddr,
+				    htons(PORT_ISAKMP),
+				    &iph->saddr, htons(PORT_ISAKMP), res_dir);
+	}
+
 	if (!cp) {
 		IP_VS_DBG_BUF(12, "Unknown ISAKMP entry for inout packet "
 			      "%s%s %s->%s\n",
 			      inverse ? "ICMP+" : "",
-			      ip_vs_proto_get(iph->protocol)->name,
+			      pp->name,
 			      IP_VS_DBG_ADDR(af, &iph->saddr),
 			      IP_VS_DBG_ADDR(af, &iph->daddr));
 	}
@@ -107,9 +110,8 @@ ah_esp_conn_out_get(int af, const struct sk_buff *skb,
 
 
 static int
-ah_esp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
-		     int *verdict, struct ip_vs_conn **cpp,
-		     struct ip_vs_iphdr *iph)
+ah_esp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
+		     int *verdict, struct ip_vs_conn **cpp)
 {
 	/*
 	 * AH/ESP is only related traffic. Pass the packet to IP stack.
@@ -118,14 +120,76 @@ ah_esp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
 	return 0;
 }
 
+
+static void
+ah_esp_debug_packet_v4(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+		       int offset, const char *msg)
+{
+	char buf[256];
+	struct iphdr _iph, *ih;
+
+	ih = skb_header_pointer(skb, offset, sizeof(_iph), &_iph);
+	if (ih == NULL)
+		sprintf(buf, "%s TRUNCATED", pp->name);
+	else
+		sprintf(buf, "%s %pI4->%pI4",
+			pp->name, &ih->saddr, &ih->daddr);
+
+	pr_debug("%s: %s\n", msg, buf);
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static void
+ah_esp_debug_packet_v6(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+		       int offset, const char *msg)
+{
+	char buf[256];
+	struct ipv6hdr _iph, *ih;
+
+	ih = skb_header_pointer(skb, offset, sizeof(_iph), &_iph);
+	if (ih == NULL)
+		sprintf(buf, "%s TRUNCATED", pp->name);
+	else
+		sprintf(buf, "%s %pI6->%pI6",
+			pp->name, &ih->saddr, &ih->daddr);
+
+	pr_debug("%s: %s\n", msg, buf);
+}
+#endif
+
+static void
+ah_esp_debug_packet(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+		    int offset, const char *msg)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (skb->protocol == htons(ETH_P_IPV6))
+		ah_esp_debug_packet_v6(pp, skb, offset, msg);
+	else
+#endif
+		ah_esp_debug_packet_v4(pp, skb, offset, msg);
+}
+
+
+static void ah_esp_init(struct ip_vs_protocol *pp)
+{
+	/* nothing to do now */
+}
+
+
+static void ah_esp_exit(struct ip_vs_protocol *pp)
+{
+	/* nothing to do now */
+}
+
+
 #ifdef CONFIG_IP_VS_PROTO_AH
 struct ip_vs_protocol ip_vs_protocol_ah = {
 	.name =			"AH",
 	.protocol =		IPPROTO_AH,
 	.num_states =		1,
 	.dont_defrag =		1,
-	.init =			NULL,
-	.exit =			NULL,
+	.init =			ah_esp_init,
+	.exit =			ah_esp_exit,
 	.conn_schedule =	ah_esp_conn_schedule,
 	.conn_in_get =		ah_esp_conn_in_get,
 	.conn_out_get =		ah_esp_conn_out_get,
@@ -136,8 +200,9 @@ struct ip_vs_protocol ip_vs_protocol_ah = {
 	.register_app =		NULL,
 	.unregister_app =	NULL,
 	.app_conn_bind =	NULL,
-	.debug_packet =		ip_vs_tcpudp_debug_packet,
+	.debug_packet =		ah_esp_debug_packet,
 	.timeout_change =	NULL,		/* ISAKMP */
+	.set_state_timeout =	NULL,
 };
 #endif
 
@@ -147,8 +212,8 @@ struct ip_vs_protocol ip_vs_protocol_esp = {
 	.protocol =		IPPROTO_ESP,
 	.num_states =		1,
 	.dont_defrag =		1,
-	.init =			NULL,
-	.exit =			NULL,
+	.init =			ah_esp_init,
+	.exit =			ah_esp_exit,
 	.conn_schedule =	ah_esp_conn_schedule,
 	.conn_in_get =		ah_esp_conn_in_get,
 	.conn_out_get =		ah_esp_conn_out_get,
@@ -159,7 +224,7 @@ struct ip_vs_protocol ip_vs_protocol_esp = {
 	.register_app =		NULL,
 	.unregister_app =	NULL,
 	.app_conn_bind =	NULL,
-	.debug_packet =		ip_vs_tcpudp_debug_packet,
+	.debug_packet =		ah_esp_debug_packet,
 	.timeout_change =	NULL,		/* ISAKMP */
 };
 #endif
diff --git a/net/netfilter/ipvs/ip_vs_proto_sctp.c b/net/netfilter/ipvs/ip_vs_proto_sctp.c
deleted file mode 100644
index 1a74c62..0000000
--- a/net/netfilter/ipvs/ip_vs_proto_sctp.c
+++ /dev/null
@@ -1,1137 +0,0 @@
-#include <linux/kernel.h>
-#include <linux/ip.h>
-#include <linux/sctp.h>
-#include <net/ip.h>
-#include <net/ip6_checksum.h>
-#include <linux/netfilter.h>
-#include <linux/netfilter_ipv4.h>
-#include <net/sctp/checksum.h>
-#include <net/ip_vs.h>
-
-static int
-sctp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
-		   int *verdict, struct ip_vs_conn **cpp,
-		   struct ip_vs_iphdr *iph)
-{
-	struct net *net;
-	struct ip_vs_service *svc;
-	sctp_chunkhdr_t _schunkh, *sch;
-	sctp_sctphdr_t *sh, _sctph;
-
-	sh = skb_header_pointer(skb, iph->len, sizeof(_sctph), &_sctph);
-	if (sh == NULL) {
-		*verdict = NF_DROP;
-		return 0;
-	}
-
-	sch = skb_header_pointer(skb, iph->len + sizeof(sctp_sctphdr_t),
-				 sizeof(_schunkh), &_schunkh);
-	if (sch == NULL) {
-		*verdict = NF_DROP;
-		return 0;
-	}
-
-	net = skb_net(skb);
-	rcu_read_lock();
-	if ((sch->type == SCTP_CID_INIT) &&
-	    (svc = ip_vs_service_find(net, af, skb->mark, iph->protocol,
-				      &iph->daddr, sh->dest))) {
-		int ignored;
-
-		if (ip_vs_todrop(net_ipvs(net))) {
-			/*
-			 * It seems that we are very loaded.
-			 * We have to drop this packet :(
-			 */
-			rcu_read_unlock();
-			*verdict = NF_DROP;
-			return 0;
-		}
-		/*
-		 * Let the virtual server select a real server for the
-		 * incoming connection, and create a connection entry.
-		 */
-		*cpp = ip_vs_schedule(svc, skb, pd, &ignored, iph);
-		if (!*cpp && ignored <= 0) {
-			if (!ignored)
-				*verdict = ip_vs_leave(svc, skb, pd, iph);
-			else
-				*verdict = NF_DROP;
-			rcu_read_unlock();
-			return 0;
-		}
-	}
-	rcu_read_unlock();
-	/* NF_ACCEPT */
-	return 1;
-}
-
-static void sctp_nat_csum(struct sk_buff *skb, sctp_sctphdr_t *sctph,
-			  unsigned int sctphoff)
-{
-	sctph->checksum = sctp_compute_cksum(skb, sctphoff);
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-}
-
-static int
-sctp_snat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		  struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
-{
-	sctp_sctphdr_t *sctph;
-	unsigned int sctphoff = iph->len;
-	bool payload_csum = false;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6 && iph->fragoffs)
-		return 1;
-#endif
-
-	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, sctphoff + sizeof(*sctph)))
-		return 0;
-
-	if (unlikely(cp->app != NULL)) {
-		int ret;
-
-		/* Some checks before mangling */
-		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
-			return 0;
-
-		/* Call application helper if needed */
-		ret = ip_vs_app_pkt_out(cp, skb);
-		if (ret == 0)
-			return 0;
-		/* ret=2: csum update is needed after payload mangling */
-		if (ret == 2)
-			payload_csum = true;
-	}
-
-	sctph = (void *) skb_network_header(skb) + sctphoff;
-
-	/* Only update csum if we really have to */
-	if (sctph->source != cp->vport || payload_csum ||
-	    skb->ip_summed == CHECKSUM_PARTIAL) {
-		sctph->source = cp->vport;
-		sctp_nat_csum(skb, sctph, sctphoff);
-	} else {
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
-	}
-
-	return 1;
-}
-
-static int
-sctp_dnat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		  struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
-{
-	sctp_sctphdr_t *sctph;
-	unsigned int sctphoff = iph->len;
-	bool payload_csum = false;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6 && iph->fragoffs)
-		return 1;
-#endif
-
-	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, sctphoff + sizeof(*sctph)))
-		return 0;
-
-	if (unlikely(cp->app != NULL)) {
-		int ret;
-
-		/* Some checks before mangling */
-		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
-			return 0;
-
-		/* Call application helper if needed */
-		ret = ip_vs_app_pkt_in(cp, skb);
-		if (ret == 0)
-			return 0;
-		/* ret=2: csum update is needed after payload mangling */
-		if (ret == 2)
-			payload_csum = true;
-	}
-
-	sctph = (void *) skb_network_header(skb) + sctphoff;
-
-	/* Only update csum if we really have to */
-	if (sctph->dest != cp->dport || payload_csum ||
-	    (skb->ip_summed == CHECKSUM_PARTIAL &&
-	     !(skb_dst(skb)->dev->features & NETIF_F_SCTP_CSUM))) {
-		sctph->dest = cp->dport;
-		sctp_nat_csum(skb, sctph, sctphoff);
-	} else if (skb->ip_summed != CHECKSUM_PARTIAL) {
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
-	}
-
-	return 1;
-}
-
-static int
-sctp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
-{
-	unsigned int sctphoff;
-	struct sctphdr *sh, _sctph;
-	__le32 cmp, val;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		sctphoff = sizeof(struct ipv6hdr);
-	else
-#endif
-		sctphoff = ip_hdrlen(skb);
-
-	sh = skb_header_pointer(skb, sctphoff, sizeof(_sctph), &_sctph);
-	if (sh == NULL)
-		return 0;
-
-	cmp = sh->checksum;
-	val = sctp_compute_cksum(skb, sctphoff);
-
-	if (val != cmp) {
-		/* CRC failure, dump it. */
-		IP_VS_DBG_RL_PKT(0, af, pp, skb, 0,
-				"Failed checksum for");
-		return 0;
-	}
-	return 1;
-}
-
-struct ipvs_sctp_nextstate {
-	int next_state;
-};
-enum ipvs_sctp_event_t {
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_DATA_SER,
-	IP_VS_SCTP_EVE_INIT_CLI,
-	IP_VS_SCTP_EVE_INIT_SER,
-	IP_VS_SCTP_EVE_INIT_ACK_CLI,
-	IP_VS_SCTP_EVE_INIT_ACK_SER,
-	IP_VS_SCTP_EVE_COOKIE_ECHO_CLI,
-	IP_VS_SCTP_EVE_COOKIE_ECHO_SER,
-	IP_VS_SCTP_EVE_COOKIE_ACK_CLI,
-	IP_VS_SCTP_EVE_COOKIE_ACK_SER,
-	IP_VS_SCTP_EVE_ABORT_CLI,
-	IP_VS_SCTP_EVE__ABORT_SER,
-	IP_VS_SCTP_EVE_SHUT_CLI,
-	IP_VS_SCTP_EVE_SHUT_SER,
-	IP_VS_SCTP_EVE_SHUT_ACK_CLI,
-	IP_VS_SCTP_EVE_SHUT_ACK_SER,
-	IP_VS_SCTP_EVE_SHUT_COM_CLI,
-	IP_VS_SCTP_EVE_SHUT_COM_SER,
-	IP_VS_SCTP_EVE_LAST
-};
-
-static enum ipvs_sctp_event_t sctp_events[256] = {
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_INIT_CLI,
-	IP_VS_SCTP_EVE_INIT_ACK_CLI,
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_ABORT_CLI,
-	IP_VS_SCTP_EVE_SHUT_CLI,
-	IP_VS_SCTP_EVE_SHUT_ACK_CLI,
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_COOKIE_ECHO_CLI,
-	IP_VS_SCTP_EVE_COOKIE_ACK_CLI,
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_DATA_CLI,
-	IP_VS_SCTP_EVE_SHUT_COM_CLI,
-};
-
-static struct ipvs_sctp_nextstate
- sctp_states_table[IP_VS_SCTP_S_LAST][IP_VS_SCTP_EVE_LAST] = {
-	/*
-	 * STATE : IP_VS_SCTP_S_NONE
-	 */
-	/*next state *//*event */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ },
-	 },
-	/*
-	 * STATE : IP_VS_SCTP_S_INIT_CLI
-	 * Cient sent INIT and is waiting for reply from server(In ECHO_WAIT)
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_INIT_ACK_SER /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_ECHO_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_INIT_SER
-	 * Server sent INIT and waiting for INIT ACK from the client
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 {IP_VS_SCTP_S_INIT_ACK_CLI /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_INIT_ACK_CLI
-	 * Client sent INIT ACK and waiting for ECHO from the server
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK has been resent by the client, let us stay is in
-	  * the same state
-	  */
-	 {IP_VS_SCTP_S_INIT_ACK_CLI /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 /*
-	  * INIT_ACK sent by the server, close the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * ECHO by client, it should not happen, close the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 /*
-	  * ECHO by server, this is what we are expecting, move to ECHO_SER
-	  */
-	 {IP_VS_SCTP_S_ECHO_SER /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, it should not happen, close the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 /*
-	  * Unexpected COOKIE ACK from server, staty in the same state
-	  */
-	 {IP_VS_SCTP_S_INIT_ACK_CLI /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_INIT_ACK_SER
-	 * Server sent INIT ACK and waiting for ECHO from the client
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * Unexpected INIT_ACK by the client, let us close the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 /*
-	  * INIT_ACK resent by the server, let us move to same state
-	  */
-	 {IP_VS_SCTP_S_INIT_ACK_SER /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client send the ECHO, this is what we are expecting,
-	  * move to ECHO_CLI
-	  */
-	 {IP_VS_SCTP_S_ECHO_CLI /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 /*
-	  * ECHO received from the server, Not sure what to do,
-	  * let us close it
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, let us stay in the same state
-	  */
-	 {IP_VS_SCTP_S_INIT_ACK_SER /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 /*
-	  * COOKIE ACK from server, hmm... this should not happen, lets close
-	  * the connection.
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_ECHO_CLI
-	 * Cient  sent ECHO and waiting COOKEI ACK from the Server
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK has been by the client, let us close the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ECHO_CLI /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client resent the ECHO, let us stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ECHO_CLI /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 /*
-	  * ECHO received from the server, Not sure what to do,
-	  * let us close it
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, this shoud not happen, let's close the
-	  * connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 /*
-	  * COOKIE ACK from server, this is what we are awaiting,lets move to
-	  * ESTABLISHED.
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_ECHO_SER
-	 * Server sent ECHO and waiting COOKEI ACK from the client
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ECHO_SER /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 /*
-	  * INIT_ACK has been by the server, let us close the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client sent the ECHO, not sure what to do, let's close the
-	  * connection.
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 /*
-	  * ECHO resent by the server, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ECHO_SER /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, this is what we are expecting, let's move
-	  * to ESTABLISHED.
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 /*
-	  * COOKIE ACK from server, this should not happen, lets close the
-	  * connection.
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_ESTABLISHED
-	 * Association established
-	 */
-	{{IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client sent ECHO, Spec(sec 5.2.4) says it may be handled by the
-	  * peer and peer shall move to the ESTABISHED. if it doesn't handle
-	  * it will send ERROR chunk. So, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, not sure what to do stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 /*
-	  * SHUTDOWN from the client, move to SHUDDOWN_CLI
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 /*
-	  * SHUTDOWN from the server, move to SHUTDOWN_SER
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 /*
-	  * client sent SHUDTDOWN_ACK, this should not happen, let's close
-	  * the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_SHUT_CLI
-	 * SHUTDOWN sent from the client, waitinf for SHUT ACK from the server
-	 */
-	/*
-	 * We received the data chuck, keep the state unchanged. I assume
-	 * that still data chuncks  can be received by both the peers in
-	 * SHUDOWN state
-	 */
-
-	{{IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client sent ECHO, Spec(sec 5.2.4) says it may be handled by the
-	  * peer and peer shall move to the ESTABISHED. if it doesn't handle
-	  * it will send ERROR chunk. So, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, not sure what to do stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 /*
-	  * SHUTDOWN resent from the client, move to SHUDDOWN_CLI
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 /*
-	  * SHUTDOWN from the server, move to SHUTDOWN_SER
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 /*
-	  * client sent SHUDTDOWN_ACK, this should not happen, let's close
-	  * the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 /*
-	  * Server sent SHUTDOWN ACK, this is what we are expecting, let's move
-	  * to SHUDOWN_ACK_SER
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 /*
-	  * SHUTDOWN COM from client, this should not happen, let's close the
-	  * connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_SHUT_SER
-	 * SHUTDOWN sent from the server, waitinf for SHUTDOWN ACK from client
-	 */
-	/*
-	 * We received the data chuck, keep the state unchanged. I assume
-	 * that still data chuncks  can be received by both the peers in
-	 * SHUDOWN state
-	 */
-
-	{{IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client sent ECHO, Spec(sec 5.2.4) says it may be handled by the
-	  * peer and peer shall move to the ESTABISHED. if it doesn't handle
-	  * it will send ERROR chunk. So, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, not sure what to do stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 /*
-	  * SHUTDOWN resent from the client, move to SHUDDOWN_CLI
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 /*
-	  * SHUTDOWN resent from the server, move to SHUTDOWN_SER
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 /*
-	  * client sent SHUDTDOWN_ACK, this is what we are expecting, let's
-	  * move to SHUT_ACK_CLI
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 /*
-	  * Server sent SHUTDOWN ACK, this should not happen, let's close the
-	  * connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 /*
-	  * SHUTDOWN COM from client, this should not happen, let's close the
-	  * connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-
-	/*
-	 * State : IP_VS_SCTP_S_SHUT_ACK_CLI
-	 * SHUTDOWN ACK from the client, awaiting for SHUTDOWN COM from server
-	 */
-	/*
-	 * We received the data chuck, keep the state unchanged. I assume
-	 * that still data chuncks  can be received by both the peers in
-	 * SHUDOWN state
-	 */
-
-	{{IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client sent ECHO, Spec(sec 5.2.4) says it may be handled by the
-	  * peer and peer shall move to the ESTABISHED. if it doesn't handle
-	  * it will send ERROR chunk. So, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, not sure what to do stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 /*
-	  * SHUTDOWN sent from the client, move to SHUDDOWN_CLI
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 /*
-	  * SHUTDOWN sent from the server, move to SHUTDOWN_SER
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 /*
-	  * client resent SHUDTDOWN_ACK, let's stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_CLI /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 /*
-	  * Server sent SHUTDOWN ACK, this should not happen, let's close the
-	  * connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 /*
-	  * SHUTDOWN COM from client, this should not happen, let's close the
-	  * connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 /*
-	  * SHUTDOWN COMPLETE from server this is what we are expecting.
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-
-	/*
-	 * State : IP_VS_SCTP_S_SHUT_ACK_SER
-	 * SHUTDOWN ACK from the server, awaiting for SHUTDOWN COM from client
-	 */
-	/*
-	 * We received the data chuck, keep the state unchanged. I assume
-	 * that still data chuncks  can be received by both the peers in
-	 * SHUDOWN state
-	 */
-
-	{{IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 /*
-	  * We have got an INIT from client. From the spec.Upon receipt of
-	  * an INIT in the COOKIE-WAIT state, an endpoint MUST respond with
-	  * an INIT ACK using the same parameters it sent in its  original
-	  * INIT chunk (including its Initiate Tag, unchanged).
-	  */
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 /*
-	  * INIT_ACK sent by the server, Unexpected INIT ACK, spec says,
-	  * If an INIT ACK is received by an endpoint in any state other
-	  * than the COOKIE-WAIT state, the endpoint should discard the
-	  * INIT ACK chunk. Stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 /*
-	  * Client sent ECHO, Spec(sec 5.2.4) says it may be handled by the
-	  * peer and peer shall move to the ESTABISHED. if it doesn't handle
-	  * it will send ERROR chunk. So, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_ESTABLISHED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 /*
-	  * COOKIE ACK from client, not sure what to do stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 /*
-	  * SHUTDOWN sent from the client, move to SHUDDOWN_CLI
-	  */
-	 {IP_VS_SCTP_S_SHUT_CLI /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 /*
-	  * SHUTDOWN sent from the server, move to SHUTDOWN_SER
-	  */
-	 {IP_VS_SCTP_S_SHUT_SER /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 /*
-	  * client sent SHUDTDOWN_ACK, this should not happen let's close
-	  * the connection.
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 /*
-	  * Server resent SHUTDOWN ACK, stay in the same state
-	  */
-	 {IP_VS_SCTP_S_SHUT_ACK_SER /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 /*
-	  * SHUTDOWN COM from client, this what we are expecting, let's close
-	  * the connection
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 /*
-	  * SHUTDOWN COMPLETE from server this should not happen.
-	  */
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 },
-	/*
-	 * State : IP_VS_SCTP_S_CLOSED
-	 */
-	{{IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_DATA_SER */ },
-	 {IP_VS_SCTP_S_INIT_CLI /* IP_VS_SCTP_EVE_INIT_CLI */ },
-	 {IP_VS_SCTP_S_INIT_SER /* IP_VS_SCTP_EVE_INIT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_INIT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ECHO_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_COOKIE_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_ABORT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_ACK_SER */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_CLI */ },
-	 {IP_VS_SCTP_S_CLOSED /* IP_VS_SCTP_EVE_SHUT_COM_SER */ }
-	 }
-};
-
-/*
- *      Timeout table[state]
- */
-static const int sctp_timeouts[IP_VS_SCTP_S_LAST + 1] = {
-	[IP_VS_SCTP_S_NONE]         =     2 * HZ,
-	[IP_VS_SCTP_S_INIT_CLI]     =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_INIT_SER]     =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_INIT_ACK_CLI] =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_INIT_ACK_SER] =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_ECHO_CLI]     =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_ECHO_SER]     =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_ESTABLISHED]  =    15 * 60 * HZ,
-	[IP_VS_SCTP_S_SHUT_CLI]     =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_SHUT_SER]     =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_SHUT_ACK_CLI] =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_SHUT_ACK_SER] =     1 * 60 * HZ,
-	[IP_VS_SCTP_S_CLOSED]       =    10 * HZ,
-	[IP_VS_SCTP_S_LAST]         =     2 * HZ,
-};
-
-static const char *sctp_state_name_table[IP_VS_SCTP_S_LAST + 1] = {
-	[IP_VS_SCTP_S_NONE]         =    "NONE",
-	[IP_VS_SCTP_S_INIT_CLI]     =    "INIT_CLI",
-	[IP_VS_SCTP_S_INIT_SER]     =    "INIT_SER",
-	[IP_VS_SCTP_S_INIT_ACK_CLI] =    "INIT_ACK_CLI",
-	[IP_VS_SCTP_S_INIT_ACK_SER] =    "INIT_ACK_SER",
-	[IP_VS_SCTP_S_ECHO_CLI]     =    "COOKIE_ECHO_CLI",
-	[IP_VS_SCTP_S_ECHO_SER]     =    "COOKIE_ECHO_SER",
-	[IP_VS_SCTP_S_ESTABLISHED]  =    "ESTABISHED",
-	[IP_VS_SCTP_S_SHUT_CLI]     =    "SHUTDOWN_CLI",
-	[IP_VS_SCTP_S_SHUT_SER]     =    "SHUTDOWN_SER",
-	[IP_VS_SCTP_S_SHUT_ACK_CLI] =    "SHUTDOWN_ACK_CLI",
-	[IP_VS_SCTP_S_SHUT_ACK_SER] =    "SHUTDOWN_ACK_SER",
-	[IP_VS_SCTP_S_CLOSED]       =    "CLOSED",
-	[IP_VS_SCTP_S_LAST]         =    "BUG!"
-};
-
-
-static const char *sctp_state_name(int state)
-{
-	if (state >= IP_VS_SCTP_S_LAST)
-		return "ERR!";
-	if (sctp_state_name_table[state])
-		return sctp_state_name_table[state];
-	return "?";
-}
-
-static inline void
-set_sctp_state(struct ip_vs_proto_data *pd, struct ip_vs_conn *cp,
-		int direction, const struct sk_buff *skb)
-{
-	sctp_chunkhdr_t _sctpch, *sch;
-	unsigned char chunk_type;
-	int event, next_state;
-	int ihl, cofs;
-
-#ifdef CONFIG_IP_VS_IPV6
-	ihl = cp->af == AF_INET ? ip_hdrlen(skb) : sizeof(struct ipv6hdr);
-#else
-	ihl = ip_hdrlen(skb);
-#endif
-
-	cofs = ihl + sizeof(sctp_sctphdr_t);
-	sch = skb_header_pointer(skb, cofs, sizeof(_sctpch), &_sctpch);
-	if (sch == NULL)
-		return;
-
-	chunk_type = sch->type;
-	/*
-	 * Section 3: Multiple chunks can be bundled into one SCTP packet
-	 * up to the MTU size, except for the INIT, INIT ACK, and
-	 * SHUTDOWN COMPLETE chunks. These chunks MUST NOT be bundled with
-	 * any other chunk in a packet.
-	 *
-	 * Section 3.3.7: DATA chunks MUST NOT be bundled with ABORT. Control
-	 * chunks (except for INIT, INIT ACK, and SHUTDOWN COMPLETE) MAY be
-	 * bundled with an ABORT, but they MUST be placed before the ABORT
-	 * in the SCTP packet or they will be ignored by the receiver.
-	 */
-	if ((sch->type == SCTP_CID_COOKIE_ECHO) ||
-	    (sch->type == SCTP_CID_COOKIE_ACK)) {
-		int clen = ntohs(sch->length);
-
-		if (clen >= sizeof(sctp_chunkhdr_t)) {
-			sch = skb_header_pointer(skb, cofs + ALIGN(clen, 4),
-						 sizeof(_sctpch), &_sctpch);
-			if (sch && sch->type == SCTP_CID_ABORT)
-				chunk_type = sch->type;
-		}
-	}
-
-	event = sctp_events[chunk_type];
-
-	/*
-	 *  If the direction is IP_VS_DIR_OUTPUT, this event is from server
-	 */
-	if (direction == IP_VS_DIR_OUTPUT)
-		event++;
-	/*
-	 * get next state
-	 */
-	next_state = sctp_states_table[cp->state][event].next_state;
-
-	if (next_state != cp->state) {
-		struct ip_vs_dest *dest = cp->dest;
-
-		IP_VS_DBG_BUF(8, "%s %s  %s:%d->"
-				"%s:%d state: %s->%s conn->refcnt:%d\n",
-				pd->pp->name,
-				((direction == IP_VS_DIR_OUTPUT) ?
-				 "output " : "input "),
-				IP_VS_DBG_ADDR(cp->af, &cp->daddr),
-				ntohs(cp->dport),
-				IP_VS_DBG_ADDR(cp->af, &cp->caddr),
-				ntohs(cp->cport),
-				sctp_state_name(cp->state),
-				sctp_state_name(next_state),
-				atomic_read(&cp->refcnt));
-		if (dest) {
-			if (!(cp->flags & IP_VS_CONN_F_INACTIVE) &&
-				(next_state != IP_VS_SCTP_S_ESTABLISHED)) {
-				atomic_dec(&dest->activeconns);
-				atomic_inc(&dest->inactconns);
-				cp->flags |= IP_VS_CONN_F_INACTIVE;
-			} else if ((cp->flags & IP_VS_CONN_F_INACTIVE) &&
-				   (next_state == IP_VS_SCTP_S_ESTABLISHED)) {
-				atomic_inc(&dest->activeconns);
-				atomic_dec(&dest->inactconns);
-				cp->flags &= ~IP_VS_CONN_F_INACTIVE;
-			}
-		}
-	}
-	if (likely(pd))
-		cp->timeout = pd->timeout_table[cp->state = next_state];
-	else	/* What to do ? */
-		cp->timeout = sctp_timeouts[cp->state = next_state];
-}
-
-static void
-sctp_state_transition(struct ip_vs_conn *cp, int direction,
-		const struct sk_buff *skb, struct ip_vs_proto_data *pd)
-{
-	spin_lock_bh(&cp->lock);
-	set_sctp_state(pd, cp, direction, skb);
-	spin_unlock_bh(&cp->lock);
-}
-
-static inline __u16 sctp_app_hashkey(__be16 port)
-{
-	return (((__force u16)port >> SCTP_APP_TAB_BITS) ^ (__force u16)port)
-		& SCTP_APP_TAB_MASK;
-}
-
-static int sctp_register_app(struct net *net, struct ip_vs_app *inc)
-{
-	struct ip_vs_app *i;
-	__u16 hash;
-	__be16 port = inc->port;
-	int ret = 0;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_SCTP);
-
-	hash = sctp_app_hashkey(port);
-
-	list_for_each_entry(i, &ipvs->sctp_apps[hash], p_list) {
-		if (i->port == port) {
-			ret = -EEXIST;
-			goto out;
-		}
-	}
-	list_add_rcu(&inc->p_list, &ipvs->sctp_apps[hash]);
-	atomic_inc(&pd->appcnt);
-out:
-
-	return ret;
-}
-
-static void sctp_unregister_app(struct net *net, struct ip_vs_app *inc)
-{
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_SCTP);
-
-	atomic_dec(&pd->appcnt);
-	list_del_rcu(&inc->p_list);
-}
-
-static int sctp_app_conn_bind(struct ip_vs_conn *cp)
-{
-	struct netns_ipvs *ipvs = net_ipvs(ip_vs_conn_net(cp));
-	int hash;
-	struct ip_vs_app *inc;
-	int result = 0;
-
-	/* Default binding: bind app only for NAT */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)
-		return 0;
-	/* Lookup application incarnations and bind the right one */
-	hash = sctp_app_hashkey(cp->vport);
-
-	rcu_read_lock();
-	list_for_each_entry_rcu(inc, &ipvs->sctp_apps[hash], p_list) {
-		if (inc->port == cp->vport) {
-			if (unlikely(!ip_vs_app_inc_get(inc)))
-				break;
-			rcu_read_unlock();
-
-			IP_VS_DBG_BUF(9, "%s: Binding conn %s:%u->"
-					"%s:%u to app %s on port %u\n",
-					__func__,
-					IP_VS_DBG_ADDR(cp->af, &cp->caddr),
-					ntohs(cp->cport),
-					IP_VS_DBG_ADDR(cp->af, &cp->vaddr),
-					ntohs(cp->vport),
-					inc->name, ntohs(inc->port));
-			cp->app = inc;
-			if (inc->init_conn)
-				result = inc->init_conn(inc, cp);
-			goto out;
-		}
-	}
-	rcu_read_unlock();
-out:
-	return result;
-}
-
-/* ---------------------------------------------
- *   timeouts is netns related now.
- * ---------------------------------------------
- */
-static int __ip_vs_sctp_init(struct net *net, struct ip_vs_proto_data *pd)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	ip_vs_init_hash_table(ipvs->sctp_apps, SCTP_APP_TAB_SIZE);
-	pd->timeout_table = ip_vs_create_timeout_table((int *)sctp_timeouts,
-							sizeof(sctp_timeouts));
-	if (!pd->timeout_table)
-		return -ENOMEM;
-	return 0;
-}
-
-static void __ip_vs_sctp_exit(struct net *net, struct ip_vs_proto_data *pd)
-{
-	kfree(pd->timeout_table);
-}
-
-struct ip_vs_protocol ip_vs_protocol_sctp = {
-	.name		= "SCTP",
-	.protocol	= IPPROTO_SCTP,
-	.num_states	= IP_VS_SCTP_S_LAST,
-	.dont_defrag	= 0,
-	.init		= NULL,
-	.exit		= NULL,
-	.init_netns	= __ip_vs_sctp_init,
-	.exit_netns	= __ip_vs_sctp_exit,
-	.register_app	= sctp_register_app,
-	.unregister_app = sctp_unregister_app,
-	.conn_schedule	= sctp_conn_schedule,
-	.conn_in_get	= ip_vs_conn_in_get_proto,
-	.conn_out_get	= ip_vs_conn_out_get_proto,
-	.snat_handler	= sctp_snat_handler,
-	.dnat_handler	= sctp_dnat_handler,
-	.csum_check	= sctp_csum_check,
-	.state_name	= sctp_state_name,
-	.state_transition = sctp_state_transition,
-	.app_conn_bind	= sctp_app_conn_bind,
-	.debug_packet	= ip_vs_tcpudp_debug_packet,
-	.timeout_change	= NULL,
-};
diff --git a/net/netfilter/ipvs/ip_vs_proto_tcp.c b/net/netfilter/ipvs/ip_vs_proto_tcp.c
index 50a1594..20571b8 100644
--- a/net/netfilter/ipvs/ip_vs_proto_tcp.c
+++ b/net/netfilter/ipvs/ip_vs_proto_tcp.c
@@ -9,12 +9,8 @@
  *              as published by the Free Software Foundation; either version
  *              2 of the License, or (at your option) any later version.
  *
- * Changes:     Hans Schillstrom <hans.schillstrom@ericsson.com>
+ * Changes:
  *
- *              Network name space (netns) aware.
- *              Global data moved to netns i.e struct netns_ipvs
- *              tcp_timeouts table has copy per netns in a hash table per
- *              protocol ip_vs_proto_data and is handled by netns
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -22,43 +18,100 @@
 
 #include <linux/kernel.h>
 #include <linux/ip.h>
-#include <linux/tcp.h>                  /* for tcphdr */
+#include <linux/tcp.h>		/* for tcphdr */
 #include <net/ip.h>
-#include <net/tcp.h>                    /* for csum_tcpudp_magic */
-#include <net/ip6_checksum.h>
+#include <net/tcp.h>		/* for csum_tcpudp_magic */
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
+#include <net/secure_seq.h>
+
+#ifdef CONFIG_IP_VS_IPV6
+#include <net/ipv6.h>
+#include <net/ip6_checksum.h>
+#endif
 
 #include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
+
+static struct ip_vs_conn *tcp_conn_in_get(int af, const struct sk_buff *skb,
+					  struct ip_vs_protocol *pp,
+					  const struct ip_vs_iphdr *iph,
+					  unsigned int proto_off, int inverse,
+					  int *res_dir)
+{
+	__be16 _ports[2], *pptr;
+
+	pptr = skb_header_pointer(skb, proto_off, sizeof(_ports), _ports);
+	if (pptr == NULL)
+		return NULL;
+
+	if (likely(!inverse)) {
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->saddr, pptr[0],
+				      &iph->daddr, pptr[1], res_dir);
+	} else {
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->daddr, pptr[1],
+				      &iph->saddr, pptr[0], res_dir);
+	}
+}
+
+static struct ip_vs_conn *tcp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
+{
+	__be16 _ports[2], *pptr;
+
+	pptr = skb_header_pointer(skb, proto_off, sizeof(_ports), _ports);
+	if (pptr == NULL)
+		return NULL;
+
+	if (likely(!inverse)) {
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->saddr, pptr[0],
+				      &iph->daddr, pptr[1], res_dir);
+	} else {
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->daddr, pptr[1],
+				      &iph->saddr, pptr[0], res_dir);
+	}
+}
 
 static int
-tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
-		  int *verdict, struct ip_vs_conn **cpp,
-		  struct ip_vs_iphdr *iph)
+tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
+		  int *verdict, struct ip_vs_conn **cpp)
 {
-	struct net *net;
 	struct ip_vs_service *svc;
 	struct tcphdr _tcph, *th;
+	struct ip_vs_iphdr iph;
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 
-	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	th = skb_header_pointer(skb, iph.len, sizeof(_tcph), &_tcph);
 	if (th == NULL) {
 		*verdict = NF_DROP;
 		return 0;
 	}
-	net = skb_net(skb);
-	/* No !th->ack check to allow scheduling on SYN+ACK for Active FTP */
-	rcu_read_lock();
-	if (th->syn &&
-	    (svc = ip_vs_service_find(net, af, skb->mark, iph->protocol,
-				      &iph->daddr, th->dest))) {
-		int ignored;
 
-		if (ip_vs_todrop(net_ipvs(net))) {
+	/*
+	 * Syn-proxy step 2 logic: receive client's
+	 * 3-handshake Ack packet
+	 */
+	if (ip_vs_synproxy_ack_rcv(af, skb, th, pp, cpp, &iph, verdict) == 0) {
+		return 0;
+	}
+
+	if (th->syn && !th->ack && !th->fin && !th->rst &&
+	    (svc = ip_vs_service_get(af, skb->mark, iph.protocol, &iph.daddr,
+				     th->dest))) {
+		if (ip_vs_todrop()) {
 			/*
 			 * It seems that we are very loaded.
 			 * We have to drop this packet :(
 			 */
-			rcu_read_unlock();
+			ip_vs_service_put(svc);
 			*verdict = NF_DROP;
 			return 0;
 		}
@@ -67,22 +120,26 @@ tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
 		 * Let the virtual server select a real server for the
 		 * incoming connection, and create a connection entry.
 		 */
-		*cpp = ip_vs_schedule(svc, skb, pd, &ignored, iph);
-		if (!*cpp && ignored <= 0) {
-			if (!ignored)
-				*verdict = ip_vs_leave(svc, skb, pd, iph);
-			else
-				*verdict = NF_DROP;
-			rcu_read_unlock();
+		*cpp = ip_vs_schedule(svc, skb, 0);
+		if (!*cpp) {
+			*verdict = ip_vs_leave(svc, skb, pp);
 			return 0;
 		}
+		ip_vs_service_put(svc);
+		return 1;
+	}
+
+	/* drop tcp packet which send to vip and !vport */
+	if (sysctl_ip_vs_tcp_drop_entry &&
+	    (svc = ip_vs_lookup_vip(af, iph.protocol, &iph.daddr))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_TCP_DROP);
+		*verdict = NF_DROP;
+		return 0;
 	}
-	rcu_read_unlock();
-	/* NF_ACCEPT */
+
 	return 1;
 }
 
-
 static inline void
 tcp_fast_csum_update(int af, struct tcphdr *tcph,
 		     const union nf_inet_addr *oldip,
@@ -92,90 +149,282 @@ tcp_fast_csum_update(int af, struct tcphdr *tcph,
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		tcph->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(tcph->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (tcph->
+								    check))));
 	else
 #endif
-	tcph->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(tcph->check))));
+		tcph->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (tcph->
+								   check))));
 }
 
-
 static inline void
 tcp_partial_csum_update(int af, struct tcphdr *tcph,
-		     const union nf_inet_addr *oldip,
-		     const union nf_inet_addr *newip,
-		     __be16 oldlen, __be16 newlen)
+			const union nf_inet_addr *oldip,
+			const union nf_inet_addr *newip,
+			__be16 oldlen, __be16 newlen)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		tcph->check =
 			~csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldlen, newlen,
+					ip_vs_check_diff2(oldlen, newlen,
 						csum_unfold(tcph->check))));
 	else
 #endif
-	tcph->check =
-		~csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				ip_vs_check_diff2(oldlen, newlen,
+		tcph->check =
+			~csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+					ip_vs_check_diff2(oldlen, newlen,
 						csum_unfold(tcph->check))));
 }
 
+/* Calculate TCP checksum, only for PARTICAL */
+static inline void
+tcp_partial_csum_reset(int af, int len, struct tcphdr *tcph,
+				const union nf_inet_addr *saddr,
+				const union nf_inet_addr *daddr)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		tcph->check = ~csum_ipv6_magic(&saddr->in6, &daddr->in6,
+							len, IPPROTO_TCP, 0);
+        else
+#endif
+		tcph->check = ~tcp_v4_check(len, saddr->ip, daddr->ip, 0);
+}
+
+/* adjust tcp opt mss, sub TCPOLEN_CIP */
+static void tcp_opt_adjust_mss(int af, struct tcphdr *tcph)
+{
+	unsigned char *ptr;
+	int length;
+
+	if (sysctl_ip_vs_mss_adjust_entry == 0)
+		return;
+
+	ptr = (unsigned char *)(tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			if ((opcode == TCPOPT_MSS) && (opsize == TCPOLEN_MSS)) {
+				__be16 old = *(__be16 *) ptr;
+				__u16 in_mss = ntohs(*(__be16 *) ptr);
+#ifdef CONFIG_IP_VS_IPV6
+				if (af == AF_INET6)
+					in_mss -= TCPOLEN_ADDR_V6;
+				else
+#endif
+					in_mss -= TCPOLEN_ADDR;
+				*((__be16 *) ptr) = htons(in_mss);/* set mss, 16bit */
+				tcph->check = csum_fold(ip_vs_check_diff2(old,
+								*(__be16 *) ptr,
+						~csum_unfold(tcph->check)));
+				return;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/* save tcp sequense for fullnat/nat, INside to OUTside */
+static void
+tcp_save_out_seq(struct sk_buff *skb, struct ip_vs_conn *cp,
+		 struct tcphdr *th, int ihl)
+{
+	if (unlikely(th == NULL) || unlikely(cp == NULL) ||
+	    unlikely(skb == NULL))
+		return;
+
+	if (sysctl_ip_vs_conn_expire_tcp_rst && !th->rst) {
+
+		/* seq out of order. just skip */
+		if (before(ntohl(th->ack_seq), ntohl(cp->rs_ack_seq)) &&
+							(cp->rs_ack_seq != 0))
+			return;
+
+		if (th->syn && th->ack)
+			cp->rs_end_seq = htonl(ntohl(th->seq) + 1);
+		else
+			cp->rs_end_seq = htonl(ntohl(th->seq) + skb->len
+					       - ihl - (th->doff << 2));
+		cp->rs_ack_seq = th->ack_seq;
+		IP_VS_DBG_RL("packet from RS, seq:%u ack_seq:%u.",
+			     ntohl(th->seq), ntohl(th->ack_seq));
+		IP_VS_DBG_RL("port:%u->%u", ntohs(th->source), ntohs(th->dest));
+	}
+}
+
+/*
+ * 1. adjust tcp ack/sack sequence for FULL-NAT, INside to OUTside
+ * 2. adjust tcp sequence for SYNPROXY, OUTside to INside
+ */
+static int tcp_out_adjust_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	__u8 i;
+	__u8 *ptr;
+	int length;
+	__be32 old_seq;
+
+	/*
+	 * Syn-proxy seq change, include tcp hdr and
+	 * check ack storm.
+	 */
+	if (ip_vs_synproxy_snat_handler(tcph, cp) == 0) {
+		return 0;
+	}
+
+	/*
+	 * FULLNAT ack-seq change
+	 */
+
+	old_seq = tcph->ack_seq;
+	/* adjust ack sequence */
+	tcph->ack_seq = htonl(ntohl(tcph->ack_seq) - cp->fnat_seq.delta);
+	/* update checksum */
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, tcph->ack_seq,
+						~csum_unfold(tcph->check)));
+
+	/* adjust sack sequence */
+	ptr = (__u8 *) (tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	/* Fast path for timestamp-only option */
+	if (length == TCPOLEN_TSTAMP_ALIGNED &&
+		*(__be32 *) ptr == htonl((TCPOPT_NOP << 24) |
+					(TCPOPT_NOP << 16) |
+					(TCPOPT_TIMESTAMP << 8) |
+					TCPOLEN_TIMESTAMP))
+		return 1;
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return 1;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return 1;
+			if (opsize > length)
+				return 1;	/* don't parse partial options */
+			if ((opcode == TCPOPT_SACK) &&
+			(opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK))
+			&& !((opsize - TCPOLEN_SACK_BASE) %
+						TCPOLEN_SACK_PERBLOCK)) {
+				for (i = 0; i < opsize - TCPOLEN_SACK_BASE;
+						i += TCPOLEN_SACK_PERBLOCK) {
+					__be32 *tmp = (__be32 *) (ptr + i);
+					old_seq = *tmp;
+					*tmp = htonl(ntohl(*tmp) -
+							cp->fnat_seq.delta);
+					tcph->check =
+						csum_fold(ip_vs_check_diff4(
+								old_seq, *tmp,
+						~csum_unfold(tcph->check)));
+
+					tmp++;
+
+					old_seq = *tmp;
+					*tmp = htonl(ntohl(*tmp) -
+							cp->fnat_seq.delta);
+					tcph->check =
+						csum_fold(ip_vs_check_diff4(
+								old_seq, *tmp,
+						~csum_unfold(tcph->check)));
+				}
+				return 1;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+
+	return 1;
+}
 
 static int
-tcp_snat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
+tcp_snat_handler(struct sk_buff *skb,
+		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
 {
 	struct tcphdr *tcph;
-	unsigned int tcphoff = iph->len;
+	unsigned int tcphoff;
 	int oldlen;
-	int payload_csum = 0;
 
 #ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6 && iph->fragoffs)
-		return 1;
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
 #endif
+		tcphoff = ip_hdrlen(skb);
 	oldlen = skb->len - tcphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, tcphoff+sizeof(*tcph)))
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
-		int ret;
-
 		/* Some checks before mangling */
 		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
 			return 0;
 
 		/* Call application helper if needed */
-		if (!(ret = ip_vs_app_pkt_out(cp, skb)))
+		if (!ip_vs_app_pkt_out(cp, skb))
 			return 0;
-		/* ret=2: csum update is needed after payload mangling */
-		if (ret == 1)
-			oldlen = skb->len - tcphoff;
-		else
-			payload_csum = 1;
 	}
 
 	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcp_save_out_seq(skb, cp, tcph, tcphoff);
 	tcph->source = cp->vport;
 
+	/*
+	 * Syn-proxy seq change, include tcp hdr and
+	 * check ack storm.
+	 */
+	if (ip_vs_synproxy_snat_handler(tcph, cp) == 0) {
+		return 0;
+	}
+
 	/* Adjust TCP checksums */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		tcp_partial_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
-					htons(oldlen),
-					htons(skb->len - tcphoff));
-	} else if (!payload_csum) {
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->vaddr, &cp->caddr);
+	} else if (!cp->app) {
 		/* Only port and addr are changed, do fast csum update */
 		tcp_fast_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
 				     cp->dport, cp->vport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
-			skb->ip_summed = (cp->app && pp->csum_check) ?
-					 CHECKSUM_UNNECESSARY : CHECKSUM_NONE;
+			skb->ip_summed = CHECKSUM_NONE;
 	} else {
 		/* full checksum calculation */
 		tcph->check = 0;
@@ -193,72 +442,519 @@ tcp_snat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
 							skb->len - tcphoff,
 							cp->protocol,
 							skb->csum);
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
 
 		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
 			  pp->name, tcph->check,
-			  (char*)&(tcph->check) - (char*)tcph);
+			  (char *)&(tcph->check) - (char *)tcph);
 	}
 	return 1;
 }
 
+/*
+ * init first data sequence, INside to OUTside;
+ */
+static inline void
+tcp_out_init_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	cp->fnat_seq.fdata_seq = ntohl(tcph->seq) + 1;
+}
+
 
 static int
-tcp_dnat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
+tcp_fnat_out_handler(struct sk_buff *skb,
+		     struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
 {
 	struct tcphdr *tcph;
-	unsigned int tcphoff = iph->len;
+	unsigned int tcphoff;
 	int oldlen;
-	int payload_csum = 0;
 
 #ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6 && iph->fragoffs)
-		return 1;
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
 #endif
+		tcphoff = ip_hdrlen(skb);
 	oldlen = skb->len - tcphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, tcphoff+sizeof(*tcph)))
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
-		int ret;
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/* Call application helper if needed */
+		if (!ip_vs_app_pkt_out(cp, skb))
+			return 0;
+	}
+
+	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcp_save_out_seq(skb, cp, tcph, tcphoff);
+	tcph->source = cp->vport;
+	tcph->dest = cp->cport;
+
+	/*
+	 * for syn_ack
+	 * 1. adjust tcp opt mss in rs->client
+	 */
+	if (tcph->syn && tcph->ack) {
+		tcp_opt_adjust_mss(cp->af, tcph);
+	}
+
+	/* adjust tcp ack/sack sequence */
+	if (tcp_out_adjust_seq(cp, tcph) == 0) {
+		return 0;
+	}
+
+	/*
+	 * for syn_ack
+	 * 2. init sequence
+	 */
+	if (tcph->syn && tcph->ack) {
+		tcp_out_init_seq(cp, tcph);
+	}
+
+	/* Adjust TCP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->vaddr, &cp->caddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		tcp_fast_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
+				     cp->dport, cp->vport);
+		tcp_fast_csum_update(cp->af, tcph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		tcph->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcph->check = csum_ipv6_magic(&cp->vaddr.in6,
+						      &cp->caddr.in6,
+						      skb->len - tcphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			tcph->check = csum_tcpudp_magic(cp->vaddr.ip,
+							cp->caddr.ip,
+							skb->len - tcphoff,
+							cp->protocol, skb->csum);
+
+		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
+			pp->name, tcph->check,
+			(char *)&(tcph->check) - (char *)tcph);
+	}
+	return 1;
+}
+
+/*
+ * remove tcp timestamp opt in one packet, just set it to TCPOPT_NOP
+ * reference to tcp_parse_options in tcp_input.c
+ */
+static void tcp_opt_remove_timestamp(struct tcphdr *tcph)
+{
+	unsigned char *ptr;
+	__be32 old[4], new[4];
+	int length;
+	int i;
+
+	if (sysctl_ip_vs_timestamp_remove_entry == 0)
+		return;
+
+	ptr = (unsigned char *)(tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			if ((opcode == TCPOPT_TIMESTAMP)
+			    && (opsize == TCPOLEN_TIMESTAMP)) {
+				/* the length of buf is 16Byte,
+				 * but data is 10Byte. zero the buf
+				 */
+				memset((__u8*)old, 0, sizeof(old));
+				memset((__u8*)new, 0, sizeof(new));
+				memcpy((__u8*)old, ptr - 2, TCPOLEN_TIMESTAMP);
+				for (i = 0; i < TCPOLEN_TIMESTAMP; i++) {
+					*(ptr - 2 + i) = TCPOPT_NOP;	/* TCPOPT_NOP replace timestamp opt */
+				}
+				memcpy((__u8*)new, ptr - 2, TCPOLEN_TIMESTAMP);
+#ifdef CONFIG_IP_VS_IPV6
+				tcph->check = csum_fold(ip_vs_check_diff16(
+								old, new,
+						~csum_unfold(tcph->check)));
+#endif
+				return;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * recompute tcp sequence, OUTside to INside;
+ */
+static void
+tcp_in_init_seq(struct ip_vs_conn *cp, struct sk_buff *skb, struct tcphdr *tcph)
+{
+	struct ip_vs_seq *fseq = &(cp->fnat_seq);
+	__u32 seq = ntohl(tcph->seq);
+	int conn_reused_entry;
+
+	if ((fseq->delta == fseq->init_seq - seq) && (fseq->init_seq != 0)) {
+		/* retransmit */
+		return;
+	}
+
+	/* init syn seq, lvs2rs */
+	conn_reused_entry = (sysctl_ip_vs_conn_reused_entry == 1)
+	    && (fseq->init_seq != 0)
+	    && ((cp->state == IP_VS_TCP_S_SYN_RECV)
+		|| (cp->state == IP_VS_TCP_S_SYN_SENT));
+	if ((fseq->init_seq == 0) || conn_reused_entry) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			fseq->init_seq =
+			    secure_tcpv6_sequence_number(cp->laddr.ip6,
+							 cp->daddr.ip6,
+							 cp->lport, cp->dport);
+		else
+#endif
+			fseq->init_seq =
+			    secure_tcp_sequence_number(cp->laddr.ip,
+						       cp->daddr.ip, cp->lport,
+						       cp->dport);
+		fseq->delta = fseq->init_seq - seq;
+
+		if (conn_reused_entry) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_CONN_REUSED);
+			switch (cp->old_state) {
+			case IP_VS_TCP_S_CLOSE:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_CLOSE);
+				break;
+			case IP_VS_TCP_S_TIME_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_TIMEWAIT);
+				break;
+			case IP_VS_TCP_S_FIN_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_FINWAIT);
+				break;
+			case IP_VS_TCP_S_CLOSE_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_CLOSEWAIT);
+				break;
+			case IP_VS_TCP_S_LAST_ACK:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_LASTACK);
+				break;
+			case IP_VS_TCP_S_ESTABLISHED:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_ESTAB);
+				break;
+			}
+		}
+	}
+}
+
+/* adjust tcp sequence, OUTside to INside */
+static void tcp_in_adjust_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	__be32 old_seq = tcph->seq;
+	/* adjust seq for FULLNAT */
+	tcph->seq = htonl(ntohl(tcph->seq) + cp->fnat_seq.delta);
+	/* update checksum */
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, tcph->seq,
+					~csum_unfold(tcph->check)));
+
+	/* adjust ack_seq for SYNPROXY, include tcp hdr and sack opt */
+	ip_vs_synproxy_dnat_handler(tcph, &cp->syn_proxy_seq);
+}
+
+/*
+ * add client address in tcp option
+ * alloc a new skb, and free the old skb
+ * return new skb
+ */
+static struct sk_buff *tcp_opt_add_toa(struct ip_vs_conn *cp,
+				       struct sk_buff *old_skb,
+				       struct tcphdr **tcph)
+{
+	__u32 mtu;
+	struct sk_buff *new_skb = NULL;
+	struct ip_vs_tcpo_addr *toa;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	__u8 *p, *q;
+
+	/* now only process IPV4 */
+	if (cp->af != AF_INET) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_PROTO);
+		return old_skb;
+	}
+
+	/* skb length and tcp option length checking */
+	mtu = dst_mtu((struct dst_entry *)skb_dst(old_skb));
+	if (old_skb->len > (mtu - sizeof(struct ip_vs_tcpo_addr))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_LEN);
+		return old_skb;
+	}
+
+	/* the maximum length of TCP head is 60 bytes, so only 40 bytes for options */
+	if ((60 - ((*tcph)->doff << 2)) < sizeof(struct ip_vs_tcpo_addr)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_HEAD_FULL);
+		return old_skb;
+	}
+
+	/* copy all skb, plus ttm space , new skb is linear */
+	new_skb = skb_copy_expand(old_skb,
+				  skb_headroom(old_skb),
+				  skb_tailroom(old_skb) +
+				  sizeof(struct ip_vs_tcpo_addr), GFP_ATOMIC);
+	if (new_skb == NULL) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_MEM);
+		return old_skb;
+	}
+
+	/* free old skb */
+	kfree_skb(old_skb);
+
+	/*
+	 * add client ip
+	 */
+	tcphoff = ip_hdrlen(new_skb);
+	/* get new tcp header */
+	*tcph = th =
+	    (struct tcphdr *)((void *)skb_network_header(new_skb) + tcphoff);
+
+	/* ptr to old opts */
+	p = skb_tail_pointer(new_skb) - 1;
+	q = p + sizeof(struct ip_vs_tcpo_addr);
+
+	/* move data down, offset is sizeof(struct ip_vs_tcpo_addr) */
+	while (p >= ((__u8 *) th + sizeof(struct tcphdr))) {
+		*q = *p;
+		p--;
+		q--;
+	}
+
+	/* move tail to new postion */
+	new_skb->tail += sizeof(struct ip_vs_tcpo_addr);
+
+	/* put client ip opt , ptr point to opts */
+	toa = (struct ip_vs_tcpo_addr *)(th + 1);
+	toa->opcode = TCPOPT_ADDR;
+	toa->opsize = TCPOLEN_ADDR;
+	toa->port = cp->cport;
+	toa->addr = cp->caddr.ip;
+
+	/* reset tcp header length */
+	th->doff += sizeof(struct ip_vs_tcpo_addr) / 4;
+	/* reset ip header totoal length */
+	ip_hdr(new_skb)->tot_len =
+	    htons(ntohs(ip_hdr(new_skb)->tot_len) +
+		  sizeof(struct ip_vs_tcpo_addr));
+	/* reset skb length */
+	new_skb->len += sizeof(struct ip_vs_tcpo_addr);
+
+	/* re-calculate tcp csum */
+	th->check = 0;
+	new_skb->csum = skb_checksum(new_skb, tcphoff,
+					new_skb->len - tcphoff, 0);
+	th->check = csum_tcpudp_magic(cp->caddr.ip,
+					cp->vaddr.ip,
+					new_skb->len - tcphoff,
+					cp->protocol, new_skb->csum);
+
+	/* re-calculate ip head csum, tot_len has been adjusted */
+	ip_send_check(ip_hdr(new_skb));
+
+	if(new_skb->ip_summed == CHECKSUM_PARTIAL) {
+		new_skb->ip_summed = CHECKSUM_COMPLETE;
+		skb_shinfo(new_skb)->gso_size = 0;
+	}
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_OK);
+
+	return new_skb;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static struct sk_buff *tcp_opt_add_toa_v6(struct ip_vs_conn *cp,
+				       struct sk_buff *old_skb,
+				       struct tcphdr **tcph)
+{
+	__u32 mtu;
+	struct sk_buff *new_skb = NULL;
+	struct ip_vs_tcpo_addr_v6 *toa;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	__u8 *p, *q;
+
+	/* IPV6 */
+	if (cp->af != AF_INET6) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_PROTO);
+		return old_skb;
+	}
+
+	/* skb length and tcph length checking */
+	mtu = dst_mtu((struct dst_entry *)skb_dst(old_skb));
+	if (old_skb->len > (mtu - sizeof(struct ip_vs_tcpo_addr_v6))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_LEN);
+		return old_skb;
+	}
+
+	/* the maximum length of TCP head is 60 bytes, so only 40 bytes for options */
+	if ((60 - ((*tcph)->doff << 2)) < sizeof(struct ip_vs_tcpo_addr_v6)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_HEAD_FULL);
+		return old_skb;
+	}
+
+	/* copy all skb, plus ttm space , new skb is linear */
+	new_skb = skb_copy_expand(old_skb,
+				  skb_headroom(old_skb),
+				  skb_tailroom(old_skb) +
+				  sizeof(struct ip_vs_tcpo_addr_v6), GFP_ATOMIC);
+	if (new_skb == NULL) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_MEM);
+		return old_skb;
+	}
+
+	/* free old skb */
+	kfree_skb(old_skb);
+
+	/*
+	 * add client ip
+	 */
+	tcphoff = sizeof(struct ipv6hdr);
+	/* get new tcp header */
+	*tcph = th =
+	    (struct tcphdr *)((void *)skb_network_header(new_skb) + tcphoff);
+
+	/* ptr to old opts */
+	p = skb_tail_pointer(new_skb) - 1;
+	q = p + sizeof(struct ip_vs_tcpo_addr_v6);
+
+	/* move data down, offset is sizeof(struct ip_vs_tcpo_addr) */
+	while (p >= ((__u8 *) th + sizeof(struct tcphdr))) {
+		*q = *p;
+		p--;
+		q--;
+	}
+
+	/* move tail to new postion */
+	new_skb->tail += sizeof(struct ip_vs_tcpo_addr_v6);
+
+	/* put client ip opt , ptr point to opts */
+	toa = (struct ip_vs_tcpo_addr_v6 *)(th + 1);
+	toa->opcode = TCPOPT_ADDR_V6;
+	toa->opsize = TCPOLEN_ADDR_V6;
+	toa->port = cp->cport;
+	toa->addr = cp->caddr.in6;
+
+	/* reset tcp header length */
+	th->doff += sizeof(struct ip_vs_tcpo_addr_v6) >> 2;
+	/* reset ip header totoal length */
+	ipv6_hdr(new_skb)->payload_len =
+	    htons(ntohs(ipv6_hdr(new_skb)->payload_len) +
+		  sizeof(struct ip_vs_tcpo_addr_v6));
+	/* reset skb length */
+	new_skb->len += sizeof(struct ip_vs_tcpo_addr_v6);
+
+	/* re-calculate tcp csum */
+	th->check = 0;
+	new_skb->csum = skb_checksum(new_skb, tcphoff,
+					new_skb->len - tcphoff, 0);
+	th->check = csum_ipv6_magic(&cp->caddr.in6,
+					&cp->vaddr.in6,
+					new_skb->len - tcphoff,
+					cp->protocol, new_skb->csum);
+
+	if(new_skb->ip_summed == CHECKSUM_PARTIAL) {
+		new_skb->ip_summed = CHECKSUM_COMPLETE;
+		skb_shinfo(new_skb)->gso_size = 0;
+	}
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_OK);
+
+	return new_skb;
+}
+#endif
+
+static int
+tcp_dnat_handler(struct sk_buff *skb,
+		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct tcphdr *tcph;
+	unsigned int tcphoff;
+	int oldlen;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+	oldlen = skb->len - tcphoff;
 
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
 		/* Some checks before mangling */
 		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
 			return 0;
 
 		/*
-		 *	Attempt ip_vs_app call.
-		 *	It will fix ip_vs_conn and iph ack_seq stuff
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
 		 */
-		if (!(ret = ip_vs_app_pkt_in(cp, skb)))
+		if (!ip_vs_app_pkt_in(cp, skb))
 			return 0;
-		/* ret=2: csum update is needed after payload mangling */
-		if (ret == 1)
-			oldlen = skb->len - tcphoff;
-		else
-			payload_csum = 1;
 	}
 
 	tcph = (void *)skb_network_header(skb) + tcphoff;
 	tcph->dest = cp->dport;
 
 	/*
-	 *	Adjust TCP checksums
+	 * Syn-proxy ack_seq change, include tcp hdr and sack opt.
+	 */
+	ip_vs_synproxy_dnat_handler(tcph, &cp->syn_proxy_seq);
+
+	/*
+	 *      Adjust TCP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		tcp_partial_csum_update(cp->af, tcph, &cp->vaddr, &cp->daddr,
-					htons(oldlen),
-					htons(skb->len - tcphoff));
-	} else if (!payload_csum) {
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->caddr, &cp->daddr);
+	} else if (!cp->app) {
 		/* Only port and addr are changed, do fast csum update */
 		tcp_fast_csum_update(cp->af, tcph, &cp->vaddr, &cp->daddr,
 				     cp->vport, cp->dport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
-			skb->ip_summed = (cp->app && pp->csum_check) ?
-					 CHECKSUM_UNNECESSARY : CHECKSUM_NONE;
+			skb->ip_summed = CHECKSUM_NONE;
 	} else {
 		/* full checksum calculation */
 		tcph->check = 0;
@@ -281,6 +977,360 @@ tcp_dnat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
 	return 1;
 }
 
+static int
+tcp_fnat_in_handler(struct sk_buff **skb_p,
+		    struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct tcphdr *tcph;
+	unsigned int tcphoff;
+	int oldlen;
+	struct sk_buff *skb = *skb_p;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+	oldlen = skb->len - tcphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
+		 */
+		if (!ip_vs_app_pkt_in(cp, skb))
+			return 0;
+	}
+
+	tcph = (void *)skb_network_header(skb) + tcphoff;
+	/*
+	 * for syn packet
+	 * 1. remove tcp timestamp opt,
+	 *    because local address with diffrent client have the diffrent timestamp;
+	 * 2. recompute tcp sequence
+	 * 3. add toa
+	 */
+	if (tcph->syn & !tcph->ack) {
+		tcp_opt_remove_timestamp(tcph);
+		tcp_in_init_seq(cp, skb, tcph);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			skb = *skb_p = tcp_opt_add_toa_v6(cp, skb, &tcph);
+		else
+#endif
+			skb = *skb_p = tcp_opt_add_toa(cp, skb, &tcph);
+	}
+
+	/* TOA: add client ip */
+	if ((sysctl_ip_vs_toa_entry == 1)
+	    && (ntohl(tcph->ack_seq) == cp->fnat_seq.fdata_seq)
+	    && !tcph->syn && !tcph->rst && !tcph->fin) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			skb = *skb_p = tcp_opt_add_toa_v6(cp, skb, &tcph);
+		else
+#endif
+			skb = *skb_p = tcp_opt_add_toa(cp, skb, &tcph);
+	}
+
+	/*
+	 * adjust tcp sequence, becase
+	 * 1. FULLNAT: local address with diffrent client have the diffrent sequence
+	 * 2. SYNPROXY: dont know rs->client synack sequence
+	 */
+	tcp_in_adjust_seq(cp, tcph);
+
+	/* adjust src/dst port */
+	tcph->source = cp->lport;
+	tcph->dest = cp->dport;
+
+	/* Adjust TCP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->laddr, &cp->daddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		tcp_fast_csum_update(cp->af, tcph, &cp->vaddr, &cp->daddr,
+				     cp->vport, cp->dport);
+		tcp_fast_csum_update(cp->af, tcph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		tcph->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcph->check = csum_ipv6_magic(&cp->laddr.in6,
+						      &cp->daddr.in6,
+						      skb->len - tcphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			tcph->check = csum_tcpudp_magic(cp->laddr.ip,
+							cp->daddr.ip,
+							skb->len - tcphoff,
+							cp->protocol, skb->csum);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	}
+	return 1;
+}
+
+/* send reset packet to RS */
+static void tcp_send_rst_in(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct sk_buff *skb = NULL;
+	struct sk_buff *tmp_skb = NULL;
+	struct tcphdr *th;
+	unsigned int tcphoff;
+
+	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send rs RST packet\n");
+		return;
+	}
+
+	skb_reserve(skb, MAX_TCP_HEADER);
+	th = (struct tcphdr *)skb_push(skb, sizeof(struct tcphdr));
+	skb_reset_transport_header(skb);
+	skb->csum = 0;
+
+	/* set tcp head */
+	memset(th, 0, sizeof(struct tcphdr));
+	th->source = cp->cport;
+	th->dest = cp->vport;
+
+	/* set the reset seq of tcp head */
+	if ((cp->state == IP_VS_TCP_S_SYN_SENT) &&
+			((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL)) {
+		struct tcphdr *tcph;
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdrlen(tmp_skb);
+		tcph = (void *)skb_network_header(tmp_skb) + tcphoff;
+
+		th->seq = tcph->seq;
+		/* put back. Just for sending reset packet to client */
+		skb_queue_head(&cp->ack_skb, tmp_skb);
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_IN_SYN_SENT);
+	} else if (cp->state == IP_VS_TCP_S_ESTABLISHED) {
+		th->seq = cp->rs_ack_seq;
+		/* Be careful! fullnat */
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			th->seq = htonl(ntohl(th->seq) - cp->fnat_seq.delta);
+
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_IN_ESTABLISHED);
+	} else {
+		kfree_skb(skb);
+		IP_VS_DBG_RL("IPVS: Is SYN_SENT or ESTABLISHED ?");
+		return;
+	}
+
+	IP_VS_DBG_RL("IPVS: rst to rs seq: %u", htonl(th->seq));
+	th->ack_seq = 0;
+	th->doff = sizeof(struct tcphdr) >> 2;
+	th->rst = 1;
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on packet_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6) {
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(skb, sizeof(struct ipv6hdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(skb);
+		memcpy(&iph->saddr, &cp->caddr.in6, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &cp->vaddr.in6, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+		iph->payload_len = htons(sizeof(struct tcphdr));
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tot_len = htons(skb->len);
+		iph->frag_off = htons(IP_DF);
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = cp->caddr.ip;
+		iph->daddr = cp->vaddr.ip;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+	}
+
+	cp->packet_xmit(skb, cp, pp);
+}
+
+/* send reset packet to client */
+static void tcp_send_rst_out(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct sk_buff *skb = NULL;
+	struct sk_buff *tmp_skb = NULL;
+	struct tcphdr *th;
+	unsigned int tcphoff;
+
+	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send client RST packet\n");
+		return;
+	}
+
+	skb_reserve(skb, MAX_TCP_HEADER);
+	th = (struct tcphdr *)skb_push(skb, sizeof(struct tcphdr));
+	skb_reset_transport_header(skb);
+	skb->csum = 0;
+
+	/* set tcp head */
+	memset(th, 0, sizeof(struct tcphdr));
+	th->source = cp->dport;
+	if (cp->flags & IP_VS_CONN_F_FULLNAT)
+		th->dest = cp->lport;
+	else
+		th->dest = cp->cport;
+
+	/* set the reset seq of tcp head*/
+	if ((cp->state == IP_VS_TCP_S_SYN_SENT) &&
+			((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL)) {
+		struct tcphdr *tcph;
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdrlen(tmp_skb);
+		tcph = (void *)skb_network_header(tmp_skb) + tcphoff;
+		/* Perhaps delta is 0 */
+		th->seq = htonl(ntohl(tcph->ack_seq) - cp->syn_proxy_seq.delta);
+		/* put back. Just for sending reset packet to RS */
+		skb_queue_head(&cp->ack_skb, tmp_skb);
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_OUT_SYN_SENT);
+	} else if (cp->state == IP_VS_TCP_S_ESTABLISHED) {
+		th->seq = cp->rs_end_seq;
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_OUT_ESTABLISHED);
+	} else {
+		kfree_skb(skb);
+		IP_VS_DBG_RL("IPVS: Is in SYN_SENT or ESTABLISHED ?");
+		return;
+	}
+
+	IP_VS_DBG_RL("IPVS: rst to client seq: %u", htonl(th->seq));
+	th->ack_seq = 0;
+	th->doff = sizeof(struct tcphdr) >> 2;
+	th->rst = 1;
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on response_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6) {
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(skb, sizeof(struct ipv6hdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(skb);
+		memcpy(&iph->saddr, &cp->daddr.in6, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &cp->laddr.in6, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+		iph->payload_len = htons(sizeof(struct tcphdr));
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			ip_vs_fnat_response_xmit_v6(skb, pp, cp,
+						    sizeof(struct ipv6hdr));
+		else
+			ip_vs_normal_response_xmit_v6(skb, pp, cp,
+						      sizeof(struct ipv6hdr));
+	} else
+#endif
+	{
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tot_len = htons(skb->len);
+		iph->frag_off = htons(IP_DF);
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = cp->daddr.ip;
+		iph->daddr = cp->laddr.ip;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			ip_vs_fnat_response_xmit(skb, pp, cp, iph->ihl << 2);
+		else
+			ip_vs_normal_response_xmit(skb, pp, cp, iph->ihl << 2);
+	}
+}
+
+static void
+tcp_conn_expire_handler(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	/* support fullnat and nat */
+	if (sysctl_ip_vs_conn_expire_tcp_rst &&
+	    (cp->flags & (IP_VS_CONN_F_FULLNAT | IP_VS_CONN_F_MASQ))) {
+		/* send reset packet to RS */
+		tcp_send_rst_in(pp, cp);
+		/* send reset packet to client */
+		tcp_send_rst_out(pp, cp);
+	}
+}
 
 static int
 tcp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
@@ -305,21 +1355,20 @@ tcp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
 					    skb->len - tcphoff,
 					    ipv6_hdr(skb)->nexthdr,
 					    skb->csum)) {
-				IP_VS_DBG_RL_PKT(0, af, pp, skb, 0,
+				IP_VS_DBG_RL_PKT(0, pp, skb, 0,
 						 "Failed checksum for");
 				return 0;
 			}
 		} else
 #endif
-			if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
+		if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
 					      ip_hdr(skb)->daddr,
 					      skb->len - tcphoff,
 					      ip_hdr(skb)->protocol,
 					      skb->csum)) {
-				IP_VS_DBG_RL_PKT(0, af, pp, skb, 0,
-						 "Failed checksum for");
-				return 0;
-			}
+			IP_VS_DBG_RL_PKT(0, pp, skb, 0, "Failed checksum for");
+			return 0;
+		}
 		break;
 	default:
 		/* No need to checksum. */
@@ -329,48 +1378,47 @@ tcp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
 	return 1;
 }
 
-
 #define TCP_DIR_INPUT		0
 #define TCP_DIR_OUTPUT		4
 #define TCP_DIR_INPUT_ONLY	8
 
 static const int tcp_state_off[IP_VS_DIR_LAST] = {
-	[IP_VS_DIR_INPUT]		=	TCP_DIR_INPUT,
-	[IP_VS_DIR_OUTPUT]		=	TCP_DIR_OUTPUT,
-	[IP_VS_DIR_INPUT_ONLY]		=	TCP_DIR_INPUT_ONLY,
+	[IP_VS_DIR_INPUT] = TCP_DIR_INPUT,
+	[IP_VS_DIR_OUTPUT] = TCP_DIR_OUTPUT,
+	[IP_VS_DIR_INPUT_ONLY] = TCP_DIR_INPUT_ONLY,
 };
 
 /*
  *	Timeout table[state]
  */
-static const int tcp_timeouts[IP_VS_TCP_S_LAST+1] = {
-	[IP_VS_TCP_S_NONE]		=	2*HZ,
-	[IP_VS_TCP_S_ESTABLISHED]	=	15*60*HZ,
-	[IP_VS_TCP_S_SYN_SENT]		=	2*60*HZ,
-	[IP_VS_TCP_S_SYN_RECV]		=	1*60*HZ,
-	[IP_VS_TCP_S_FIN_WAIT]		=	2*60*HZ,
-	[IP_VS_TCP_S_TIME_WAIT]		=	2*60*HZ,
-	[IP_VS_TCP_S_CLOSE]		=	10*HZ,
-	[IP_VS_TCP_S_CLOSE_WAIT]	=	60*HZ,
-	[IP_VS_TCP_S_LAST_ACK]		=	30*HZ,
-	[IP_VS_TCP_S_LISTEN]		=	2*60*HZ,
-	[IP_VS_TCP_S_SYNACK]		=	120*HZ,
-	[IP_VS_TCP_S_LAST]		=	2*HZ,
+int sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST + 1] = {
+	[IP_VS_TCP_S_NONE] = 2 * HZ,
+	[IP_VS_TCP_S_ESTABLISHED] = 90 * HZ,
+	[IP_VS_TCP_S_SYN_SENT] = 3 * HZ,
+	[IP_VS_TCP_S_SYN_RECV] = 30 * HZ,
+	[IP_VS_TCP_S_FIN_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_TIME_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_CLOSE] = 3 * HZ,
+	[IP_VS_TCP_S_CLOSE_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_LAST_ACK] = 3 * HZ,
+	[IP_VS_TCP_S_LISTEN] = 2 * 60 * HZ,
+	[IP_VS_TCP_S_SYNACK] = 30 * HZ,
+	[IP_VS_TCP_S_LAST] = 2 * HZ,
 };
 
-static const char *const tcp_state_name_table[IP_VS_TCP_S_LAST+1] = {
-	[IP_VS_TCP_S_NONE]		=	"NONE",
-	[IP_VS_TCP_S_ESTABLISHED]	=	"ESTABLISHED",
-	[IP_VS_TCP_S_SYN_SENT]		=	"SYN_SENT",
-	[IP_VS_TCP_S_SYN_RECV]		=	"SYN_RECV",
-	[IP_VS_TCP_S_FIN_WAIT]		=	"FIN_WAIT",
-	[IP_VS_TCP_S_TIME_WAIT]		=	"TIME_WAIT",
-	[IP_VS_TCP_S_CLOSE]		=	"CLOSE",
-	[IP_VS_TCP_S_CLOSE_WAIT]	=	"CLOSE_WAIT",
-	[IP_VS_TCP_S_LAST_ACK]		=	"LAST_ACK",
-	[IP_VS_TCP_S_LISTEN]		=	"LISTEN",
-	[IP_VS_TCP_S_SYNACK]		=	"SYNACK",
-	[IP_VS_TCP_S_LAST]		=	"BUG!",
+static const char *const tcp_state_name_table[IP_VS_TCP_S_LAST + 1] = {
+	[IP_VS_TCP_S_NONE] = "NONE",
+	[IP_VS_TCP_S_ESTABLISHED] = "ESTABLISHED",
+	[IP_VS_TCP_S_SYN_SENT] = "SYN_SENT",
+	[IP_VS_TCP_S_SYN_RECV] = "SYN_RECV",
+	[IP_VS_TCP_S_FIN_WAIT] = "FIN_WAIT",
+	[IP_VS_TCP_S_TIME_WAIT] = "TIME_WAIT",
+	[IP_VS_TCP_S_CLOSE] = "CLOSE",
+	[IP_VS_TCP_S_CLOSE_WAIT] = "CLOSE_WAIT",
+	[IP_VS_TCP_S_LAST_ACK] = "LAST_ACK",
+	[IP_VS_TCP_S_LISTEN] = "LISTEN",
+	[IP_VS_TCP_S_SYNACK] = "SYNACK",
+	[IP_VS_TCP_S_LAST] = "BUG!",
 };
 
 #define sNO IP_VS_TCP_S_NONE
@@ -389,70 +1437,78 @@ struct tcp_states_t {
 	int next_state[IP_VS_TCP_S_LAST];
 };
 
-static const char * tcp_state_name(int state)
+static const char *tcp_state_name(int state)
 {
 	if (state >= IP_VS_TCP_S_LAST)
 		return "ERR!";
 	return tcp_state_name_table[state] ? tcp_state_name_table[state] : "?";
 }
 
-static struct tcp_states_t tcp_states [] = {
+static struct tcp_states_t tcp_states[] = {
 /*	INPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR }},
-/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sTW }},
-/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sSR }},
+/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR}},
+/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sTW}},
+/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sSR}},
 
 /*	OUTPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSS, sES, sSS, sSR, sSS, sSS, sSS, sSS, sSS, sLI, sSR }},
-/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW }},
-/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES }},
-/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL }},
+/*syn*/ {{sSS, sES, sSS, sSR, sSS, sSS, sSS, sSS, sSS, sLI, sSR}},
+/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW}},
+/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES}},
+/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL}},
 
 /*	INPUT-ONLY */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR }},
-/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW }},
-/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL }},
+/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR}},
+/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW}},
+/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL}},
 };
 
-static struct tcp_states_t tcp_states_dos [] = {
+static struct tcp_states_t tcp_states_dos[] = {
 /*	INPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSA }},
-/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sSA }},
-/*ack*/ {{sCL, sES, sSS, sSR, sFW, sTW, sCL, sCW, sCL, sLI, sSA }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL }},
+/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSA}},
+/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sSA}},
+/*ack*/ {{sCL, sES, sSS, sSR, sFW, sTW, sCL, sCW, sCL, sLI, sSA}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL}},
 
 /*	OUTPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSS, sES, sSS, sSA, sSS, sSS, sSS, sSS, sSS, sLI, sSA }},
-/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW }},
-/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES }},
-/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL }},
+/*syn*/ {{sSS, sES, sSS, sSA, sSS, sSS, sSS, sSS, sSS, sLI, sSA}},
+/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW}},
+/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES}},
+/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL}},
 
 /*	INPUT-ONLY */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSA, sES, sES, sSR, sSA, sSA, sSA, sSA, sSA, sSA, sSA }},
-/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW }},
-/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL }},
+/*syn*/ {{sSA, sES, sES, sSR, sSA, sSA, sSA, sSA, sSA, sSA, sSA}},
+/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW}},
+/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL}},
 };
 
-static void tcp_timeout_change(struct ip_vs_proto_data *pd, int flags)
+static struct tcp_states_t *tcp_state_table = tcp_states;
+
+static void tcp_timeout_change(struct ip_vs_protocol *pp, int flags)
 {
-	int on = (flags & 1);		/* secure_tcp */
+	int on = (flags & 1);	/* secure_tcp */
 
 	/*
-	** FIXME: change secure_tcp to independent sysctl var
-	** or make it per-service or per-app because it is valid
-	** for most if not for all of the applications. Something
-	** like "capabilities" (flags) for each object.
-	*/
-	pd->tcp_state_table = (on ? tcp_states_dos : tcp_states);
+	 ** FIXME: change secure_tcp to independent sysctl var
+	 ** or make it per-service or per-app because it is valid
+	 ** for most if not for all of the applications. Something
+	 ** like "capabilities" (flags) for each object.
+	 */
+	tcp_state_table = (on ? tcp_states_dos : tcp_states);
+}
+
+static int tcp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
+{
+	return ip_vs_set_state_timeout(pp->timeout_table, IP_VS_TCP_S_LAST,
+				       tcp_state_name_table, sname, to);
 }
 
 static inline int tcp_state_idx(struct tcphdr *th)
@@ -469,7 +1525,7 @@ static inline int tcp_state_idx(struct tcphdr *th)
 }
 
 static inline void
-set_tcp_state(struct ip_vs_proto_data *pd, struct ip_vs_conn *cp,
+set_tcp_state(struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
 	      int direction, struct tcphdr *th)
 {
 	int state_idx;
@@ -493,15 +1549,15 @@ set_tcp_state(struct ip_vs_proto_data *pd, struct ip_vs_conn *cp,
 	}
 
 	new_state =
-		pd->tcp_state_table[state_off+state_idx].next_state[cp->state];
+	    tcp_state_table[state_off + state_idx].next_state[cp->state];
 
-  tcp_state_out:
+      tcp_state_out:
 	if (new_state != cp->state) {
 		struct ip_vs_dest *dest = cp->dest;
 
 		IP_VS_DBG_BUF(8, "%s %s [%c%c%c%c] %s:%d->"
 			      "%s:%d state: %s->%s conn->refcnt:%d\n",
-			      pd->pp->name,
+			      pp->name,
 			      ((state_off == TCP_DIR_OUTPUT) ?
 			       "output " : "input "),
 			      th->syn ? 'S' : '.',
@@ -531,19 +1587,16 @@ set_tcp_state(struct ip_vs_proto_data *pd, struct ip_vs_conn *cp,
 		}
 	}
 
-	if (likely(pd))
-		cp->timeout = pd->timeout_table[cp->state = new_state];
-	else	/* What to do ? */
-		cp->timeout = tcp_timeouts[cp->state = new_state];
+	cp->old_state = cp->state;	// old_state called when connection reused
+	cp->timeout = pp->timeout_table[cp->state = new_state];
 }
 
 /*
  *	Handle state transitions
  */
-static void
+static int
 tcp_state_transition(struct ip_vs_conn *cp, int direction,
-		     const struct sk_buff *skb,
-		     struct ip_vs_proto_data *pd)
+		     const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
 	struct tcphdr _tcph, *th;
 
@@ -555,59 +1608,65 @@ tcp_state_transition(struct ip_vs_conn *cp, int direction,
 
 	th = skb_header_pointer(skb, ihl, sizeof(_tcph), &_tcph);
 	if (th == NULL)
-		return;
+		return 0;
+
+	spin_lock(&cp->lock);
+	set_tcp_state(pp, cp, direction, th);
+	spin_unlock(&cp->lock);
 
-	spin_lock_bh(&cp->lock);
-	set_tcp_state(pd, cp, direction, th);
-	spin_unlock_bh(&cp->lock);
+	return 1;
 }
 
+/*
+ *	Hash table for TCP application incarnations
+ */
+#define	TCP_APP_TAB_BITS	4
+#define	TCP_APP_TAB_SIZE	(1 << TCP_APP_TAB_BITS)
+#define	TCP_APP_TAB_MASK	(TCP_APP_TAB_SIZE - 1)
+
+static struct list_head tcp_apps[TCP_APP_TAB_SIZE];
+static DEFINE_SPINLOCK(tcp_app_lock);
+
 static inline __u16 tcp_app_hashkey(__be16 port)
 {
-	return (((__force u16)port >> TCP_APP_TAB_BITS) ^ (__force u16)port)
-		& TCP_APP_TAB_MASK;
+	return (((__force u16) port >> TCP_APP_TAB_BITS) ^ (__force u16) port)
+	    & TCP_APP_TAB_MASK;
 }
 
-
-static int tcp_register_app(struct net *net, struct ip_vs_app *inc)
+static int tcp_register_app(struct ip_vs_app *inc)
 {
 	struct ip_vs_app *i;
 	__u16 hash;
 	__be16 port = inc->port;
 	int ret = 0;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_TCP);
 
 	hash = tcp_app_hashkey(port);
 
-	list_for_each_entry(i, &ipvs->tcp_apps[hash], p_list) {
+	spin_lock_bh(&tcp_app_lock);
+	list_for_each_entry(i, &tcp_apps[hash], p_list) {
 		if (i->port == port) {
 			ret = -EEXIST;
 			goto out;
 		}
 	}
-	list_add_rcu(&inc->p_list, &ipvs->tcp_apps[hash]);
-	atomic_inc(&pd->appcnt);
+	list_add(&inc->p_list, &tcp_apps[hash]);
+	atomic_inc(&ip_vs_protocol_tcp.appcnt);
 
-  out:
+      out:
+	spin_unlock_bh(&tcp_app_lock);
 	return ret;
 }
 
-
-static void
-tcp_unregister_app(struct net *net, struct ip_vs_app *inc)
+static void tcp_unregister_app(struct ip_vs_app *inc)
 {
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_TCP);
-
-	atomic_dec(&pd->appcnt);
-	list_del_rcu(&inc->p_list);
+	spin_lock_bh(&tcp_app_lock);
+	atomic_dec(&ip_vs_protocol_tcp.appcnt);
+	list_del(&inc->p_list);
+	spin_unlock_bh(&tcp_app_lock);
 }
 
-
-static int
-tcp_app_conn_bind(struct ip_vs_conn *cp)
+static int tcp_app_conn_bind(struct ip_vs_conn *cp)
 {
-	struct netns_ipvs *ipvs = net_ipvs(ip_vs_conn_net(cp));
 	int hash;
 	struct ip_vs_app *inc;
 	int result = 0;
@@ -619,12 +1678,12 @@ tcp_app_conn_bind(struct ip_vs_conn *cp)
 	/* Lookup application incarnations and bind the right one */
 	hash = tcp_app_hashkey(cp->vport);
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(inc, &ipvs->tcp_apps[hash], p_list) {
+	spin_lock(&tcp_app_lock);
+	list_for_each_entry(inc, &tcp_apps[hash], p_list) {
 		if (inc->port == cp->vport) {
 			if (unlikely(!ip_vs_app_inc_get(inc)))
 				break;
-			rcu_read_unlock();
+			spin_unlock(&tcp_app_lock);
 
 			IP_VS_DBG_BUF(9, "%s(): Binding conn %s:%u->"
 				      "%s:%u to app %s on port %u\n",
@@ -641,70 +1700,56 @@ tcp_app_conn_bind(struct ip_vs_conn *cp)
 			goto out;
 		}
 	}
-	rcu_read_unlock();
+	spin_unlock(&tcp_app_lock);
 
-  out:
+      out:
 	return result;
 }
 
-
 /*
  *	Set LISTEN timeout. (ip_vs_conn_put will setup timer)
  */
-void ip_vs_tcp_conn_listen(struct net *net, struct ip_vs_conn *cp)
+void ip_vs_tcp_conn_listen(struct ip_vs_conn *cp)
 {
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_TCP);
-
-	spin_lock_bh(&cp->lock);
+	spin_lock(&cp->lock);
 	cp->state = IP_VS_TCP_S_LISTEN;
-	cp->timeout = (pd ? pd->timeout_table[IP_VS_TCP_S_LISTEN]
-			   : tcp_timeouts[IP_VS_TCP_S_LISTEN]);
-	spin_unlock_bh(&cp->lock);
+	cp->timeout = ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_LISTEN];
+	spin_unlock(&cp->lock);
 }
 
-/* ---------------------------------------------
- *   timeouts is netns related now.
- * ---------------------------------------------
- */
-static int __ip_vs_tcp_init(struct net *net, struct ip_vs_proto_data *pd)
+static void ip_vs_tcp_init(struct ip_vs_protocol *pp)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	ip_vs_init_hash_table(ipvs->tcp_apps, TCP_APP_TAB_SIZE);
-	pd->timeout_table = ip_vs_create_timeout_table((int *)tcp_timeouts,
-							sizeof(tcp_timeouts));
-	if (!pd->timeout_table)
-		return -ENOMEM;
-	pd->tcp_state_table =  tcp_states;
-	return 0;
+	IP_VS_INIT_HASH_TABLE(tcp_apps);
+	pp->timeout_table = sysctl_ip_vs_tcp_timeouts;
 }
 
-static void __ip_vs_tcp_exit(struct net *net, struct ip_vs_proto_data *pd)
+static void ip_vs_tcp_exit(struct ip_vs_protocol *pp)
 {
-	kfree(pd->timeout_table);
 }
 
-
 struct ip_vs_protocol ip_vs_protocol_tcp = {
-	.name =			"TCP",
-	.protocol =		IPPROTO_TCP,
-	.num_states =		IP_VS_TCP_S_LAST,
-	.dont_defrag =		0,
-	.init =			NULL,
-	.exit =			NULL,
-	.init_netns =		__ip_vs_tcp_init,
-	.exit_netns =		__ip_vs_tcp_exit,
-	.register_app =		tcp_register_app,
-	.unregister_app =	tcp_unregister_app,
-	.conn_schedule =	tcp_conn_schedule,
-	.conn_in_get =		ip_vs_conn_in_get_proto,
-	.conn_out_get =		ip_vs_conn_out_get_proto,
-	.snat_handler =		tcp_snat_handler,
-	.dnat_handler =		tcp_dnat_handler,
-	.csum_check =		tcp_csum_check,
-	.state_name =		tcp_state_name,
-	.state_transition =	tcp_state_transition,
-	.app_conn_bind =	tcp_app_conn_bind,
-	.debug_packet =		ip_vs_tcpudp_debug_packet,
-	.timeout_change =	tcp_timeout_change,
+	.name = "TCP",
+	.protocol = IPPROTO_TCP,
+	.num_states = IP_VS_TCP_S_LAST,
+	.dont_defrag = 0,
+	.appcnt = ATOMIC_INIT(0),
+	.init = ip_vs_tcp_init,
+	.exit = ip_vs_tcp_exit,
+	.register_app = tcp_register_app,
+	.unregister_app = tcp_unregister_app,
+	.conn_schedule = tcp_conn_schedule,
+	.conn_in_get = tcp_conn_in_get,
+	.conn_out_get = tcp_conn_out_get,
+	.snat_handler = tcp_snat_handler,
+	.dnat_handler = tcp_dnat_handler,
+	.fnat_in_handler = tcp_fnat_in_handler,
+	.fnat_out_handler = tcp_fnat_out_handler,
+	.csum_check = tcp_csum_check,
+	.state_name = tcp_state_name,
+	.state_transition = tcp_state_transition,
+	.app_conn_bind = tcp_app_conn_bind,
+	.debug_packet = ip_vs_tcpudp_debug_packet,
+	.timeout_change = tcp_timeout_change,
+	.set_state_timeout = tcp_set_state_timeout,
+	.conn_expire_handler = tcp_conn_expire_handler,
 };
diff --git a/net/netfilter/ipvs/ip_vs_proto_udp.c b/net/netfilter/ipvs/ip_vs_proto_udp.c
index b62a3c0..716e406 100644
--- a/net/netfilter/ipvs/ip_vs_proto_udp.c
+++ b/net/netfilter/ipvs/ip_vs_proto_udp.c
@@ -9,8 +9,7 @@
  *              as published by the Free Software Foundation; either version
  *              2 of the License, or (at your option) any later version.
  *
- * Changes:     Hans Schillstrom <hans.schillstrom@ericsson.com>
- *              Network name space (netns) aware.
+ * Changes:
  *
  */
 
@@ -28,34 +27,83 @@
 #include <net/ip.h>
 #include <net/ip6_checksum.h>
 
+static struct ip_vs_conn *udp_conn_in_get(int af, const struct sk_buff *skb,
+					  struct ip_vs_protocol *pp,
+					  const struct ip_vs_iphdr *iph,
+					  unsigned int proto_off, int inverse,
+					  int *res_dir)
+{
+	struct ip_vs_conn *cp;
+	__be16 _ports[2], *pptr;
+
+	pptr = skb_header_pointer(skb, proto_off, sizeof(_ports), _ports);
+	if (pptr == NULL)
+		return NULL;
+
+	if (likely(!inverse)) {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
+	} else {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
+	}
+
+	return cp;
+}
+
+static struct ip_vs_conn *udp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
+{
+	struct ip_vs_conn *cp;
+	__be16 _ports[2], *pptr;
+
+	pptr = skb_header_pointer(skb, proto_off, sizeof(_ports), _ports);
+	if (pptr == NULL)
+		return NULL;
+
+	if (likely(!inverse)) {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
+	} else {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
+	}
+
+	return cp;
+}
+
 static int
-udp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
-		  int *verdict, struct ip_vs_conn **cpp,
-		  struct ip_vs_iphdr *iph)
+udp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
+		  int *verdict, struct ip_vs_conn **cpp)
 {
-	struct net *net;
 	struct ip_vs_service *svc;
 	struct udphdr _udph, *uh;
+	struct ip_vs_iphdr iph;
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 
-	/* IPv6 fragments, only first fragment will hit this */
-	uh = skb_header_pointer(skb, iph->len, sizeof(_udph), &_udph);
+	uh = skb_header_pointer(skb, iph.len, sizeof(_udph), &_udph);
 	if (uh == NULL) {
 		*verdict = NF_DROP;
 		return 0;
 	}
-	net = skb_net(skb);
-	rcu_read_lock();
-	svc = ip_vs_service_find(net, af, skb->mark, iph->protocol,
-				 &iph->daddr, uh->dest);
-	if (svc) {
-		int ignored;
 
-		if (ip_vs_todrop(net_ipvs(net))) {
+	svc = ip_vs_service_get(af, skb->mark, iph.protocol,
+				&iph.daddr, uh->dest);
+	if (svc) {
+		if (ip_vs_todrop()) {
 			/*
 			 * It seems that we are very loaded.
 			 * We have to drop this packet :(
 			 */
-			rcu_read_unlock();
+			ip_vs_service_put(svc);
 			*verdict = NF_DROP;
 			return 0;
 		}
@@ -64,22 +112,16 @@ udp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd,
 		 * Let the virtual server select a real server for the
 		 * incoming connection, and create a connection entry.
 		 */
-		*cpp = ip_vs_schedule(svc, skb, pd, &ignored, iph);
-		if (!*cpp && ignored <= 0) {
-			if (!ignored)
-				*verdict = ip_vs_leave(svc, skb, pd, iph);
-			else
-				*verdict = NF_DROP;
-			rcu_read_unlock();
+		*cpp = ip_vs_schedule(svc, skb, 0);
+		if (!*cpp) {
+			*verdict = ip_vs_leave(svc, skb, pp);
 			return 0;
 		}
+		ip_vs_service_put(svc);
 	}
-	rcu_read_unlock();
-	/* NF_ACCEPT */
 	return 1;
 }
 
-
 static inline void
 udp_fast_csum_update(int af, struct udphdr *uhdr,
 		     const union nf_inet_addr *oldip,
@@ -89,95 +131,120 @@ udp_fast_csum_update(int af, struct udphdr *uhdr,
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		uhdr->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
 	else
 #endif
 		uhdr->check =
-			csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
 	if (!uhdr->check)
 		uhdr->check = CSUM_MANGLED_0;
 }
 
 static inline void
 udp_partial_csum_update(int af, struct udphdr *uhdr,
-		     const union nf_inet_addr *oldip,
-		     const union nf_inet_addr *newip,
-		     __be16 oldlen, __be16 newlen)
+			const union nf_inet_addr *oldip,
+			const union nf_inet_addr *newip,
+			__be16 oldlen, __be16 newlen)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		uhdr->check =
-			~csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldlen, newlen,
-						csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldlen,
+								   newlen,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
 	else
 #endif
-	uhdr->check =
-		~csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				ip_vs_check_diff2(oldlen, newlen,
-						csum_unfold(uhdr->check))));
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldlen,
+								  newlen,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
 }
 
+/* Calculate UDP checksum, only for PARTICAL */
+static inline void
+udp_partial_csum_reset(int af, int len, struct udphdr *uhdr,
+				const union nf_inet_addr *saddr,
+				const union nf_inet_addr *daddr)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		uhdr->check = ~csum_ipv6_magic(&saddr->in6, &daddr->in6,
+							len, IPPROTO_UDP, 0);
+        else
+#endif
+		uhdr->check = ~csum_tcpudp_magic(saddr->ip, daddr->ip, len, IPPROTO_UDP, 0);
+}
 
 static int
-udp_snat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
+udp_snat_handler(struct sk_buff *skb,
+		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
 {
 	struct udphdr *udph;
-	unsigned int udphoff = iph->len;
+	unsigned int udphoff;
 	int oldlen;
-	int payload_csum = 0;
 
 #ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6 && iph->fragoffs)
-		return 1;
+	if (cp->af == AF_INET6)
+		udphoff = sizeof(struct ipv6hdr);
+	else
 #endif
+		udphoff = ip_hdrlen(skb);
 	oldlen = skb->len - udphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, udphoff+sizeof(*udph)))
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
-		int ret;
-
 		/* Some checks before mangling */
 		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
 			return 0;
 
 		/*
-		 *	Call application helper if needed
+		 *      Call application helper if needed
 		 */
-		if (!(ret = ip_vs_app_pkt_out(cp, skb)))
+		if (!ip_vs_app_pkt_out(cp, skb))
 			return 0;
-		/* ret=2: csum update is needed after payload mangling */
-		if (ret == 1)
-			oldlen = skb->len - udphoff;
-		else
-			payload_csum = 1;
 	}
 
 	udph = (void *)skb_network_header(skb) + udphoff;
 	udph->source = cp->vport;
+	udph->dest = cp->cport;
 
 	/*
-	 *	Adjust UDP checksums
+	 *      Adjust UDP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
 		udp_partial_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
 					htons(oldlen),
 					htons(skb->len - udphoff));
-	} else if (!payload_csum && (udph->check != 0)) {
+		udp_partial_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+					htons(oldlen),
+					htons(skb->len - udphoff));
+	} else if (!cp->app && (udph->check != 0)) {
 		/* Only port and addr are changed, do fast csum update */
 		udp_fast_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
 				     cp->dport, cp->vport);
+		udp_fast_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
-			skb->ip_summed = (cp->app && pp->csum_check) ?
-					 CHECKSUM_UNNECESSARY : CHECKSUM_NONE;
+			skb->ip_summed = CHECKSUM_NONE;
 	} else {
 		/* full checksum calculation */
 		udph->check = 0;
@@ -197,71 +264,68 @@ udp_snat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
 							skb->csum);
 		if (udph->check == 0)
 			udph->check = CSUM_MANGLED_0;
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
 			  pp->name, udph->check,
-			  (char*)&(udph->check) - (char*)udph);
+			  (char *)&(udph->check) - (char *)udph);
 	}
 	return 1;
 }
 
-
 static int
-udp_dnat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 struct ip_vs_conn *cp, struct ip_vs_iphdr *iph)
+udp_dnat_handler(struct sk_buff *skb,
+		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
 {
 	struct udphdr *udph;
-	unsigned int udphoff = iph->len;
+	unsigned int udphoff;
 	int oldlen;
-	int payload_csum = 0;
 
 #ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6 && iph->fragoffs)
-		return 1;
+	if (cp->af == AF_INET6)
+		udphoff = sizeof(struct ipv6hdr);
+	else
 #endif
+		udphoff = ip_hdrlen(skb);
 	oldlen = skb->len - udphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, udphoff+sizeof(*udph)))
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
-		int ret;
-
 		/* Some checks before mangling */
 		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
 			return 0;
 
 		/*
-		 *	Attempt ip_vs_app call.
-		 *	It will fix ip_vs_conn
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn
 		 */
-		if (!(ret = ip_vs_app_pkt_in(cp, skb)))
+		if (!ip_vs_app_pkt_in(cp, skb))
 			return 0;
-		/* ret=2: csum update is needed after payload mangling */
-		if (ret == 1)
-			oldlen = skb->len - udphoff;
-		else
-			payload_csum = 1;
 	}
 
 	udph = (void *)skb_network_header(skb) + udphoff;
+	udph->source = cp->lport;
 	udph->dest = cp->dport;
 
 	/*
-	 *	Adjust UDP checksums
+	 *      Adjust UDP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
 		udp_partial_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
 					htons(oldlen),
 					htons(skb->len - udphoff));
-	} else if (!payload_csum && (udph->check != 0)) {
+		udp_partial_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
+					htons(oldlen),
+					htons(skb->len - udphoff));
+	} else if (!cp->app && (udph->check != 0)) {
 		/* Only port and addr are changed, do fast csum update */
 		udp_fast_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
 				     cp->vport, cp->dport);
+		udp_fast_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
-			skb->ip_summed = (cp->app && pp->csum_check) ?
-					 CHECKSUM_UNNECESSARY : CHECKSUM_NONE;
+			skb->ip_summed = CHECKSUM_NONE;
 	} else {
 		/* full checksum calculation */
 		udph->check = 0;
@@ -286,6 +350,149 @@ udp_dnat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp,
 	return 1;
 }
 
+static int
+udp_fnat_in_handler(struct sk_buff **skb_p,
+		    struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct udphdr *udph;
+	unsigned int udphoff;
+	int oldlen;
+	struct sk_buff *skb = *skb_p;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		udphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		udphoff = ip_hdrlen(skb);
+	oldlen = skb->len - udphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
+		 */
+		if (!ip_vs_app_pkt_in(cp, skb))
+			return 0;
+	}
+
+	udph = (void *)skb_network_header(skb) + udphoff;
+
+	/* adjust src/dst port */
+	udph->source = cp->lport;
+	udph->dest = cp->dport;
+
+	/* Adjust UDP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		udp_partial_csum_reset(cp->af, (skb->len - udphoff),
+					udph, &cp->laddr, &cp->daddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		udp_fast_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
+				     cp->vport, cp->dport);
+		udp_fast_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		udph->check = 0;
+		skb->csum = skb_checksum(skb, udphoff, skb->len - udphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			udph->check = csum_ipv6_magic(&cp->laddr.in6,
+						      &cp->daddr.in6,
+						      skb->len - udphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			udph->check = csum_tcpudp_magic(cp->laddr.ip,
+							cp->daddr.ip,
+							skb->len - udphoff,
+							cp->protocol, skb->csum);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	}
+	return 1;
+}
+
+static int
+udp_fnat_out_handler(struct sk_buff *skb,
+		     struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct udphdr *udph;
+	unsigned int udphoff;
+	int oldlen;
+
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		udphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		udphoff = ip_hdrlen(skb);
+	oldlen = skb->len - udphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/* Call application helper if needed */
+		if (!ip_vs_app_pkt_out(cp, skb))
+			return 0;
+	}
+
+	udph = (void *)skb_network_header(skb) + udphoff;
+	udph->source = cp->vport;
+	udph->dest = cp->cport;
+
+	/* Adjust UDP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		udp_partial_csum_reset(cp->af, (skb->len - udphoff),
+					udph, &cp->vaddr, &cp->caddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		udp_fast_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
+				     cp->dport, cp->vport);
+		udp_fast_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		udph->check = 0;
+		skb->csum = skb_checksum(skb, udphoff, skb->len - udphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			udph->check = csum_ipv6_magic(&cp->vaddr.in6,
+						      &cp->caddr.in6,
+						      skb->len - udphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			udph->check = csum_tcpudp_magic(cp->vaddr.ip,
+							cp->caddr.ip,
+							skb->len - udphoff,
+							cp->protocol, skb->csum);
+
+		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
+			pp->name, udph->check,
+			(char *)&(udph->check) - (char *)udph);
+	}
+	return 1;
+}
 
 static int
 udp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
@@ -317,21 +524,23 @@ udp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
 						    skb->len - udphoff,
 						    ipv6_hdr(skb)->nexthdr,
 						    skb->csum)) {
-					IP_VS_DBG_RL_PKT(0, af, pp, skb, 0,
+					IP_VS_DBG_RL_PKT(0, pp, skb, 0,
 							 "Failed checksum for");
 					return 0;
 				}
 			} else
 #endif
-				if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
-						      ip_hdr(skb)->daddr,
-						      skb->len - udphoff,
-						      ip_hdr(skb)->protocol,
-						      skb->csum)) {
-					IP_VS_DBG_RL_PKT(0, af, pp, skb, 0,
-							 "Failed checksum for");
-					return 0;
-				}
+			if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
+						      ip_hdr(skb)->
+						      daddr,
+						      skb->len -
+						      udphoff,
+						      ip_hdr(skb)->
+						      protocol, skb->csum)) {
+				IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+						 "Failed checksum for");
+				return 0;
+			}
 			break;
 		default:
 			/* No need to checksum. */
@@ -341,51 +550,58 @@ udp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
 	return 1;
 }
 
+/*
+ *	Note: the caller guarantees that only one of register_app,
+ *	unregister_app or app_conn_bind is called each time.
+ */
+
+#define	UDP_APP_TAB_BITS	4
+#define	UDP_APP_TAB_SIZE	(1 << UDP_APP_TAB_BITS)
+#define	UDP_APP_TAB_MASK	(UDP_APP_TAB_SIZE - 1)
+
+static struct list_head udp_apps[UDP_APP_TAB_SIZE];
+static DEFINE_SPINLOCK(udp_app_lock);
+
 static inline __u16 udp_app_hashkey(__be16 port)
 {
-	return (((__force u16)port >> UDP_APP_TAB_BITS) ^ (__force u16)port)
-		& UDP_APP_TAB_MASK;
+	return (((__force u16) port >> UDP_APP_TAB_BITS) ^ (__force u16) port)
+	    & UDP_APP_TAB_MASK;
 }
 
-
-static int udp_register_app(struct net *net, struct ip_vs_app *inc)
+static int udp_register_app(struct ip_vs_app *inc)
 {
 	struct ip_vs_app *i;
 	__u16 hash;
 	__be16 port = inc->port;
 	int ret = 0;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_UDP);
 
 	hash = udp_app_hashkey(port);
 
-	list_for_each_entry(i, &ipvs->udp_apps[hash], p_list) {
+	spin_lock_bh(&udp_app_lock);
+	list_for_each_entry(i, &udp_apps[hash], p_list) {
 		if (i->port == port) {
 			ret = -EEXIST;
 			goto out;
 		}
 	}
-	list_add_rcu(&inc->p_list, &ipvs->udp_apps[hash]);
-	atomic_inc(&pd->appcnt);
+	list_add(&inc->p_list, &udp_apps[hash]);
+	atomic_inc(&ip_vs_protocol_udp.appcnt);
 
-  out:
+      out:
+	spin_unlock_bh(&udp_app_lock);
 	return ret;
 }
 
-
-static void
-udp_unregister_app(struct net *net, struct ip_vs_app *inc)
+static void udp_unregister_app(struct ip_vs_app *inc)
 {
-	struct ip_vs_proto_data *pd = ip_vs_proto_data_get(net, IPPROTO_UDP);
-
-	atomic_dec(&pd->appcnt);
-	list_del_rcu(&inc->p_list);
+	spin_lock_bh(&udp_app_lock);
+	atomic_dec(&ip_vs_protocol_udp.appcnt);
+	list_del(&inc->p_list);
+	spin_unlock_bh(&udp_app_lock);
 }
 
-
 static int udp_app_conn_bind(struct ip_vs_conn *cp)
 {
-	struct netns_ipvs *ipvs = net_ipvs(ip_vs_conn_net(cp));
 	int hash;
 	struct ip_vs_app *inc;
 	int result = 0;
@@ -397,12 +613,12 @@ static int udp_app_conn_bind(struct ip_vs_conn *cp)
 	/* Lookup application incarnations and bind the right one */
 	hash = udp_app_hashkey(cp->vport);
 
-	rcu_read_lock();
-	list_for_each_entry_rcu(inc, &ipvs->udp_apps[hash], p_list) {
+	spin_lock(&udp_app_lock);
+	list_for_each_entry(inc, &udp_apps[hash], p_list) {
 		if (inc->port == cp->vport) {
 			if (unlikely(!ip_vs_app_inc_get(inc)))
 				break;
-			rcu_read_unlock();
+			spin_unlock(&udp_app_lock);
 
 			IP_VS_DBG_BUF(9, "%s(): Binding conn %s:%u->"
 				      "%s:%u to app %s on port %u\n",
@@ -419,81 +635,74 @@ static int udp_app_conn_bind(struct ip_vs_conn *cp)
 			goto out;
 		}
 	}
-	rcu_read_unlock();
+	spin_unlock(&udp_app_lock);
 
-  out:
+      out:
 	return result;
 }
 
-
-static const int udp_timeouts[IP_VS_UDP_S_LAST+1] = {
-	[IP_VS_UDP_S_NORMAL]		=	5*60*HZ,
-	[IP_VS_UDP_S_LAST]		=	2*HZ,
+static int udp_timeouts[IP_VS_UDP_S_LAST + 1] = {
+	[IP_VS_UDP_S_NORMAL] = 5 * 60 * HZ,
+	[IP_VS_UDP_S_LAST] = 2 * HZ,
 };
 
-static const char *const udp_state_name_table[IP_VS_UDP_S_LAST+1] = {
-	[IP_VS_UDP_S_NORMAL]		=	"UDP",
-	[IP_VS_UDP_S_LAST]		=	"BUG!",
+static const char *const udp_state_name_table[IP_VS_UDP_S_LAST + 1] = {
+	[IP_VS_UDP_S_NORMAL] = "UDP",
+	[IP_VS_UDP_S_LAST] = "BUG!",
 };
 
-static const char * udp_state_name(int state)
+static int udp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
+{
+	return ip_vs_set_state_timeout(pp->timeout_table, IP_VS_UDP_S_LAST,
+				       udp_state_name_table, sname, to);
+}
+
+static const char *udp_state_name(int state)
 {
 	if (state >= IP_VS_UDP_S_LAST)
 		return "ERR!";
 	return udp_state_name_table[state] ? udp_state_name_table[state] : "?";
 }
 
-static void
+static int
 udp_state_transition(struct ip_vs_conn *cp, int direction,
-		     const struct sk_buff *skb,
-		     struct ip_vs_proto_data *pd)
+		     const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
-	if (unlikely(!pd)) {
-		pr_err("UDP no ns data\n");
-		return;
-	}
-
-	cp->timeout = pd->timeout_table[IP_VS_UDP_S_NORMAL];
+	cp->timeout = pp->timeout_table[IP_VS_UDP_S_NORMAL];
+	return 1;
 }
 
-static int __udp_init(struct net *net, struct ip_vs_proto_data *pd)
+static void udp_init(struct ip_vs_protocol *pp)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	ip_vs_init_hash_table(ipvs->udp_apps, UDP_APP_TAB_SIZE);
-	pd->timeout_table = ip_vs_create_timeout_table((int *)udp_timeouts,
-							sizeof(udp_timeouts));
-	if (!pd->timeout_table)
-		return -ENOMEM;
-	return 0;
+	IP_VS_INIT_HASH_TABLE(udp_apps);
+	pp->timeout_table = udp_timeouts;
 }
 
-static void __udp_exit(struct net *net, struct ip_vs_proto_data *pd)
+static void udp_exit(struct ip_vs_protocol *pp)
 {
-	kfree(pd->timeout_table);
 }
 
-
 struct ip_vs_protocol ip_vs_protocol_udp = {
-	.name =			"UDP",
-	.protocol =		IPPROTO_UDP,
-	.num_states =		IP_VS_UDP_S_LAST,
-	.dont_defrag =		0,
-	.init =			NULL,
-	.exit =			NULL,
-	.init_netns =		__udp_init,
-	.exit_netns =		__udp_exit,
-	.conn_schedule =	udp_conn_schedule,
-	.conn_in_get =		ip_vs_conn_in_get_proto,
-	.conn_out_get =		ip_vs_conn_out_get_proto,
-	.snat_handler =		udp_snat_handler,
-	.dnat_handler =		udp_dnat_handler,
-	.csum_check =		udp_csum_check,
-	.state_transition =	udp_state_transition,
-	.state_name =		udp_state_name,
-	.register_app =		udp_register_app,
-	.unregister_app =	udp_unregister_app,
-	.app_conn_bind =	udp_app_conn_bind,
-	.debug_packet =		ip_vs_tcpudp_debug_packet,
-	.timeout_change =	NULL,
+	.name = "UDP",
+	.protocol = IPPROTO_UDP,
+	.num_states = IP_VS_UDP_S_LAST,
+	.dont_defrag = 0,
+	.init = udp_init,
+	.exit = udp_exit,
+	.conn_schedule = udp_conn_schedule,
+	.conn_in_get = udp_conn_in_get,
+	.conn_out_get = udp_conn_out_get,
+	.snat_handler = udp_snat_handler,
+	.dnat_handler = udp_dnat_handler,
+	.fnat_in_handler = udp_fnat_in_handler,
+	.fnat_out_handler = udp_fnat_out_handler,
+	.csum_check = udp_csum_check,
+	.state_transition = udp_state_transition,
+	.state_name = udp_state_name,
+	.register_app = udp_register_app,
+	.unregister_app = udp_unregister_app,
+	.app_conn_bind = udp_app_conn_bind,
+	.debug_packet = ip_vs_tcpudp_debug_packet,
+	.timeout_change = NULL,
+	.set_state_timeout = udp_set_state_timeout,
 };
diff --git a/net/netfilter/ipvs/ip_vs_rr.c b/net/netfilter/ipvs/ip_vs_rr.c
index c35986c..e210f37 100644
--- a/net/netfilter/ipvs/ip_vs_rr.c
+++ b/net/netfilter/ipvs/ip_vs_rr.c
@@ -35,18 +35,9 @@ static int ip_vs_rr_init_svc(struct ip_vs_service *svc)
 }
 
 
-static int ip_vs_rr_del_dest(struct ip_vs_service *svc, struct ip_vs_dest *dest)
+static int ip_vs_rr_update_svc(struct ip_vs_service *svc)
 {
-	struct list_head *p;
-
-	spin_lock_bh(&svc->sched_lock);
-	p = (struct list_head *) svc->sched_data;
-	/* dest is already unlinked, so p->prev is not valid but
-	 * p->next is valid, use it to reach previous entry.
-	 */
-	if (p == &dest->n_list)
-		svc->sched_data = p->next->prev;
-	spin_unlock_bh(&svc->sched_lock);
+	svc->sched_data = &svc->destinations;
 	return 0;
 }
 
@@ -57,41 +48,36 @@ static int ip_vs_rr_del_dest(struct ip_vs_service *svc, struct ip_vs_dest *dest)
 static struct ip_vs_dest *
 ip_vs_rr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 {
-	struct list_head *p;
-	struct ip_vs_dest *dest, *last;
-	int pass = 0;
+	struct list_head *p, *q;
+	struct ip_vs_dest *dest;
 
 	IP_VS_DBG(6, "%s(): Scheduling...\n", __func__);
 
-	spin_lock_bh(&svc->sched_lock);
-	p = (struct list_head *) svc->sched_data;
-	last = dest = list_entry(p, struct ip_vs_dest, n_list);
-
+	write_lock(&svc->sched_lock);
+	p = (struct list_head *)svc->sched_data;
+	p = p->next;
+	q = p;
 	do {
-		list_for_each_entry_continue_rcu(dest,
-						 &svc->destinations,
-						 n_list) {
-			if (!(dest->flags & IP_VS_DEST_F_OVERLOAD) &&
-			    atomic_read(&dest->weight) > 0)
-				/* HIT */
-				goto out;
-			if (dest == last)
-				goto stop;
+		/* skip list head */
+		if (q == &svc->destinations) {
+			q = q->next;
+			continue;
 		}
-		pass++;
-		/* Previous dest could be unlinked, do not loop forever.
-		 * If we stay at head there is no need for 2nd pass.
-		 */
-	} while (pass < 2 && p != &svc->destinations);
-
-stop:
-	spin_unlock_bh(&svc->sched_lock);
-	ip_vs_scheduler_err(svc, "no destination available");
+
+		dest = list_entry(q, struct ip_vs_dest, n_list);
+		if (!(dest->flags & IP_VS_DEST_F_OVERLOAD) &&
+		    atomic_read(&dest->weight) > 0)
+			/* HIT */
+			goto out;
+		q = q->next;
+	} while (q != p);
+	write_unlock(&svc->sched_lock);
+	IP_VS_ERR_RL("RR: no destination available\n");
 	return NULL;
 
   out:
-	svc->sched_data = &dest->n_list;
-	spin_unlock_bh(&svc->sched_lock);
+	svc->sched_data = q;
+	write_unlock(&svc->sched_lock);
 	IP_VS_DBG_BUF(6, "RR: server %s:%u "
 		      "activeconns %d refcnt %d weight %d\n",
 		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port),
@@ -108,8 +94,7 @@ static struct ip_vs_scheduler ip_vs_rr_scheduler = {
 	.module =		THIS_MODULE,
 	.n_list =		LIST_HEAD_INIT(ip_vs_rr_scheduler.n_list),
 	.init_service =		ip_vs_rr_init_svc,
-	.add_dest =		NULL,
-	.del_dest =		ip_vs_rr_del_dest,
+	.update_service =	ip_vs_rr_update_svc,
 	.schedule =		ip_vs_rr_schedule,
 };
 
@@ -121,7 +106,6 @@ static int __init ip_vs_rr_init(void)
 static void __exit ip_vs_rr_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_rr_scheduler);
-	synchronize_rcu();
 }
 
 module_init(ip_vs_rr_init);
diff --git a/net/netfilter/ipvs/ip_vs_sched.c b/net/netfilter/ipvs/ip_vs_sched.c
index 4dbcda6..bbc1ac7 100644
--- a/net/netfilter/ipvs/ip_vs_sched.c
+++ b/net/netfilter/ipvs/ip_vs_sched.c
@@ -29,14 +29,13 @@
 
 #include <net/ip_vs.h>
 
-EXPORT_SYMBOL(ip_vs_scheduler_err);
 /*
  *  IPVS scheduler list
  */
 static LIST_HEAD(ip_vs_schedulers);
 
-/* semaphore for schedulers */
-static DEFINE_MUTEX(ip_vs_sched_mutex);
+/* lock for service table */
+static DEFINE_RWLOCK(__ip_vs_sched_lock);
 
 
 /*
@@ -47,6 +46,17 @@ int ip_vs_bind_scheduler(struct ip_vs_service *svc,
 {
 	int ret;
 
+	if (svc == NULL) {
+		pr_err("%s(): svc arg NULL\n", __func__);
+		return -EINVAL;
+	}
+	if (scheduler == NULL) {
+		pr_err("%s(): scheduler arg NULL\n", __func__);
+		return -EINVAL;
+	}
+
+	svc->scheduler = scheduler;
+
 	if (scheduler->init_service) {
 		ret = scheduler->init_service(svc);
 		if (ret) {
@@ -54,7 +64,7 @@ int ip_vs_bind_scheduler(struct ip_vs_service *svc,
 			return ret;
 		}
 	}
-	rcu_assign_pointer(svc->scheduler, scheduler);
+
 	return 0;
 }
 
@@ -62,19 +72,30 @@ int ip_vs_bind_scheduler(struct ip_vs_service *svc,
 /*
  *  Unbind a service with its scheduler
  */
-void ip_vs_unbind_scheduler(struct ip_vs_service *svc,
-			    struct ip_vs_scheduler *sched)
+int ip_vs_unbind_scheduler(struct ip_vs_service *svc)
 {
-	struct ip_vs_scheduler *cur_sched;
+	struct ip_vs_scheduler *sched;
+
+	if (svc == NULL) {
+		pr_err("%s(): svc arg NULL\n", __func__);
+		return -EINVAL;
+	}
 
-	cur_sched = rcu_dereference_protected(svc->scheduler, 1);
-	/* This check proves that old 'sched' was installed */
-	if (!cur_sched)
-		return;
+	sched = svc->scheduler;
+	if (sched == NULL) {
+		pr_err("%s(): svc isn't bound\n", __func__);
+		return -EINVAL;
+	}
 
-	if (sched->done_service)
-		sched->done_service(svc);
-	/* svc->scheduler can not be set to NULL */
+	if (sched->done_service) {
+		if (sched->done_service(svc) != 0) {
+			pr_err("%s(): done error\n", __func__);
+			return -EINVAL;
+		}
+	}
+
+	svc->scheduler = NULL;
+	return 0;
 }
 
 
@@ -87,7 +108,7 @@ static struct ip_vs_scheduler *ip_vs_sched_getbyname(const char *sched_name)
 
 	IP_VS_DBG(2, "%s(): sched_name \"%s\"\n", __func__, sched_name);
 
-	mutex_lock(&ip_vs_sched_mutex);
+	read_lock_bh(&__ip_vs_sched_lock);
 
 	list_for_each_entry(sched, &ip_vs_schedulers, n_list) {
 		/*
@@ -101,14 +122,14 @@ static struct ip_vs_scheduler *ip_vs_sched_getbyname(const char *sched_name)
 		}
 		if (strcmp(sched_name, sched->name)==0) {
 			/* HIT */
-			mutex_unlock(&ip_vs_sched_mutex);
+			read_unlock_bh(&__ip_vs_sched_lock);
 			return sched;
 		}
 		if (sched->module)
 			module_put(sched->module);
 	}
 
-	mutex_unlock(&ip_vs_sched_mutex);
+	read_unlock_bh(&__ip_vs_sched_lock);
 	return NULL;
 }
 
@@ -138,34 +159,10 @@ struct ip_vs_scheduler *ip_vs_scheduler_get(const char *sched_name)
 
 void ip_vs_scheduler_put(struct ip_vs_scheduler *scheduler)
 {
-	if (scheduler && scheduler->module)
+	if (scheduler->module)
 		module_put(scheduler->module);
 }
 
-/*
- * Common error output helper for schedulers
- */
-
-void ip_vs_scheduler_err(struct ip_vs_service *svc, const char *msg)
-{
-	struct ip_vs_scheduler *sched;
-
-	sched = rcu_dereference(svc->scheduler);
-	if (svc->fwmark) {
-		IP_VS_ERR_RL("%s: FWM %u 0x%08X - %s\n",
-			     sched->name, svc->fwmark, svc->fwmark, msg);
-#ifdef CONFIG_IP_VS_IPV6
-	} else if (svc->af == AF_INET6) {
-		IP_VS_ERR_RL("%s: %s [%pI6c]:%d - %s\n",
-			     sched->name, ip_vs_proto_name(svc->protocol),
-			     &svc->addr.in6, ntohs(svc->port), msg);
-#endif
-	} else {
-		IP_VS_ERR_RL("%s: %s %pI4:%d - %s\n",
-			     sched->name, ip_vs_proto_name(svc->protocol),
-			     &svc->addr.ip, ntohs(svc->port), msg);
-	}
-}
 
 /*
  *  Register a scheduler in the scheduler list
@@ -187,10 +184,10 @@ int register_ip_vs_scheduler(struct ip_vs_scheduler *scheduler)
 	/* increase the module use count */
 	ip_vs_use_count_inc();
 
-	mutex_lock(&ip_vs_sched_mutex);
+	write_lock_bh(&__ip_vs_sched_lock);
 
 	if (!list_empty(&scheduler->n_list)) {
-		mutex_unlock(&ip_vs_sched_mutex);
+		write_unlock_bh(&__ip_vs_sched_lock);
 		ip_vs_use_count_dec();
 		pr_err("%s(): [%s] scheduler already linked\n",
 		       __func__, scheduler->name);
@@ -203,7 +200,7 @@ int register_ip_vs_scheduler(struct ip_vs_scheduler *scheduler)
 	 */
 	list_for_each_entry(sched, &ip_vs_schedulers, n_list) {
 		if (strcmp(scheduler->name, sched->name) == 0) {
-			mutex_unlock(&ip_vs_sched_mutex);
+			write_unlock_bh(&__ip_vs_sched_lock);
 			ip_vs_use_count_dec();
 			pr_err("%s(): [%s] scheduler already existed "
 			       "in the system\n", __func__, scheduler->name);
@@ -214,7 +211,7 @@ int register_ip_vs_scheduler(struct ip_vs_scheduler *scheduler)
 	 *	Add it into the d-linked scheduler list
 	 */
 	list_add(&scheduler->n_list, &ip_vs_schedulers);
-	mutex_unlock(&ip_vs_sched_mutex);
+	write_unlock_bh(&__ip_vs_sched_lock);
 
 	pr_info("[%s] scheduler registered.\n", scheduler->name);
 
@@ -232,9 +229,9 @@ int unregister_ip_vs_scheduler(struct ip_vs_scheduler *scheduler)
 		return -EINVAL;
 	}
 
-	mutex_lock(&ip_vs_sched_mutex);
+	write_lock_bh(&__ip_vs_sched_lock);
 	if (list_empty(&scheduler->n_list)) {
-		mutex_unlock(&ip_vs_sched_mutex);
+		write_unlock_bh(&__ip_vs_sched_lock);
 		pr_err("%s(): [%s] scheduler is not in the list. failed\n",
 		       __func__, scheduler->name);
 		return -EINVAL;
@@ -244,7 +241,7 @@ int unregister_ip_vs_scheduler(struct ip_vs_scheduler *scheduler)
 	 *	Remove it from the d-linked scheduler list
 	 */
 	list_del(&scheduler->n_list);
-	mutex_unlock(&ip_vs_sched_mutex);
+	write_unlock_bh(&__ip_vs_sched_lock);
 
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
diff --git a/net/netfilter/ipvs/ip_vs_sed.c b/net/netfilter/ipvs/ip_vs_sed.c
index f320592..1ab75a9 100644
--- a/net/netfilter/ipvs/ip_vs_sed.c
+++ b/net/netfilter/ipvs/ip_vs_sed.c
@@ -79,7 +79,7 @@ ip_vs_sed_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	 * new connections.
 	 */
 
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 		if (!(dest->flags & IP_VS_DEST_F_OVERLOAD) &&
 		    atomic_read(&dest->weight) > 0) {
 			least = dest;
@@ -87,14 +87,14 @@ ip_vs_sed_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 			goto nextstage;
 		}
 	}
-	ip_vs_scheduler_err(svc, "no destination available");
+	IP_VS_ERR_RL("SED: no destination available\n");
 	return NULL;
 
 	/*
 	 *    Find the destination with the least load.
 	 */
   nextstage:
-	list_for_each_entry_continue_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 		doh = ip_vs_sed_dest_overhead(dest);
@@ -134,7 +134,6 @@ static int __init ip_vs_sed_init(void)
 static void __exit ip_vs_sed_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_sed_scheduler);
-	synchronize_rcu();
 }
 
 module_init(ip_vs_sed_init);
diff --git a/net/netfilter/ipvs/ip_vs_sh.c b/net/netfilter/ipvs/ip_vs_sh.c
index a65edfe..8e6cfd3 100644
--- a/net/netfilter/ipvs/ip_vs_sh.c
+++ b/net/netfilter/ipvs/ip_vs_sh.c
@@ -30,18 +30,12 @@
  * server is dead or overloaded, the load balancer can bypass the cache
  * server and send requests to the original server directly.
  *
- * The weight destination attribute can be used to control the
- * distribution of connections to the destinations in servernode. The
- * greater the weight, the more connections the destination
- * will receive.
- *
  */
 
 #define KMSG_COMPONENT "IPVS"
 #define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
 
 #include <linux/ip.h>
-#include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/skbuff.h>
@@ -53,7 +47,7 @@
  *      IPVS SH bucket
  */
 struct ip_vs_sh_bucket {
-	struct ip_vs_dest __rcu	*dest;	/* real server (cache) */
+	struct ip_vs_dest       *dest;          /* real server (cache) */
 };
 
 /*
@@ -66,15 +60,11 @@ struct ip_vs_sh_bucket {
 #define IP_VS_SH_TAB_SIZE               (1 << IP_VS_SH_TAB_BITS)
 #define IP_VS_SH_TAB_MASK               (IP_VS_SH_TAB_SIZE - 1)
 
-struct ip_vs_sh_state {
-	struct rcu_head			rcu_head;
-	struct ip_vs_sh_bucket		buckets[IP_VS_SH_TAB_SIZE];
-};
 
 /*
  *	Returns hash value for IPVS SH entry
  */
-static inline unsigned int ip_vs_sh_hashkey(int af, const union nf_inet_addr *addr)
+static inline unsigned ip_vs_sh_hashkey(int af, const union nf_inet_addr *addr)
 {
 	__be32 addr_fold = addr->ip;
 
@@ -91,9 +81,10 @@ static inline unsigned int ip_vs_sh_hashkey(int af, const union nf_inet_addr *ad
  *      Get ip_vs_dest associated with supplied parameters.
  */
 static inline struct ip_vs_dest *
-ip_vs_sh_get(int af, struct ip_vs_sh_state *s, const union nf_inet_addr *addr)
+ip_vs_sh_get(int af, struct ip_vs_sh_bucket *tbl,
+	     const union nf_inet_addr *addr)
 {
-	return rcu_dereference(s->buckets[ip_vs_sh_hashkey(af, addr)].dest);
+	return (tbl[ip_vs_sh_hashkey(af, addr)]).dest;
 }
 
 
@@ -101,43 +92,27 @@ ip_vs_sh_get(int af, struct ip_vs_sh_state *s, const union nf_inet_addr *addr)
  *      Assign all the hash buckets of the specified table with the service.
  */
 static int
-ip_vs_sh_reassign(struct ip_vs_sh_state *s, struct ip_vs_service *svc)
+ip_vs_sh_assign(struct ip_vs_sh_bucket *tbl, struct ip_vs_service *svc)
 {
 	int i;
 	struct ip_vs_sh_bucket *b;
 	struct list_head *p;
 	struct ip_vs_dest *dest;
-	int d_count;
-	bool empty;
 
-	b = &s->buckets[0];
+	b = tbl;
 	p = &svc->destinations;
-	empty = list_empty(p);
-	d_count = 0;
 	for (i=0; i<IP_VS_SH_TAB_SIZE; i++) {
-		dest = rcu_dereference_protected(b->dest, 1);
-		if (dest)
-			ip_vs_dest_put(dest);
-		if (empty)
-			RCU_INIT_POINTER(b->dest, NULL);
-		else {
+		if (list_empty(p)) {
+			b->dest = NULL;
+		} else {
 			if (p == &svc->destinations)
 				p = p->next;
 
 			dest = list_entry(p, struct ip_vs_dest, n_list);
-			ip_vs_dest_hold(dest);
-			RCU_INIT_POINTER(b->dest, dest);
-
-			IP_VS_DBG_BUF(6, "assigned i: %d dest: %s weight: %d\n",
-				      i, IP_VS_DBG_ADDR(svc->af, &dest->addr),
-				      atomic_read(&dest->weight));
-
-			/* Don't move to next dest until filling weight */
-			if (++d_count >= atomic_read(&dest->weight)) {
-				p = p->next;
-				d_count = 0;
-			}
+			atomic_inc(&dest->refcnt);
+			b->dest = dest;
 
+			p = p->next;
 		}
 		b++;
 	}
@@ -148,18 +123,16 @@ ip_vs_sh_reassign(struct ip_vs_sh_state *s, struct ip_vs_service *svc)
 /*
  *      Flush all the hash buckets of the specified table.
  */
-static void ip_vs_sh_flush(struct ip_vs_sh_state *s)
+static void ip_vs_sh_flush(struct ip_vs_sh_bucket *tbl)
 {
 	int i;
 	struct ip_vs_sh_bucket *b;
-	struct ip_vs_dest *dest;
 
-	b = &s->buckets[0];
+	b = tbl;
 	for (i=0; i<IP_VS_SH_TAB_SIZE; i++) {
-		dest = rcu_dereference_protected(b->dest, 1);
-		if (dest) {
-			ip_vs_dest_put(dest);
-			RCU_INIT_POINTER(b->dest, NULL);
+		if (b->dest) {
+			atomic_dec(&b->dest->refcnt);
+			b->dest = NULL;
 		}
 		b++;
 	}
@@ -168,46 +141,52 @@ static void ip_vs_sh_flush(struct ip_vs_sh_state *s)
 
 static int ip_vs_sh_init_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_sh_state *s;
+	struct ip_vs_sh_bucket *tbl;
 
 	/* allocate the SH table for this service */
-	s = kzalloc(sizeof(struct ip_vs_sh_state), GFP_KERNEL);
-	if (s == NULL)
+	tbl = kmalloc(sizeof(struct ip_vs_sh_bucket)*IP_VS_SH_TAB_SIZE,
+		      GFP_ATOMIC);
+	if (tbl == NULL) {
+		pr_err("%s(): no memory\n", __func__);
 		return -ENOMEM;
-
-	svc->sched_data = s;
+	}
+	svc->sched_data = tbl;
 	IP_VS_DBG(6, "SH hash table (memory=%Zdbytes) allocated for "
 		  "current service\n",
 		  sizeof(struct ip_vs_sh_bucket)*IP_VS_SH_TAB_SIZE);
 
-	/* assign the hash buckets with current dests */
-	ip_vs_sh_reassign(s, svc);
+	/* assign the hash buckets with the updated service */
+	ip_vs_sh_assign(tbl, svc);
 
 	return 0;
 }
 
 
-static void ip_vs_sh_done_svc(struct ip_vs_service *svc)
+static int ip_vs_sh_done_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_sh_state *s = svc->sched_data;
+	struct ip_vs_sh_bucket *tbl = svc->sched_data;
 
 	/* got to clean up hash buckets here */
-	ip_vs_sh_flush(s);
+	ip_vs_sh_flush(tbl);
 
 	/* release the table itself */
-	kfree_rcu(s, rcu_head);
+	kfree(svc->sched_data);
 	IP_VS_DBG(6, "SH hash table (memory=%Zdbytes) released\n",
 		  sizeof(struct ip_vs_sh_bucket)*IP_VS_SH_TAB_SIZE);
+
+	return 0;
 }
 
 
-static int ip_vs_sh_dest_changed(struct ip_vs_service *svc,
-				 struct ip_vs_dest *dest)
+static int ip_vs_sh_update_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_sh_state *s = svc->sched_data;
+	struct ip_vs_sh_bucket *tbl = svc->sched_data;
+
+	/* got to clean up hash buckets here */
+	ip_vs_sh_flush(tbl);
 
 	/* assign the hash buckets with the updated service */
-	ip_vs_sh_reassign(s, svc);
+	ip_vs_sh_assign(tbl, svc);
 
 	return 0;
 }
@@ -230,20 +209,20 @@ static struct ip_vs_dest *
 ip_vs_sh_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest;
-	struct ip_vs_sh_state *s;
+	struct ip_vs_sh_bucket *tbl;
 	struct ip_vs_iphdr iph;
 
-	ip_vs_fill_iph_addr_only(svc->af, skb, &iph);
+	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
 
 	IP_VS_DBG(6, "ip_vs_sh_schedule(): Scheduling...\n");
 
-	s = (struct ip_vs_sh_state *) svc->sched_data;
-	dest = ip_vs_sh_get(svc->af, s, &iph.saddr);
+	tbl = (struct ip_vs_sh_bucket *)svc->sched_data;
+	dest = ip_vs_sh_get(svc->af, tbl, &iph.saddr);
 	if (!dest
 	    || !(dest->flags & IP_VS_DEST_F_AVAILABLE)
 	    || atomic_read(&dest->weight) <= 0
 	    || is_overloaded(dest)) {
-		ip_vs_scheduler_err(svc, "no destination available");
+		IP_VS_ERR_RL("SH: no destination available\n");
 		return NULL;
 	}
 
@@ -267,9 +246,7 @@ static struct ip_vs_scheduler ip_vs_sh_scheduler =
 	.n_list	 =		LIST_HEAD_INIT(ip_vs_sh_scheduler.n_list),
 	.init_service =		ip_vs_sh_init_svc,
 	.done_service =		ip_vs_sh_done_svc,
-	.add_dest =		ip_vs_sh_dest_changed,
-	.del_dest =		ip_vs_sh_dest_changed,
-	.upd_dest =		ip_vs_sh_dest_changed,
+	.update_service =	ip_vs_sh_update_svc,
 	.schedule =		ip_vs_sh_schedule,
 };
 
@@ -283,7 +260,6 @@ static int __init ip_vs_sh_init(void)
 static void __exit ip_vs_sh_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_sh_scheduler);
-	synchronize_rcu();
 }
 
 
diff --git a/net/netfilter/ipvs/ip_vs_stats.c b/net/netfilter/ipvs/ip_vs_stats.c
new file mode 100644
index 0000000..3557e5d
--- /dev/null
+++ b/net/netfilter/ipvs/ip_vs_stats.c
@@ -0,0 +1,99 @@
+#include <linux/types.h>
+#include <linux/percpu.h>
+#include <net/ip_vs.h>
+
+
+int ip_vs_new_stats(struct ip_vs_stats **p)
+{
+	if(NULL == p)
+		return -EINVAL;
+
+	*p = alloc_percpu(struct ip_vs_stats);
+	if(NULL == *p) {
+		pr_err("%s: allocate per cpu varible failed \n", __func__);
+		return -ENOMEM;
+	}
+
+	/* Initial stats */
+	ip_vs_zero_stats(*p);
+
+	return 0;
+}
+
+void ip_vs_del_stats(struct ip_vs_stats* p)
+{
+	if(NULL == p)
+		return;
+
+	free_percpu(p);
+
+	return;
+}
+
+void ip_vs_zero_stats(struct ip_vs_stats* stats)
+{
+	int i = 0;
+
+	if(NULL == stats) {
+		pr_err("%s: Invaild point \n", __func__);
+		return;
+	}
+
+	for_each_online_cpu(i) {
+		ip_vs_stats_cpu(stats, i).conns    = 0;
+		ip_vs_stats_cpu(stats, i).inpkts   = 0;
+		ip_vs_stats_cpu(stats, i).outpkts  = 0;
+		ip_vs_stats_cpu(stats, i).inbytes  = 0;
+		ip_vs_stats_cpu(stats, i).outbytes = 0;
+	}
+
+	return;
+}
+
+void ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
+		ip_vs_stats_this_cpu(dest->stats).inpkts++;
+		ip_vs_stats_this_cpu(dest->stats).inbytes += skb->len;
+
+		ip_vs_stats_this_cpu(dest->svc->stats).inpkts++;
+		ip_vs_stats_this_cpu(dest->svc->stats).inbytes += skb->len;
+
+		ip_vs_stats_this_cpu(ip_vs_stats).inpkts++;
+		ip_vs_stats_this_cpu(ip_vs_stats).inbytes += skb->len;
+	}
+
+	return;
+}
+
+void ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
+		ip_vs_stats_this_cpu(dest->stats).outpkts++;
+		ip_vs_stats_this_cpu(dest->stats).outbytes += skb->len;
+
+		ip_vs_stats_this_cpu(dest->svc->stats).outpkts++;
+		ip_vs_stats_this_cpu(dest->svc->stats).outbytes += skb->len;
+
+		ip_vs_stats_this_cpu(ip_vs_stats).outpkts++;
+		ip_vs_stats_this_cpu(ip_vs_stats).outbytes += skb->len;
+	}
+	return;
+}
+
+void ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	if(dest) {
+		ip_vs_stats_this_cpu(dest->stats).conns++;
+
+		ip_vs_stats_this_cpu(dest->svc->stats).conns++;
+
+		ip_vs_stats_this_cpu(ip_vs_stats).conns++;
+	}
+
+	return;
+}
+
diff --git a/net/netfilter/ipvs/ip_vs_sync.c b/net/netfilter/ipvs/ip_vs_sync.c
index a2d8103..f1a122a 100644
--- a/net/netfilter/ipvs/ip_vs_sync.c
+++ b/net/netfilter/ipvs/ip_vs_sync.c
@@ -5,18 +5,6 @@
  *              high-performance and highly available server based on a
  *              cluster of servers.
  *
- * Version 1,   is capable of handling both version 0 and 1 messages.
- *              Version 0 is the plain old format.
- *              Note Version 0 receivers will just drop Ver 1 messages.
- *              Version 1 is capable of handle IPv6, Persistence data,
- *              time-outs, and firewall marks.
- *              In ver.1 "ip_vs_sync_conn_options" will be sent in netw. order.
- *              Ver. 0 can be turned on by sysctl -w net.ipv4.vs.sync_version=0
- *
- * Definitions  Message: is a complete datagram
- *              Sync_conn: is a part of a Message
- *              Param Data is an option to a Sync_conn.
- *
  * Authors:     Wensong Zhang <wensong@linuxvirtualserver.org>
  *
  * ip_vs_sync:  sync connection info from master load balancer to backups
@@ -27,8 +15,6 @@
  *	Alexandre Cassen	:	Added SyncID support for incoming sync
  *					messages filtering.
  *	Justin Ossevoort	:	Fix endian problem on sync message size.
- *	Hans Schillstrom	:	Added Version 1: i.e. IPv6,
- *					Persistence support, fwmark and time-out.
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -42,305 +28,164 @@
 #include <linux/delay.h>
 #include <linux/skbuff.h>
 #include <linux/in.h>
-#include <linux/igmp.h>                 /* for ip_mc_join_group */
+#include <linux/igmp.h>		/* for ip_mc_join_group */
 #include <linux/udp.h>
 #include <linux/err.h>
 #include <linux/kthread.h>
 #include <linux/wait.h>
 #include <linux/kernel.h>
 
-#include <asm/unaligned.h>		/* Used for ntoh_seq and hton_seq */
-
 #include <net/ip.h>
 #include <net/sock.h>
 
 #include <net/ip_vs.h>
 
-#define IP_VS_SYNC_GROUP 0xe0000051    /* multicast addr - 224.0.0.81 */
-#define IP_VS_SYNC_PORT  8848          /* multicast port */
+#define IP_VS_SYNC_GROUP 0xe0000051	/* multicast addr - 224.0.0.81 */
+#define IP_VS_SYNC_PORT  8848	/* multicast port */
 
-#define SYNC_PROTO_VER  1		/* Protocol version in header */
-
-static struct lock_class_key __ipvs_sync_key;
 /*
  *	IPVS sync connection entry
- *	Version 0, i.e. original version.
  */
-struct ip_vs_sync_conn_v0 {
-	__u8			reserved;
+struct ip_vs_sync_conn {
+	__u8 reserved;
 
 	/* Protocol, addresses and port numbers */
-	__u8			protocol;       /* Which protocol (TCP/UDP) */
-	__be16			cport;
-	__be16                  vport;
-	__be16                  dport;
-	__be32                  caddr;          /* client address */
-	__be32                  vaddr;          /* virtual address */
-	__be32                  daddr;          /* destination address */
+	__u8 protocol;		/* Which protocol (TCP/UDP) */
+	__be16 cport;
+	__be16 vport;
+	__be16 dport;
+	__be32 caddr;		/* client address */
+	__be32 vaddr;		/* virtual address */
+	__be32 daddr;		/* destination address */
 
 	/* Flags and state transition */
-	__be16                  flags;          /* status flags */
-	__be16                  state;          /* state info */
+	__be16 flags;		/* status flags */
+	__be16 state;		/* state info */
 
 	/* The sequence options start here */
 };
 
 struct ip_vs_sync_conn_options {
-	struct ip_vs_seq        in_seq;         /* incoming seq. struct */
-	struct ip_vs_seq        out_seq;        /* outgoing seq. struct */
+	struct ip_vs_seq in_seq;	/* incoming seq. struct */
+	struct ip_vs_seq out_seq;	/* outgoing seq. struct */
 };
 
-/*
-     Sync Connection format (sync_conn)
-
-       0                   1                   2                   3
-       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |    Type       |    Protocol   | Ver.  |        Size           |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                             Flags                             |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |            State              |         cport                 |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |            vport              |         dport                 |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                             fwmark                            |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                             timeout  (in sec.)                |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                              ...                              |
-      |                        IP-Addresses  (v4 or v6)               |
-      |                              ...                              |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-  Optional Parameters.
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      | Param. Type    | Param. Length |   Param. data                |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                               |
-      |                              ...                              |
-      |                               +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                               | Param Type    | Param. Length |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                           Param  data                         |
-      |         Last Param data should be padded for 32 bit alignment |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-*/
-
-/*
- *  Type 0, IPv4 sync connection format
- */
-struct ip_vs_sync_v4 {
-	__u8			type;
-	__u8			protocol;	/* Which protocol (TCP/UDP) */
-	__be16			ver_size;	/* Version msb 4 bits */
-	/* Flags and state transition */
-	__be32			flags;		/* status flags */
-	__be16			state;		/* state info 	*/
-	/* Protocol, addresses and port numbers */
-	__be16			cport;
-	__be16			vport;
-	__be16			dport;
-	__be32			fwmark;		/* Firewall mark from skb */
-	__be32			timeout;	/* cp timeout */
-	__be32			caddr;		/* client address */
-	__be32			vaddr;		/* virtual address */
-	__be32			daddr;		/* destination address */
-	/* The sequence options start here */
-	/* PE data padded to 32bit alignment after seq. options */
-};
-/*
- * Type 2 messages IPv6
- */
-struct ip_vs_sync_v6 {
-	__u8			type;
-	__u8			protocol;	/* Which protocol (TCP/UDP) */
-	__be16			ver_size;	/* Version msb 4 bits */
-	/* Flags and state transition */
-	__be32			flags;		/* status flags */
-	__be16			state;		/* state info 	*/
-	/* Protocol, addresses and port numbers */
-	__be16			cport;
-	__be16			vport;
-	__be16			dport;
-	__be32			fwmark;		/* Firewall mark from skb */
-	__be32			timeout;	/* cp timeout */
-	struct in6_addr		caddr;		/* client address */
-	struct in6_addr		vaddr;		/* virtual address */
-	struct in6_addr		daddr;		/* destination address */
-	/* The sequence options start here */
-	/* PE data padded to 32bit alignment after seq. options */
-};
-
-union ip_vs_sync_conn {
-	struct ip_vs_sync_v4	v4;
-	struct ip_vs_sync_v6	v6;
-};
-
-/* Bits in Type field in above */
-#define STYPE_INET6		0
-#define STYPE_F_INET6		(1 << STYPE_INET6)
-
-#define SVER_SHIFT		12		/* Shift to get version */
-#define SVER_MASK		0x0fff		/* Mask to strip version */
-
-#define IPVS_OPT_SEQ_DATA	1
-#define IPVS_OPT_PE_DATA	2
-#define IPVS_OPT_PE_NAME	3
-#define IPVS_OPT_PARAM		7
-
-#define IPVS_OPT_F_SEQ_DATA	(1 << (IPVS_OPT_SEQ_DATA-1))
-#define IPVS_OPT_F_PE_DATA	(1 << (IPVS_OPT_PE_DATA-1))
-#define IPVS_OPT_F_PE_NAME	(1 << (IPVS_OPT_PE_NAME-1))
-#define IPVS_OPT_F_PARAM	(1 << (IPVS_OPT_PARAM-1))
-
 struct ip_vs_sync_thread_data {
-	struct net *net;
 	struct socket *sock;
 	char *buf;
-	int id;
 };
 
-/* Version 0 definition of packet sizes */
-#define SIMPLE_CONN_SIZE  (sizeof(struct ip_vs_sync_conn_v0))
+#define SIMPLE_CONN_SIZE  (sizeof(struct ip_vs_sync_conn))
 #define FULL_CONN_SIZE  \
-(sizeof(struct ip_vs_sync_conn_v0) + sizeof(struct ip_vs_sync_conn_options))
-
+(sizeof(struct ip_vs_sync_conn) + sizeof(struct ip_vs_sync_conn_options))
 
 /*
-  The master mulitcasts messages (Datagrams) to the backup load balancers
-  in the following format.
-
- Version 1:
-  Note, first byte should be Zero, so ver 0 receivers will drop the packet.
+  The master mulitcasts messages to the backup load balancers in the
+  following format.
 
        0                   1                   2                   3
        0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |      0        |    SyncID     |            Size               |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |  Count Conns  |    Version    |    Reserved, set to Zero      |
+      |  Count Conns  |    SyncID     |            Size               |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                                                               |
       |                    IPVS Sync Connection (1)                   |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                            .                                  |
-      ~                            .                                  ~
+      |                            .                                  |
       |                            .                                  |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                                                               |
       |                    IPVS Sync Connection (n)                   |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-
- Version 0 Header
-       0                   1                   2                   3
-       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |  Count Conns  |    SyncID     |            Size               |
-      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
-      |                    IPVS Sync Connection (1)                   |
 */
 
 #define SYNC_MESG_HEADER_LEN	4
-#define MAX_CONNS_PER_SYNCBUFF	255 /* nr_conns in ip_vs_sync_mesg is 8 bit */
+#define MAX_CONNS_PER_SYNCBUFF	255	/* nr_conns in ip_vs_sync_mesg is 8 bit */
 
-/* Version 0 header */
-struct ip_vs_sync_mesg_v0 {
-	__u8                    nr_conns;
-	__u8                    syncid;
-	__be16                  size;
+struct ip_vs_sync_mesg {
+	__u8 nr_conns;
+	__u8 syncid;
+	__u16 size;
 
 	/* ip_vs_sync_conn entries start here */
 };
 
-/* Version 1 header */
-struct ip_vs_sync_mesg {
-	__u8			reserved;	/* must be zero */
-	__u8			syncid;
-	__be16			size;
-	__u8			nr_conns;
-	__s8			version;	/* SYNC_PROTO_VER  */
-	__u16			spare;
-	/* ip_vs_sync_conn entries start here */
-};
+/* the maximum length of sync (sending/receiving) message */
+static int sync_send_mesg_maxlen;
+static int sync_recv_mesg_maxlen;
 
 struct ip_vs_sync_buff {
-	struct list_head        list;
-	unsigned long           firstuse;
+	struct list_head list;
+	unsigned long firstuse;
 
 	/* pointers for the message data */
-	struct ip_vs_sync_mesg  *mesg;
-	unsigned char           *head;
-	unsigned char           *end;
+	struct ip_vs_sync_mesg *mesg;
+	unsigned char *head;
+	unsigned char *end;
 };
 
-/*
- * Copy of struct ip_vs_seq
- * From unaligned network order to aligned host order
- */
-static void ntoh_seq(struct ip_vs_seq *no, struct ip_vs_seq *ho)
-{
-	ho->init_seq       = get_unaligned_be32(&no->init_seq);
-	ho->delta          = get_unaligned_be32(&no->delta);
-	ho->previous_delta = get_unaligned_be32(&no->previous_delta);
-}
+/* the sync_buff list head and the lock */
+static LIST_HEAD(ip_vs_sync_queue);
+static DEFINE_SPINLOCK(ip_vs_sync_lock);
 
-/*
- * Copy of struct ip_vs_seq
- * From Aligned host order to unaligned network order
- */
-static void hton_seq(struct ip_vs_seq *ho, struct ip_vs_seq *no)
-{
-	put_unaligned_be32(ho->init_seq, &no->init_seq);
-	put_unaligned_be32(ho->delta, &no->delta);
-	put_unaligned_be32(ho->previous_delta, &no->previous_delta);
-}
+/* current sync_buff for accepting new conn entries */
+static struct ip_vs_sync_buff *curr_sb = NULL;
+static DEFINE_SPINLOCK(curr_sb_lock);
+
+/* ipvs sync daemon state */
+volatile int ip_vs_sync_state = IP_VS_STATE_NONE;
+volatile int ip_vs_master_syncid = 0;
+volatile int ip_vs_backup_syncid = 0;
 
-static inline struct ip_vs_sync_buff *
-sb_dequeue(struct netns_ipvs *ipvs, struct ipvs_master_sync_state *ms)
+/* multicast interface name */
+char ip_vs_master_mcast_ifn[IP_VS_IFNAME_MAXLEN];
+char ip_vs_backup_mcast_ifn[IP_VS_IFNAME_MAXLEN];
+
+/* sync daemon tasks */
+static struct task_struct *sync_master_thread;
+static struct task_struct *sync_backup_thread;
+
+/* multicast addr */
+static struct sockaddr_in mcast_addr = {
+	.sin_family = AF_INET,
+	.sin_port = cpu_to_be16(IP_VS_SYNC_PORT),
+	.sin_addr.s_addr = cpu_to_be32(IP_VS_SYNC_GROUP),
+};
+
+static inline struct ip_vs_sync_buff *sb_dequeue(void)
 {
 	struct ip_vs_sync_buff *sb;
 
-	spin_lock_bh(&ipvs->sync_lock);
-	if (list_empty(&ms->sync_queue)) {
+	spin_lock_bh(&ip_vs_sync_lock);
+	if (list_empty(&ip_vs_sync_queue)) {
 		sb = NULL;
-		__set_current_state(TASK_INTERRUPTIBLE);
 	} else {
-		sb = list_entry(ms->sync_queue.next, struct ip_vs_sync_buff,
-				list);
+		sb = list_entry(ip_vs_sync_queue.next,
+				struct ip_vs_sync_buff, list);
 		list_del(&sb->list);
-		ms->sync_queue_len--;
-		if (!ms->sync_queue_len)
-			ms->sync_queue_delay = 0;
 	}
-	spin_unlock_bh(&ipvs->sync_lock);
+	spin_unlock_bh(&ip_vs_sync_lock);
 
 	return sb;
 }
 
-/*
- * Create a new sync buffer for Version 1 proto.
- */
-static inline struct ip_vs_sync_buff *
-ip_vs_sync_buff_create(struct netns_ipvs *ipvs)
+static inline struct ip_vs_sync_buff *ip_vs_sync_buff_create(void)
 {
 	struct ip_vs_sync_buff *sb;
 
-	if (!(sb=kmalloc(sizeof(struct ip_vs_sync_buff), GFP_ATOMIC)))
+	if (!(sb = kmalloc(sizeof(struct ip_vs_sync_buff), GFP_ATOMIC)))
 		return NULL;
 
-	sb->mesg = kmalloc(ipvs->send_mesg_maxlen, GFP_ATOMIC);
-	if (!sb->mesg) {
+	if (!(sb->mesg = kmalloc(sync_send_mesg_maxlen, GFP_ATOMIC))) {
 		kfree(sb);
 		return NULL;
 	}
-	sb->mesg->reserved = 0;  /* old nr_conns i.e. must be zero now */
-	sb->mesg->version = SYNC_PROTO_VER;
-	sb->mesg->syncid = ipvs->master_syncid;
-	sb->mesg->size = htons(sizeof(struct ip_vs_sync_mesg));
 	sb->mesg->nr_conns = 0;
-	sb->mesg->spare = 0;
-	sb->head = (unsigned char *)sb->mesg + sizeof(struct ip_vs_sync_mesg);
-	sb->end = (unsigned char *)sb->mesg + ipvs->send_mesg_maxlen;
-
+	sb->mesg->syncid = ip_vs_master_syncid;
+	sb->mesg->size = 4;
+	sb->head = (unsigned char *)sb->mesg + 4;
+	sb->end = (unsigned char *)sb->mesg + sync_send_mesg_maxlen;
 	sb->firstuse = jiffies;
 	return sb;
 }
@@ -351,221 +196,60 @@ static inline void ip_vs_sync_buff_release(struct ip_vs_sync_buff *sb)
 	kfree(sb);
 }
 
-static inline void sb_queue_tail(struct netns_ipvs *ipvs,
-				 struct ipvs_master_sync_state *ms)
+static inline void sb_queue_tail(struct ip_vs_sync_buff *sb)
 {
-	struct ip_vs_sync_buff *sb = ms->sync_buff;
-
-	spin_lock(&ipvs->sync_lock);
-	if (ipvs->sync_state & IP_VS_STATE_MASTER &&
-	    ms->sync_queue_len < sysctl_sync_qlen_max(ipvs)) {
-		if (!ms->sync_queue_len)
-			schedule_delayed_work(&ms->master_wakeup_work,
-					      max(IPVS_SYNC_SEND_DELAY, 1));
-		ms->sync_queue_len++;
-		list_add_tail(&sb->list, &ms->sync_queue);
-		if ((++ms->sync_queue_delay) == IPVS_SYNC_WAKEUP_RATE)
-			wake_up_process(ms->master_thread);
-	} else
+	spin_lock(&ip_vs_sync_lock);
+	if (ip_vs_sync_state & IP_VS_STATE_MASTER)
+		list_add_tail(&sb->list, &ip_vs_sync_queue);
+	else
 		ip_vs_sync_buff_release(sb);
-	spin_unlock(&ipvs->sync_lock);
+	spin_unlock(&ip_vs_sync_lock);
 }
 
 /*
  *	Get the current sync buffer if it has been created for more
  *	than the specified time or the specified time is zero.
  */
-static inline struct ip_vs_sync_buff *
-get_curr_sync_buff(struct netns_ipvs *ipvs, struct ipvs_master_sync_state *ms,
-		   unsigned long time)
+static inline struct ip_vs_sync_buff *get_curr_sync_buff(unsigned long time)
 {
 	struct ip_vs_sync_buff *sb;
 
-	spin_lock_bh(&ipvs->sync_buff_lock);
-	sb = ms->sync_buff;
-	if (sb && time_after_eq(jiffies - sb->firstuse, time)) {
-		ms->sync_buff = NULL;
-		__set_current_state(TASK_RUNNING);
+	spin_lock_bh(&curr_sb_lock);
+	if (curr_sb && (time == 0 ||
+			time_before(jiffies - curr_sb->firstuse, time))) {
+		sb = curr_sb;
+		curr_sb = NULL;
 	} else
 		sb = NULL;
-	spin_unlock_bh(&ipvs->sync_buff_lock);
+	spin_unlock_bh(&curr_sb_lock);
 	return sb;
 }
 
-static inline int
-select_master_thread_id(struct netns_ipvs *ipvs, struct ip_vs_conn *cp)
-{
-	return ((long) cp >> (1 + ilog2(sizeof(*cp)))) & ipvs->threads_mask;
-}
-
 /*
- * Create a new sync buffer for Version 0 proto.
- */
-static inline struct ip_vs_sync_buff *
-ip_vs_sync_buff_create_v0(struct netns_ipvs *ipvs)
-{
-	struct ip_vs_sync_buff *sb;
-	struct ip_vs_sync_mesg_v0 *mesg;
-
-	if (!(sb=kmalloc(sizeof(struct ip_vs_sync_buff), GFP_ATOMIC)))
-		return NULL;
-
-	sb->mesg = kmalloc(ipvs->send_mesg_maxlen, GFP_ATOMIC);
-	if (!sb->mesg) {
-		kfree(sb);
-		return NULL;
-	}
-	mesg = (struct ip_vs_sync_mesg_v0 *)sb->mesg;
-	mesg->nr_conns = 0;
-	mesg->syncid = ipvs->master_syncid;
-	mesg->size = htons(sizeof(struct ip_vs_sync_mesg_v0));
-	sb->head = (unsigned char *)mesg + sizeof(struct ip_vs_sync_mesg_v0);
-	sb->end = (unsigned char *)mesg + ipvs->send_mesg_maxlen;
-	sb->firstuse = jiffies;
-	return sb;
-}
-
-/* Check if conn should be synced.
- * pkts: conn packets, use sysctl_sync_threshold to avoid packet check
- * - (1) sync_refresh_period: reduce sync rate. Additionally, retry
- *	sync_retries times with period of sync_refresh_period/8
- * - (2) if both sync_refresh_period and sync_period are 0 send sync only
- *	for state changes or only once when pkts matches sync_threshold
- * - (3) templates: rate can be reduced only with sync_refresh_period or
- *	with (2)
- */
-static int ip_vs_sync_conn_needed(struct netns_ipvs *ipvs,
-				  struct ip_vs_conn *cp, int pkts)
-{
-	unsigned long orig = ACCESS_ONCE(cp->sync_endtime);
-	unsigned long now = jiffies;
-	unsigned long n = (now + cp->timeout) & ~3UL;
-	unsigned int sync_refresh_period;
-	int sync_period;
-	int force;
-
-	/* Check if we sync in current state */
-	if (unlikely(cp->flags & IP_VS_CONN_F_TEMPLATE))
-		force = 0;
-	else if (likely(cp->protocol == IPPROTO_TCP)) {
-		if (!((1 << cp->state) &
-		      ((1 << IP_VS_TCP_S_ESTABLISHED) |
-		       (1 << IP_VS_TCP_S_FIN_WAIT) |
-		       (1 << IP_VS_TCP_S_CLOSE) |
-		       (1 << IP_VS_TCP_S_CLOSE_WAIT) |
-		       (1 << IP_VS_TCP_S_TIME_WAIT))))
-			return 0;
-		force = cp->state != cp->old_state;
-		if (force && cp->state != IP_VS_TCP_S_ESTABLISHED)
-			goto set;
-	} else if (unlikely(cp->protocol == IPPROTO_SCTP)) {
-		if (!((1 << cp->state) &
-		      ((1 << IP_VS_SCTP_S_ESTABLISHED) |
-		       (1 << IP_VS_SCTP_S_CLOSED) |
-		       (1 << IP_VS_SCTP_S_SHUT_ACK_CLI) |
-		       (1 << IP_VS_SCTP_S_SHUT_ACK_SER))))
-			return 0;
-		force = cp->state != cp->old_state;
-		if (force && cp->state != IP_VS_SCTP_S_ESTABLISHED)
-			goto set;
-	} else {
-		/* UDP or another protocol with single state */
-		force = 0;
-	}
-
-	sync_refresh_period = sysctl_sync_refresh_period(ipvs);
-	if (sync_refresh_period > 0) {
-		long diff = n - orig;
-		long min_diff = max(cp->timeout >> 1, 10UL * HZ);
-
-		/* Avoid sync if difference is below sync_refresh_period
-		 * and below the half timeout.
-		 */
-		if (abs(diff) < min_t(long, sync_refresh_period, min_diff)) {
-			int retries = orig & 3;
-
-			if (retries >= sysctl_sync_retries(ipvs))
-				return 0;
-			if (time_before(now, orig - cp->timeout +
-					(sync_refresh_period >> 3)))
-				return 0;
-			n |= retries + 1;
-		}
-	}
-	sync_period = sysctl_sync_period(ipvs);
-	if (sync_period > 0) {
-		if (!(cp->flags & IP_VS_CONN_F_TEMPLATE) &&
-		    pkts % sync_period != sysctl_sync_threshold(ipvs))
-			return 0;
-	} else if (sync_refresh_period <= 0 &&
-		   pkts != sysctl_sync_threshold(ipvs))
-		return 0;
-
-set:
-	cp->old_state = cp->state;
-	n = cmpxchg(&cp->sync_endtime, orig, n);
-	return n == orig || force;
-}
-
-/*
- *      Version 0 , could be switched in by sys_ctl.
  *      Add an ip_vs_conn information into the current sync_buff.
+ *      Called by ip_vs_in.
  */
-static void ip_vs_sync_conn_v0(struct net *net, struct ip_vs_conn *cp,
-			       int pkts)
+void ip_vs_sync_conn(struct ip_vs_conn *cp)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_sync_mesg_v0 *m;
-	struct ip_vs_sync_conn_v0 *s;
-	struct ip_vs_sync_buff *buff;
-	struct ipvs_master_sync_state *ms;
-	int id;
+	struct ip_vs_sync_mesg *m;
+	struct ip_vs_sync_conn *s;
 	int len;
 
-	if (unlikely(cp->af != AF_INET))
-		return;
-	/* Do not sync ONE PACKET */
-	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
-		return;
-
-	if (!ip_vs_sync_conn_needed(ipvs, cp, pkts))
-		return;
-
-	spin_lock_bh(&ipvs->sync_buff_lock);
-	if (!(ipvs->sync_state & IP_VS_STATE_MASTER)) {
-		spin_unlock_bh(&ipvs->sync_buff_lock);
-		return;
-	}
-
-	id = select_master_thread_id(ipvs, cp);
-	ms = &ipvs->ms[id];
-	buff = ms->sync_buff;
-	if (buff) {
-		m = (struct ip_vs_sync_mesg_v0 *) buff->mesg;
-		/* Send buffer if it is for v1 */
-		if (!m->nr_conns) {
-			sb_queue_tail(ipvs, ms);
-			ms->sync_buff = NULL;
-			buff = NULL;
-		}
-	}
-	if (!buff) {
-		buff = ip_vs_sync_buff_create_v0(ipvs);
-		if (!buff) {
-			spin_unlock_bh(&ipvs->sync_buff_lock);
+	spin_lock(&curr_sb_lock);
+	if (!curr_sb) {
+		if (!(curr_sb = ip_vs_sync_buff_create())) {
+			spin_unlock(&curr_sb_lock);
 			pr_err("ip_vs_sync_buff_create failed.\n");
 			return;
 		}
-		ms->sync_buff = buff;
 	}
 
 	len = (cp->flags & IP_VS_CONN_F_SEQ_MASK) ? FULL_CONN_SIZE :
-		SIMPLE_CONN_SIZE;
-	m = (struct ip_vs_sync_mesg_v0 *) buff->mesg;
-	s = (struct ip_vs_sync_conn_v0 *) buff->head;
+	    SIMPLE_CONN_SIZE;
+	m = curr_sb->mesg;
+	s = (struct ip_vs_sync_conn *)curr_sb->head;
 
 	/* copy members */
-	s->reserved = 0;
 	s->protocol = cp->protocol;
 	s->cport = cp->cport;
 	s->vport = cp->vport;
@@ -577,386 +261,79 @@ static void ip_vs_sync_conn_v0(struct net *net, struct ip_vs_conn *cp,
 	s->state = htons(cp->state);
 	if (cp->flags & IP_VS_CONN_F_SEQ_MASK) {
 		struct ip_vs_sync_conn_options *opt =
-			(struct ip_vs_sync_conn_options *)&s[1];
+		    (struct ip_vs_sync_conn_options *)&s[1];
 		memcpy(opt, &cp->in_seq, sizeof(*opt));
 	}
 
 	m->nr_conns++;
-	m->size = htons(ntohs(m->size) + len);
-	buff->head += len;
+	m->size += len;
+	curr_sb->head += len;
 
 	/* check if there is a space for next one */
-	if (buff->head + FULL_CONN_SIZE > buff->end) {
-		sb_queue_tail(ipvs, ms);
-		ms->sync_buff = NULL;
+	if (curr_sb->head + FULL_CONN_SIZE > curr_sb->end) {
+		sb_queue_tail(curr_sb);
+		curr_sb = NULL;
 	}
-	spin_unlock_bh(&ipvs->sync_buff_lock);
+	spin_unlock(&curr_sb_lock);
 
 	/* synchronize its controller if it has */
-	cp = cp->control;
-	if (cp) {
-		if (cp->flags & IP_VS_CONN_F_TEMPLATE)
-			pkts = atomic_add_return(1, &cp->in_pkts);
-		else
-			pkts = sysctl_sync_threshold(ipvs);
-		ip_vs_sync_conn(net, cp->control, pkts);
-	}
+	if (cp->control)
+		ip_vs_sync_conn(cp->control);
 }
 
 /*
- *      Add an ip_vs_conn information into the current sync_buff.
- *      Called by ip_vs_in.
- *      Sending Version 1 messages
+ *      Process received multicast message and create the corresponding
+ *      ip_vs_conn entries.
  */
-void ip_vs_sync_conn(struct net *net, struct ip_vs_conn *cp, int pkts)
+static void ip_vs_process_message(const char *buffer, const size_t buflen)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_sync_mesg *m;
-	union ip_vs_sync_conn *s;
-	struct ip_vs_sync_buff *buff;
-	struct ipvs_master_sync_state *ms;
-	int id;
-	__u8 *p;
-	unsigned int len, pe_name_len, pad;
-
-	/* Handle old version of the protocol */
-	if (sysctl_sync_ver(ipvs) == 0) {
-		ip_vs_sync_conn_v0(net, cp, pkts);
-		return;
-	}
-	/* Do not sync ONE PACKET */
-	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
-		goto control;
-sloop:
-	if (!ip_vs_sync_conn_needed(ipvs, cp, pkts))
-		goto control;
-
-	/* Sanity checks */
-	pe_name_len = 0;
-	if (cp->pe_data_len) {
-		if (!cp->pe_data || !cp->dest) {
-			IP_VS_ERR_RL("SYNC, connection pe_data invalid\n");
-			return;
-		}
-		pe_name_len = strnlen(cp->pe->name, IP_VS_PENAME_MAXLEN);
-	}
+	struct ip_vs_sync_mesg *m = (struct ip_vs_sync_mesg *)buffer;
+	struct ip_vs_sync_conn *s;
+	struct ip_vs_sync_conn_options *opt;
+	struct ip_vs_conn *cp;
+	struct ip_vs_protocol *pp;
+	struct ip_vs_dest *dest;
+	char *p;
+	int i;
+	int res_dir;
 
-	spin_lock_bh(&ipvs->sync_buff_lock);
-	if (!(ipvs->sync_state & IP_VS_STATE_MASTER)) {
-		spin_unlock_bh(&ipvs->sync_buff_lock);
+	if (buflen < sizeof(struct ip_vs_sync_mesg)) {
+		IP_VS_ERR_RL("sync message header too short\n");
 		return;
 	}
 
-	id = select_master_thread_id(ipvs, cp);
-	ms = &ipvs->ms[id];
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6)
-		len = sizeof(struct ip_vs_sync_v6);
-	else
-#endif
-		len = sizeof(struct ip_vs_sync_v4);
-
-	if (cp->flags & IP_VS_CONN_F_SEQ_MASK)
-		len += sizeof(struct ip_vs_sync_conn_options) + 2;
-
-	if (cp->pe_data_len)
-		len += cp->pe_data_len + 2;	/* + Param hdr field */
-	if (pe_name_len)
-		len += pe_name_len + 2;
-
-	/* check if there is a space for this one  */
-	pad = 0;
-	buff = ms->sync_buff;
-	if (buff) {
-		m = buff->mesg;
-		pad = (4 - (size_t) buff->head) & 3;
-		/* Send buffer if it is for v0 */
-		if (buff->head + len + pad > buff->end || m->reserved) {
-			sb_queue_tail(ipvs, ms);
-			ms->sync_buff = NULL;
-			buff = NULL;
-			pad = 0;
-		}
-	}
-
-	if (!buff) {
-		buff = ip_vs_sync_buff_create(ipvs);
-		if (!buff) {
-			spin_unlock_bh(&ipvs->sync_buff_lock);
-			pr_err("ip_vs_sync_buff_create failed.\n");
-			return;
-		}
-		ms->sync_buff = buff;
-		m = buff->mesg;
-	}
-
-	p = buff->head;
-	buff->head += pad + len;
-	m->size = htons(ntohs(m->size) + pad + len);
-	/* Add ev. padding from prev. sync_conn */
-	while (pad--)
-		*(p++) = 0;
-
-	s = (union ip_vs_sync_conn *)p;
-
-	/* Set message type  & copy members */
-	s->v4.type = (cp->af == AF_INET6 ? STYPE_F_INET6 : 0);
-	s->v4.ver_size = htons(len & SVER_MASK);	/* Version 0 */
-	s->v4.flags = htonl(cp->flags & ~IP_VS_CONN_F_HASHED);
-	s->v4.state = htons(cp->state);
-	s->v4.protocol = cp->protocol;
-	s->v4.cport = cp->cport;
-	s->v4.vport = cp->vport;
-	s->v4.dport = cp->dport;
-	s->v4.fwmark = htonl(cp->fwmark);
-	s->v4.timeout = htonl(cp->timeout / HZ);
-	m->nr_conns++;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (cp->af == AF_INET6) {
-		p += sizeof(struct ip_vs_sync_v6);
-		s->v6.caddr = cp->caddr.in6;
-		s->v6.vaddr = cp->vaddr.in6;
-		s->v6.daddr = cp->daddr.in6;
-	} else
-#endif
-	{
-		p += sizeof(struct ip_vs_sync_v4);	/* options ptr */
-		s->v4.caddr = cp->caddr.ip;
-		s->v4.vaddr = cp->vaddr.ip;
-		s->v4.daddr = cp->daddr.ip;
-	}
-	if (cp->flags & IP_VS_CONN_F_SEQ_MASK) {
-		*(p++) = IPVS_OPT_SEQ_DATA;
-		*(p++) = sizeof(struct ip_vs_sync_conn_options);
-		hton_seq((struct ip_vs_seq *)p, &cp->in_seq);
-		p += sizeof(struct ip_vs_seq);
-		hton_seq((struct ip_vs_seq *)p, &cp->out_seq);
-		p += sizeof(struct ip_vs_seq);
-	}
-	/* Handle pe data */
-	if (cp->pe_data_len && cp->pe_data) {
-		*(p++) = IPVS_OPT_PE_DATA;
-		*(p++) = cp->pe_data_len;
-		memcpy(p, cp->pe_data, cp->pe_data_len);
-		p += cp->pe_data_len;
-		if (pe_name_len) {
-			/* Add PE_NAME */
-			*(p++) = IPVS_OPT_PE_NAME;
-			*(p++) = pe_name_len;
-			memcpy(p, cp->pe->name, pe_name_len);
-			p += pe_name_len;
-		}
-	}
-
-	spin_unlock_bh(&ipvs->sync_buff_lock);
+	/* Convert size back to host byte order */
+	m->size = ntohs(m->size);
 
-control:
-	/* synchronize its controller if it has */
-	cp = cp->control;
-	if (!cp)
+	if (buflen != m->size) {
+		IP_VS_ERR_RL("bogus sync message size\n");
 		return;
-	if (cp->flags & IP_VS_CONN_F_TEMPLATE)
-		pkts = atomic_add_return(1, &cp->in_pkts);
-	else
-		pkts = sysctl_sync_threshold(ipvs);
-	goto sloop;
-}
-
-/*
- *  fill_param used by version 1
- */
-static inline int
-ip_vs_conn_fill_param_sync(struct net *net, int af, union ip_vs_sync_conn *sc,
-			   struct ip_vs_conn_param *p,
-			   __u8 *pe_data, unsigned int pe_data_len,
-			   __u8 *pe_name, unsigned int pe_name_len)
-{
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ip_vs_conn_fill_param(net, af, sc->v6.protocol,
-				      (const union nf_inet_addr *)&sc->v6.caddr,
-				      sc->v6.cport,
-				      (const union nf_inet_addr *)&sc->v6.vaddr,
-				      sc->v6.vport, p);
-	else
-#endif
-		ip_vs_conn_fill_param(net, af, sc->v4.protocol,
-				      (const union nf_inet_addr *)&sc->v4.caddr,
-				      sc->v4.cport,
-				      (const union nf_inet_addr *)&sc->v4.vaddr,
-				      sc->v4.vport, p);
-	/* Handle pe data */
-	if (pe_data_len) {
-		if (pe_name_len) {
-			char buff[IP_VS_PENAME_MAXLEN+1];
-
-			memcpy(buff, pe_name, pe_name_len);
-			buff[pe_name_len]=0;
-			p->pe = __ip_vs_pe_getbyname(buff);
-			if (!p->pe) {
-				IP_VS_DBG(3, "BACKUP, no %s engine found/loaded\n",
-					     buff);
-				return 1;
-			}
-		} else {
-			IP_VS_ERR_RL("BACKUP, Invalid PE parameters\n");
-			return 1;
-		}
-
-		p->pe_data = kmemdup(pe_data, pe_data_len, GFP_ATOMIC);
-		if (!p->pe_data) {
-			if (p->pe->module)
-				module_put(p->pe->module);
-			return -ENOMEM;
-		}
-		p->pe_data_len = pe_data_len;
 	}
-	return 0;
-}
 
-/*
- *  Connection Add / Update.
- *  Common for version 0 and 1 reception of backup sync_conns.
- *  Param: ...
- *         timeout is in sec.
- */
-static void ip_vs_proc_conn(struct net *net, struct ip_vs_conn_param *param,
-			    unsigned int flags, unsigned int state,
-			    unsigned int protocol, unsigned int type,
-			    const union nf_inet_addr *daddr, __be16 dport,
-			    unsigned long timeout, __u32 fwmark,
-			    struct ip_vs_sync_conn_options *opt)
-{
-	struct ip_vs_dest *dest;
-	struct ip_vs_conn *cp;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	if (!(flags & IP_VS_CONN_F_TEMPLATE)) {
-		cp = ip_vs_conn_in_get(param);
-		if (cp && ((cp->dport != dport) ||
-			   !ip_vs_addr_equal(cp->af, &cp->daddr, daddr))) {
-			if (!(flags & IP_VS_CONN_F_INACTIVE)) {
-				ip_vs_conn_expire_now(cp);
-				__ip_vs_conn_put(cp);
-				cp = NULL;
-			} else {
-				/* This is the expiration message for the
-				 * connection that was already replaced, so we
-				 * just ignore it.
-				 */
-				__ip_vs_conn_put(cp);
-				kfree(param->pe_data);
-				return;
-			}
-		}
-	} else {
-		cp = ip_vs_ct_in_get(param);
-	}
-
-	if (cp) {
-		/* Free pe_data */
-		kfree(param->pe_data);
-
-		dest = cp->dest;
-		spin_lock_bh(&cp->lock);
-		if ((cp->flags ^ flags) & IP_VS_CONN_F_INACTIVE &&
-		    !(flags & IP_VS_CONN_F_TEMPLATE) && dest) {
-			if (flags & IP_VS_CONN_F_INACTIVE) {
-				atomic_dec(&dest->activeconns);
-				atomic_inc(&dest->inactconns);
-			} else {
-				atomic_inc(&dest->activeconns);
-				atomic_dec(&dest->inactconns);
-			}
-		}
-		flags &= IP_VS_CONN_F_BACKUP_UPD_MASK;
-		flags |= cp->flags & ~IP_VS_CONN_F_BACKUP_UPD_MASK;
-		cp->flags = flags;
-		spin_unlock_bh(&cp->lock);
-		if (!dest)
-			ip_vs_try_bind_dest(cp);
-	} else {
-		/*
-		 * Find the appropriate destination for the connection.
-		 * If it is not found the connection will remain unbound
-		 * but still handled.
-		 */
-		rcu_read_lock();
-		dest = ip_vs_find_dest(net, type, daddr, dport, param->vaddr,
-				       param->vport, protocol, fwmark, flags);
-
-		cp = ip_vs_conn_new(param, daddr, dport, flags, dest, fwmark);
-		rcu_read_unlock();
-		if (!cp) {
-			if (param->pe_data)
-				kfree(param->pe_data);
-			IP_VS_DBG(2, "BACKUP, add new conn. failed\n");
-			return;
-		}
-	}
-
-	if (opt)
-		memcpy(&cp->in_seq, opt, sizeof(*opt));
-	atomic_set(&cp->in_pkts, sysctl_sync_threshold(ipvs));
-	cp->state = state;
-	cp->old_state = cp->state;
-	/*
-	 * For Ver 0 messages style
-	 *  - Not possible to recover the right timeout for templates
-	 *  - can not find the right fwmark
-	 *    virtual service. If needed, we can do it for
-	 *    non-fwmark persistent services.
-	 * Ver 1 messages style.
-	 *  - No problem.
-	 */
-	if (timeout) {
-		if (timeout > MAX_SCHEDULE_TIMEOUT / HZ)
-			timeout = MAX_SCHEDULE_TIMEOUT / HZ;
-		cp->timeout = timeout*HZ;
-	} else {
-		struct ip_vs_proto_data *pd;
-
-		pd = ip_vs_proto_data_get(net, protocol);
-		if (!(flags & IP_VS_CONN_F_TEMPLATE) && pd && pd->timeout_table)
-			cp->timeout = pd->timeout_table[state];
-		else
-			cp->timeout = (3*60*HZ);
+	/* SyncID sanity check */
+	if (ip_vs_backup_syncid != 0 && m->syncid != ip_vs_backup_syncid) {
+		IP_VS_DBG(7, "Ignoring incoming msg with syncid = %d\n",
+			  m->syncid);
+		return;
 	}
-	ip_vs_conn_put(cp);
-}
 
-/*
- *  Process received multicast message for Version 0
- */
-static void ip_vs_process_message_v0(struct net *net, const char *buffer,
-				     const size_t buflen)
-{
-	struct ip_vs_sync_mesg_v0 *m = (struct ip_vs_sync_mesg_v0 *)buffer;
-	struct ip_vs_sync_conn_v0 *s;
-	struct ip_vs_sync_conn_options *opt;
-	struct ip_vs_protocol *pp;
-	struct ip_vs_conn_param param;
-	char *p;
-	int i;
+	p = (char *)buffer + sizeof(struct ip_vs_sync_mesg);
+	for (i = 0; i < m->nr_conns; i++) {
+		unsigned flags, state;
 
-	p = (char *)buffer + sizeof(struct ip_vs_sync_mesg_v0);
-	for (i=0; i<m->nr_conns; i++) {
-		unsigned int flags, state;
-
-		if (p + SIMPLE_CONN_SIZE > buffer+buflen) {
-			IP_VS_ERR_RL("BACKUP v0, bogus conn\n");
+		if (p + SIMPLE_CONN_SIZE > buffer + buflen) {
+			IP_VS_ERR_RL("bogus conn in sync message\n");
 			return;
 		}
-		s = (struct ip_vs_sync_conn_v0 *) p;
+		s = (struct ip_vs_sync_conn *)p;
 		flags = ntohs(s->flags) | IP_VS_CONN_F_SYNC;
 		flags &= ~IP_VS_CONN_F_HASHED;
 		if (flags & IP_VS_CONN_F_SEQ_MASK) {
 			opt = (struct ip_vs_sync_conn_options *)&s[1];
 			p += FULL_CONN_SIZE;
-			if (p > buffer+buflen) {
-				IP_VS_ERR_RL("BACKUP v0, Dropping buffer bogus conn options\n");
+			if (p > buffer + buflen) {
+				IP_VS_ERR_RL
+				    ("bogus conn options in sync message\n");
 				return;
 			}
 		} else {
@@ -968,308 +345,109 @@ static void ip_vs_process_message_v0(struct net *net, const char *buffer,
 		if (!(flags & IP_VS_CONN_F_TEMPLATE)) {
 			pp = ip_vs_proto_get(s->protocol);
 			if (!pp) {
-				IP_VS_DBG(2, "BACKUP v0, Unsupported protocol %u\n",
-					s->protocol);
+				IP_VS_ERR_RL
+				    ("Unsupported protocol %u in sync msg\n",
+				     s->protocol);
 				continue;
 			}
 			if (state >= pp->num_states) {
-				IP_VS_DBG(2, "BACKUP v0, Invalid %s state %u\n",
-					pp->name, state);
+				IP_VS_DBG(2,
+					  "Invalid %s state %u in sync msg\n",
+					  pp->name, state);
 				continue;
 			}
 		} else {
 			/* protocol in templates is not used for state/timeout */
+			pp = NULL;
 			if (state > 0) {
-				IP_VS_DBG(2, "BACKUP v0, Invalid template state %u\n",
-					state);
+				IP_VS_DBG(2,
+					  "Invalid template state %u in sync msg\n",
+					  state);
 				state = 0;
 			}
 		}
 
-		ip_vs_conn_fill_param(net, AF_INET, s->protocol,
-				      (const union nf_inet_addr *)&s->caddr,
-				      s->cport,
-				      (const union nf_inet_addr *)&s->vaddr,
-				      s->vport, &param);
-
-		/* Send timeout as Zero */
-		ip_vs_proc_conn(net, &param, flags, state, s->protocol, AF_INET,
-				(union nf_inet_addr *)&s->daddr, s->dport,
-				0, 0, opt);
-	}
-}
-
-/*
- * Handle options
- */
-static inline int ip_vs_proc_seqopt(__u8 *p, unsigned int plen,
-				    __u32 *opt_flags,
-				    struct ip_vs_sync_conn_options *opt)
-{
-	struct ip_vs_sync_conn_options *topt;
-
-	topt = (struct ip_vs_sync_conn_options *)p;
-
-	if (plen != sizeof(struct ip_vs_sync_conn_options)) {
-		IP_VS_DBG(2, "BACKUP, bogus conn options length\n");
-		return -EINVAL;
-	}
-	if (*opt_flags & IPVS_OPT_F_SEQ_DATA) {
-		IP_VS_DBG(2, "BACKUP, conn options found twice\n");
-		return -EINVAL;
-	}
-	ntoh_seq(&topt->in_seq, &opt->in_seq);
-	ntoh_seq(&topt->out_seq, &opt->out_seq);
-	*opt_flags |= IPVS_OPT_F_SEQ_DATA;
-	return 0;
-}
-
-static int ip_vs_proc_str(__u8 *p, unsigned int plen, unsigned int *data_len,
-			  __u8 **data, unsigned int maxlen,
-			  __u32 *opt_flags, __u32 flag)
-{
-	if (plen > maxlen) {
-		IP_VS_DBG(2, "BACKUP, bogus par.data len > %d\n", maxlen);
-		return -EINVAL;
-	}
-	if (*opt_flags & flag) {
-		IP_VS_DBG(2, "BACKUP, Par.data found twice 0x%x\n", flag);
-		return -EINVAL;
-	}
-	*data_len = plen;
-	*data = p;
-	*opt_flags |= flag;
-	return 0;
-}
-/*
- *   Process a Version 1 sync. connection
- */
-static inline int ip_vs_proc_sync_conn(struct net *net, __u8 *p, __u8 *msg_end)
-{
-	struct ip_vs_sync_conn_options opt;
-	union  ip_vs_sync_conn *s;
-	struct ip_vs_protocol *pp;
-	struct ip_vs_conn_param param;
-	__u32 flags;
-	unsigned int af, state, pe_data_len=0, pe_name_len=0;
-	__u8 *pe_data=NULL, *pe_name=NULL;
-	__u32 opt_flags=0;
-	int retc=0;
-
-	s = (union ip_vs_sync_conn *) p;
-
-	if (s->v6.type & STYPE_F_INET6) {
-#ifdef CONFIG_IP_VS_IPV6
-		af = AF_INET6;
-		p += sizeof(struct ip_vs_sync_v6);
-#else
-		IP_VS_DBG(3,"BACKUP, IPv6 msg received, and IPVS is not compiled for IPv6\n");
-		retc = 10;
-		goto out;
-#endif
-	} else if (!s->v4.type) {
-		af = AF_INET;
-		p += sizeof(struct ip_vs_sync_v4);
-	} else {
-		return -10;
-	}
-	if (p > msg_end)
-		return -20;
-
-	/* Process optional params check Type & Len. */
-	while (p < msg_end) {
-		int ptype;
-		int plen;
-
-		if (p+2 > msg_end)
-			return -30;
-		ptype = *(p++);
-		plen  = *(p++);
-
-		if (!plen || ((p + plen) > msg_end))
-			return -40;
-		/* Handle seq option  p = param data */
-		switch (ptype & ~IPVS_OPT_F_PARAM) {
-		case IPVS_OPT_SEQ_DATA:
-			if (ip_vs_proc_seqopt(p, plen, &opt_flags, &opt))
-				return -50;
-			break;
-
-		case IPVS_OPT_PE_DATA:
-			if (ip_vs_proc_str(p, plen, &pe_data_len, &pe_data,
-					   IP_VS_PEDATA_MAXLEN, &opt_flags,
-					   IPVS_OPT_F_PE_DATA))
-				return -60;
-			break;
-
-		case IPVS_OPT_PE_NAME:
-			if (ip_vs_proc_str(p, plen,&pe_name_len, &pe_name,
-					   IP_VS_PENAME_MAXLEN, &opt_flags,
-					   IPVS_OPT_F_PE_NAME))
-				return -70;
-			break;
-
-		default:
-			/* Param data mandatory ? */
-			if (!(ptype & IPVS_OPT_F_PARAM)) {
-				IP_VS_DBG(3, "BACKUP, Unknown mandatory param %d found\n",
-					  ptype & ~IPVS_OPT_F_PARAM);
-				retc = 20;
-				goto out;
-			}
-		}
-		p += plen;  /* Next option */
-	}
-
-	/* Get flags and Mask off unsupported */
-	flags  = ntohl(s->v4.flags) & IP_VS_CONN_F_BACKUP_MASK;
-	flags |= IP_VS_CONN_F_SYNC;
-	state = ntohs(s->v4.state);
-
-	if (!(flags & IP_VS_CONN_F_TEMPLATE)) {
-		pp = ip_vs_proto_get(s->v4.protocol);
-		if (!pp) {
-			IP_VS_DBG(3,"BACKUP, Unsupported protocol %u\n",
-				s->v4.protocol);
-			retc = 30;
-			goto out;
-		}
-		if (state >= pp->num_states) {
-			IP_VS_DBG(3, "BACKUP, Invalid %s state %u\n",
-				pp->name, state);
-			retc = 40;
-			goto out;
-		}
-	} else {
-		/* protocol in templates is not used for state/timeout */
-		if (state > 0) {
-			IP_VS_DBG(3, "BACKUP, Invalid template state %u\n",
-				state);
-			state = 0;
-		}
-	}
-	if (ip_vs_conn_fill_param_sync(net, af, s, &param, pe_data,
-				       pe_data_len, pe_name, pe_name_len)) {
-		retc = 50;
-		goto out;
-	}
-	/* If only IPv4, just silent skip IPv6 */
-	if (af == AF_INET)
-		ip_vs_proc_conn(net, &param, flags, state, s->v4.protocol, af,
-				(union nf_inet_addr *)&s->v4.daddr, s->v4.dport,
-				ntohl(s->v4.timeout), ntohl(s->v4.fwmark),
-				(opt_flags & IPVS_OPT_F_SEQ_DATA ? &opt : NULL)
-				);
-#ifdef CONFIG_IP_VS_IPV6
-	else
-		ip_vs_proc_conn(net, &param, flags, state, s->v6.protocol, af,
-				(union nf_inet_addr *)&s->v6.daddr, s->v6.dport,
-				ntohl(s->v6.timeout), ntohl(s->v6.fwmark),
-				(opt_flags & IPVS_OPT_F_SEQ_DATA ? &opt : NULL)
-				);
-#endif
-	return 0;
-	/* Error exit */
-out:
-	IP_VS_DBG(2, "BACKUP, Single msg dropped err:%d\n", retc);
-	return retc;
-
-}
-/*
- *      Process received multicast message and create the corresponding
- *      ip_vs_conn entries.
- *      Handles Version 0 & 1
- */
-static void ip_vs_process_message(struct net *net, __u8 *buffer,
-				  const size_t buflen)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_sync_mesg *m2 = (struct ip_vs_sync_mesg *)buffer;
-	__u8 *p, *msg_end;
-	int i, nr_conns;
-
-	if (buflen < sizeof(struct ip_vs_sync_mesg_v0)) {
-		IP_VS_DBG(2, "BACKUP, message header too short\n");
-		return;
-	}
-
-	if (buflen != ntohs(m2->size)) {
-		IP_VS_DBG(2, "BACKUP, bogus message size\n");
-		return;
-	}
-	/* SyncID sanity check */
-	if (ipvs->backup_syncid != 0 && m2->syncid != ipvs->backup_syncid) {
-		IP_VS_DBG(7, "BACKUP, Ignoring syncid = %d\n", m2->syncid);
-		return;
-	}
-	/* Handle version 1  message */
-	if ((m2->version == SYNC_PROTO_VER) && (m2->reserved == 0)
-	    && (m2->spare == 0)) {
-
-		msg_end = buffer + sizeof(struct ip_vs_sync_mesg);
-		nr_conns = m2->nr_conns;
-
-		for (i=0; i<nr_conns; i++) {
-			union ip_vs_sync_conn *s;
-			unsigned int size;
-			int retc;
-
-			p = msg_end;
-			if (p + sizeof(s->v4) > buffer+buflen) {
-				IP_VS_ERR_RL("BACKUP, Dropping buffer, to small\n");
-				return;
-			}
-			s = (union ip_vs_sync_conn *)p;
-			size = ntohs(s->v4.ver_size) & SVER_MASK;
-			msg_end = p + size;
-			/* Basic sanity checks */
-			if (msg_end  > buffer+buflen) {
-				IP_VS_ERR_RL("BACKUP, Dropping buffer, msg > buffer\n");
-				return;
+		if (!(flags & IP_VS_CONN_F_TEMPLATE))
+			cp = ip_vs_conn_get(AF_INET, s->protocol,
+					    (union nf_inet_addr *)&s->caddr,
+					    s->cport,
+					    (union nf_inet_addr *)&s->vaddr,
+					    s->vport, &res_dir);
+		else
+			cp = ip_vs_ct_in_get(AF_INET, s->protocol,
+					     (union nf_inet_addr *)&s->caddr,
+					     s->cport,
+					     (union nf_inet_addr *)&s->vaddr,
+					     s->vport);
+		if (!cp) {
+			/*
+			 * Find the appropriate destination for the connection.
+			 * If it is not found the connection will remain unbound
+			 * but still handled.
+			 */
+			dest = ip_vs_find_dest(AF_INET,
+					       (union nf_inet_addr *)&s->daddr,
+					       s->dport,
+					       (union nf_inet_addr *)&s->vaddr,
+					       s->vport, s->protocol);
+			/*  Set the approprite ativity flag */
+			if (s->protocol == IPPROTO_TCP) {
+				if (state != IP_VS_TCP_S_ESTABLISHED)
+					flags |= IP_VS_CONN_F_INACTIVE;
+				else
+					flags &= ~IP_VS_CONN_F_INACTIVE;
 			}
-			if (ntohs(s->v4.ver_size) >> SVER_SHIFT) {
-				IP_VS_ERR_RL("BACKUP, Dropping buffer, Unknown version %d\n",
-					      ntohs(s->v4.ver_size) >> SVER_SHIFT);
+			cp = ip_vs_conn_new(AF_INET, s->protocol,
+					    (union nf_inet_addr *)&s->caddr,
+					    s->cport,
+					    (union nf_inet_addr *)&s->vaddr,
+					    s->vport,
+					    (union nf_inet_addr *)&s->daddr,
+					    s->dport, flags, dest, NULL, 0);
+			if (dest)
+				atomic_dec(&dest->refcnt);
+			if (!cp) {
+				pr_err("ip_vs_conn_new failed\n");
 				return;
 			}
-			/* Process a single sync_conn */
-			retc = ip_vs_proc_sync_conn(net, p, msg_end);
-			if (retc < 0) {
-				IP_VS_ERR_RL("BACKUP, Dropping buffer, Err: %d in decoding\n",
-					     retc);
-				return;
+		} else if (!cp->dest) {
+			dest = ip_vs_try_bind_dest(cp);
+			if (dest)
+				atomic_dec(&dest->refcnt);
+		} else if ((cp->dest) && (cp->protocol == IPPROTO_TCP) &&
+			   (cp->state != state)) {
+			/* update active/inactive flag for the connection */
+			dest = cp->dest;
+			if (!(cp->flags & IP_VS_CONN_F_INACTIVE) &&
+			    (state != IP_VS_TCP_S_ESTABLISHED)) {
+				atomic_dec(&dest->activeconns);
+				atomic_inc(&dest->inactconns);
+				cp->flags |= IP_VS_CONN_F_INACTIVE;
+			} else if ((cp->flags & IP_VS_CONN_F_INACTIVE) &&
+				   (state == IP_VS_TCP_S_ESTABLISHED)) {
+				atomic_inc(&dest->activeconns);
+				atomic_dec(&dest->inactconns);
+				cp->flags &= ~IP_VS_CONN_F_INACTIVE;
 			}
-			/* Make sure we have 32 bit alignment */
-			msg_end = p + ((size + 3) & ~3);
 		}
-	} else {
-		/* Old type of message */
-		ip_vs_process_message_v0(net, buffer, buflen);
-		return;
-	}
-}
 
-
-/*
- *      Setup sndbuf (mode=1) or rcvbuf (mode=0)
- */
-static void set_sock_size(struct sock *sk, int mode, int val)
-{
-	/* setsockopt(sock, SOL_SOCKET, SO_SNDBUF, &val, sizeof(val)); */
-	/* setsockopt(sock, SOL_SOCKET, SO_RCVBUF, &val, sizeof(val)); */
-	lock_sock(sk);
-	if (mode) {
-		val = clamp_t(int, val, (SOCK_MIN_SNDBUF + 1) / 2,
-			      sysctl_wmem_max);
-		sk->sk_sndbuf = val * 2;
-		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
-	} else {
-		val = clamp_t(int, val, (SOCK_MIN_RCVBUF + 1) / 2,
-			      sysctl_rmem_max);
-		sk->sk_rcvbuf = val * 2;
-		sk->sk_userlocks |= SOCK_RCVBUF_LOCK;
+		if (opt)
+			memcpy(&cp->in_seq, opt, sizeof(*opt));
+		atomic_set(&cp->in_pkts, sysctl_ip_vs_sync_threshold[0]);
+		cp->state = state;
+		cp->old_state = cp->state;
+		/*
+		 * We can not recover the right timeout for templates
+		 * in all cases, we can not find the right fwmark
+		 * virtual service. If needed, we can do it for
+		 * non-fwmark persistent services.
+		 */
+		if (!(flags & IP_VS_CONN_F_TEMPLATE) && pp->timeout_table)
+			cp->timeout = pp->timeout_table[state];
+		else
+			cp->timeout = (3 * 60 * HZ);
+		ip_vs_conn_put(cp);
 	}
-	release_sock(sk);
 }
 
 /*
@@ -1305,10 +483,8 @@ static int set_mcast_if(struct sock *sk, char *ifname)
 {
 	struct net_device *dev;
 	struct inet_sock *inet = inet_sk(sk);
-	struct net *net = sock_net(sk);
 
-	dev = __dev_get_by_name(net, ifname);
-	if (!dev)
+	if ((dev = __dev_get_by_name(&init_net, ifname)) == NULL)
 		return -ENODEV;
 
 	if (sk->sk_bound_dev_if && dev->ifindex != sk->sk_bound_dev_if)
@@ -1322,53 +498,50 @@ static int set_mcast_if(struct sock *sk, char *ifname)
 	return 0;
 }
 
-
 /*
  *	Set the maximum length of sync message according to the
  *	specified interface's MTU.
  */
-static int set_sync_mesg_maxlen(struct net *net, int sync_state)
+static int set_sync_mesg_maxlen(int sync_state)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
 	struct net_device *dev;
 	int num;
 
 	if (sync_state == IP_VS_STATE_MASTER) {
-		dev = __dev_get_by_name(net, ipvs->master_mcast_ifn);
-		if (!dev)
+		if ((dev =
+		     __dev_get_by_name(&init_net,
+				       ip_vs_master_mcast_ifn)) == NULL)
 			return -ENODEV;
 
 		num = (dev->mtu - sizeof(struct iphdr) -
 		       sizeof(struct udphdr) -
 		       SYNC_MESG_HEADER_LEN - 20) / SIMPLE_CONN_SIZE;
-		ipvs->send_mesg_maxlen = SYNC_MESG_HEADER_LEN +
-			SIMPLE_CONN_SIZE * min(num, MAX_CONNS_PER_SYNCBUFF);
+		sync_send_mesg_maxlen = SYNC_MESG_HEADER_LEN +
+		    SIMPLE_CONN_SIZE * min(num, MAX_CONNS_PER_SYNCBUFF);
 		IP_VS_DBG(7, "setting the maximum length of sync sending "
-			  "message %d.\n", ipvs->send_mesg_maxlen);
+			  "message %d.\n", sync_send_mesg_maxlen);
 	} else if (sync_state == IP_VS_STATE_BACKUP) {
-		dev = __dev_get_by_name(net, ipvs->backup_mcast_ifn);
-		if (!dev)
+		if ((dev =
+		     __dev_get_by_name(&init_net,
+				       ip_vs_backup_mcast_ifn)) == NULL)
 			return -ENODEV;
 
-		ipvs->recv_mesg_maxlen = dev->mtu -
-			sizeof(struct iphdr) - sizeof(struct udphdr);
+		sync_recv_mesg_maxlen = dev->mtu -
+		    sizeof(struct iphdr) - sizeof(struct udphdr);
 		IP_VS_DBG(7, "setting the maximum length of sync receiving "
-			  "message %d.\n", ipvs->recv_mesg_maxlen);
+			  "message %d.\n", sync_recv_mesg_maxlen);
 	}
 
 	return 0;
 }
 
-
 /*
  *      Join a multicast group.
  *      the group is specified by a class D multicast address 224.0.0.0/8
  *      in the in_addr structure passed in as a parameter.
  */
-static int
-join_mcast_group(struct sock *sk, struct in_addr *addr, char *ifname)
+static int join_mcast_group(struct sock *sk, struct in_addr *addr, char *ifname)
 {
-	struct net *net = sock_net(sk);
 	struct ip_mreqn mreq;
 	struct net_device *dev;
 	int ret;
@@ -1376,33 +549,27 @@ join_mcast_group(struct sock *sk, struct in_addr *addr, char *ifname)
 	memset(&mreq, 0, sizeof(mreq));
 	memcpy(&mreq.imr_multiaddr, addr, sizeof(struct in_addr));
 
-	dev = __dev_get_by_name(net, ifname);
-	if (!dev)
+	if ((dev = __dev_get_by_name(&init_net, ifname)) == NULL)
 		return -ENODEV;
 	if (sk->sk_bound_dev_if && dev->ifindex != sk->sk_bound_dev_if)
 		return -EINVAL;
 
 	mreq.imr_ifindex = dev->ifindex;
 
-	rtnl_lock();
 	lock_sock(sk);
 	ret = ip_mc_join_group(sk, &mreq);
 	release_sock(sk);
-	rtnl_unlock();
 
 	return ret;
 }
 
-
 static int bind_mcastif_addr(struct socket *sock, char *ifname)
 {
-	struct net *net = sock_net(sock->sk);
 	struct net_device *dev;
 	__be32 addr;
 	struct sockaddr_in sin;
 
-	dev = __dev_get_by_name(net, ifname);
-	if (!dev)
+	if ((dev = __dev_get_by_name(&init_net, ifname)) == NULL)
 		return -ENODEV;
 
 	addr = inet_select_addr(dev, 0, RT_SCOPE_UNIVERSE);
@@ -1410,45 +577,32 @@ static int bind_mcastif_addr(struct socket *sock, char *ifname)
 		pr_err("You probably need to specify IP address on "
 		       "multicast interface.\n");
 
-	IP_VS_DBG(7, "binding socket with (%s) %pI4\n",
-		  ifname, &addr);
+	IP_VS_DBG(7, "binding socket with (%s) %pI4\n", ifname, &addr);
 
 	/* Now bind the socket with the address of multicast interface */
-	sin.sin_family	     = AF_INET;
-	sin.sin_addr.s_addr  = addr;
-	sin.sin_port         = 0;
+	sin.sin_family = AF_INET;
+	sin.sin_addr.s_addr = addr;
+	sin.sin_port = 0;
 
-	return sock->ops->bind(sock, (struct sockaddr*)&sin, sizeof(sin));
+	return sock->ops->bind(sock, (struct sockaddr *)&sin, sizeof(sin));
 }
 
 /*
  *      Set up sending multicast socket over UDP
  */
-static struct socket *make_send_sock(struct net *net, int id)
+static struct socket *make_send_sock(void)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	/* multicast addr */
-	struct sockaddr_in mcast_addr = {
-		.sin_family		= AF_INET,
-		.sin_port		= cpu_to_be16(IP_VS_SYNC_PORT + id),
-		.sin_addr.s_addr	= cpu_to_be32(IP_VS_SYNC_GROUP),
-	};
 	struct socket *sock;
 	int result;
 
-	/* First create a socket move it to right name space later */
+	/* First create a socket */
 	result = sock_create_kern(PF_INET, SOCK_DGRAM, IPPROTO_UDP, &sock);
 	if (result < 0) {
 		pr_err("Error during creation of socket; terminating\n");
 		return ERR_PTR(result);
 	}
-	/*
-	 * Kernel sockets that are a part of a namespace, should not
-	 * hold a reference to a namespace in order to allow to stop it.
-	 * After sk_change_net should be released using sk_release_kernel.
-	 */
-	sk_change_net(sock->sk, net);
-	result = set_mcast_if(sock->sk, ipvs->master_mcast_ifn);
+
+	result = set_mcast_if(sock->sk, ip_vs_master_mcast_ifn);
 	if (result < 0) {
 		pr_err("Error setting outbound mcast interface\n");
 		goto error;
@@ -1456,18 +610,15 @@ static struct socket *make_send_sock(struct net *net, int id)
 
 	set_mcast_loop(sock->sk, 0);
 	set_mcast_ttl(sock->sk, 1);
-	result = sysctl_sync_sock_size(ipvs);
-	if (result > 0)
-		set_sock_size(sock->sk, 1, result);
 
-	result = bind_mcastif_addr(sock, ipvs->master_mcast_ifn);
+	result = bind_mcastif_addr(sock, ip_vs_master_mcast_ifn);
 	if (result < 0) {
 		pr_err("Error binding address of the mcast interface\n");
 		goto error;
 	}
 
-	result = sock->ops->connect(sock, (struct sockaddr *) &mcast_addr,
-			sizeof(struct sockaddr), 0);
+	result = sock->ops->connect(sock, (struct sockaddr *)&mcast_addr,
+				    sizeof(struct sockaddr), 0);
 	if (result < 0) {
 		pr_err("Error connecting to the multicast addr\n");
 		goto error;
@@ -1475,24 +626,16 @@ static struct socket *make_send_sock(struct net *net, int id)
 
 	return sock;
 
-error:
-	sk_release_kernel(sock->sk);
+      error:
+	sock_release(sock);
 	return ERR_PTR(result);
 }
 
-
 /*
  *      Set up receiving multicast socket over UDP
  */
-static struct socket *make_receive_sock(struct net *net, int id)
+static struct socket *make_receive_sock(void)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	/* multicast addr */
-	struct sockaddr_in mcast_addr = {
-		.sin_family		= AF_INET,
-		.sin_port		= cpu_to_be16(IP_VS_SYNC_PORT + id),
-		.sin_addr.s_addr	= cpu_to_be32(IP_VS_SYNC_GROUP),
-	};
 	struct socket *sock;
 	int result;
 
@@ -1502,20 +645,12 @@ static struct socket *make_receive_sock(struct net *net, int id)
 		pr_err("Error during creation of socket; terminating\n");
 		return ERR_PTR(result);
 	}
-	/*
-	 * Kernel sockets that are a part of a namespace, should not
-	 * hold a reference to a namespace in order to allow to stop it.
-	 * After sk_change_net should be released using sk_release_kernel.
-	 */
-	sk_change_net(sock->sk, net);
+
 	/* it is equivalent to the REUSEADDR option in user-space */
-	sock->sk->sk_reuse = SK_CAN_REUSE;
-	result = sysctl_sync_sock_size(ipvs);
-	if (result > 0)
-		set_sock_size(sock->sk, 0, result);
+	sock->sk->sk_reuse = 1;
 
-	result = sock->ops->bind(sock, (struct sockaddr *) &mcast_addr,
-			sizeof(struct sockaddr));
+	result = sock->ops->bind(sock, (struct sockaddr *)&mcast_addr,
+				 sizeof(struct sockaddr));
 	if (result < 0) {
 		pr_err("Error binding to the multicast addr\n");
 		goto error;
@@ -1523,8 +658,8 @@ static struct socket *make_receive_sock(struct net *net, int id)
 
 	/* join the multicast group */
 	result = join_mcast_group(sock->sk,
-			(struct in_addr *) &mcast_addr.sin_addr,
-			ipvs->backup_mcast_ifn);
+				  (struct in_addr *)&mcast_addr.sin_addr,
+				  ip_vs_backup_mcast_ifn);
 	if (result < 0) {
 		pr_err("Error joining to the multicast group\n");
 		goto error;
@@ -1532,423 +667,264 @@ static struct socket *make_receive_sock(struct net *net, int id)
 
 	return sock;
 
-error:
-	sk_release_kernel(sock->sk);
+      error:
+	sock_release(sock);
 	return ERR_PTR(result);
 }
 
-
 static int
 ip_vs_send_async(struct socket *sock, const char *buffer, const size_t length)
 {
-	struct msghdr	msg = {.msg_flags = MSG_DONTWAIT|MSG_NOSIGNAL};
-	struct kvec	iov;
-	int		len;
+	struct msghdr msg = {.msg_flags = MSG_DONTWAIT | MSG_NOSIGNAL };
+	struct kvec iov;
+	int len;
 
 	EnterFunction(7);
-	iov.iov_base     = (void *)buffer;
-	iov.iov_len      = length;
+	iov.iov_base = (void *)buffer;
+	iov.iov_len = length;
 
-	len = kernel_sendmsg(sock, &msg, &iov, 1, (size_t)(length));
+	len = kernel_sendmsg(sock, &msg, &iov, 1, (size_t) (length));
 
 	LeaveFunction(7);
 	return len;
 }
 
-static int
+static void
 ip_vs_send_sync_msg(struct socket *sock, struct ip_vs_sync_mesg *msg)
 {
 	int msize;
-	int ret;
 
-	msize = ntohs(msg->size);
+	msize = msg->size;
 
-	ret = ip_vs_send_async(sock, (char *)msg, msize);
-	if (ret >= 0 || ret == -EAGAIN)
-		return ret;
-	pr_err("ip_vs_send_async error %d\n", ret);
-	return 0;
+	/* Put size in network byte order */
+	msg->size = htons(msg->size);
+
+	if (ip_vs_send_async(sock, (char *)msg, msize) != msize)
+		pr_err("ip_vs_send_async error\n");
 }
 
-static int
-ip_vs_receive(struct socket *sock, char *buffer, const size_t buflen)
+static int ip_vs_receive(struct socket *sock, char *buffer, const size_t buflen)
 {
-	struct msghdr		msg = {NULL,};
-	struct kvec		iov;
-	int			len;
+	struct msghdr msg = { NULL, };
+	struct kvec iov;
+	int len;
 
 	EnterFunction(7);
 
 	/* Receive a packet */
-	iov.iov_base     = buffer;
-	iov.iov_len      = (size_t)buflen;
+	iov.iov_base = buffer;
+	iov.iov_len = (size_t) buflen;
 
-	len = kernel_recvmsg(sock, &msg, &iov, 1, buflen, MSG_DONTWAIT);
+	len = kernel_recvmsg(sock, &msg, &iov, 1, buflen, 0);
 
 	if (len < 0)
-		return len;
+		return -1;
 
 	LeaveFunction(7);
 	return len;
 }
 
-/* Wakeup the master thread for sending */
-static void master_wakeup_work_handler(struct work_struct *work)
-{
-	struct ipvs_master_sync_state *ms =
-		container_of(work, struct ipvs_master_sync_state,
-			     master_wakeup_work.work);
-	struct netns_ipvs *ipvs = ms->ipvs;
-
-	spin_lock_bh(&ipvs->sync_lock);
-	if (ms->sync_queue_len &&
-	    ms->sync_queue_delay < IPVS_SYNC_WAKEUP_RATE) {
-		ms->sync_queue_delay = IPVS_SYNC_WAKEUP_RATE;
-		wake_up_process(ms->master_thread);
-	}
-	spin_unlock_bh(&ipvs->sync_lock);
-}
-
-/* Get next buffer to send */
-static inline struct ip_vs_sync_buff *
-next_sync_buff(struct netns_ipvs *ipvs, struct ipvs_master_sync_state *ms)
-{
-	struct ip_vs_sync_buff *sb;
-
-	sb = sb_dequeue(ipvs, ms);
-	if (sb)
-		return sb;
-	/* Do not delay entries in buffer for more than 2 seconds */
-	return get_curr_sync_buff(ipvs, ms, IPVS_SYNC_FLUSH_TIME);
-}
-
 static int sync_thread_master(void *data)
 {
 	struct ip_vs_sync_thread_data *tinfo = data;
-	struct netns_ipvs *ipvs = net_ipvs(tinfo->net);
-	struct ipvs_master_sync_state *ms = &ipvs->ms[tinfo->id];
-	struct sock *sk = tinfo->sock->sk;
 	struct ip_vs_sync_buff *sb;
 
 	pr_info("sync thread started: state = MASTER, mcast_ifn = %s, "
-		"syncid = %d, id = %d\n",
-		ipvs->master_mcast_ifn, ipvs->master_syncid, tinfo->id);
-
-	for (;;) {
-		sb = next_sync_buff(ipvs, ms);
-		if (unlikely(kthread_should_stop()))
-			break;
-		if (!sb) {
-			schedule_timeout(IPVS_SYNC_CHECK_PERIOD);
-			continue;
+		"syncid = %d\n", ip_vs_master_mcast_ifn, ip_vs_master_syncid);
+
+	while (!kthread_should_stop()) {
+		while ((sb = sb_dequeue())) {
+			ip_vs_send_sync_msg(tinfo->sock, sb->mesg);
+			ip_vs_sync_buff_release(sb);
 		}
-		while (ip_vs_send_sync_msg(tinfo->sock, sb->mesg) < 0) {
-			int ret = 0;
-
-			__wait_event_interruptible(*sk_sleep(sk),
-						   sock_writeable(sk) ||
-						   kthread_should_stop(),
-						   ret);
-			if (unlikely(kthread_should_stop()))
-				goto done;
+
+		/* check if entries stay in curr_sb for 2 seconds */
+		sb = get_curr_sync_buff(2 * HZ);
+		if (sb) {
+			ip_vs_send_sync_msg(tinfo->sock, sb->mesg);
+			ip_vs_sync_buff_release(sb);
 		}
-		ip_vs_sync_buff_release(sb);
-	}
 
-done:
-	__set_current_state(TASK_RUNNING);
-	if (sb)
-		ip_vs_sync_buff_release(sb);
+		schedule_timeout_interruptible(HZ);
+	}
 
 	/* clean up the sync_buff queue */
-	while ((sb = sb_dequeue(ipvs, ms)))
+	while ((sb = sb_dequeue())) {
 		ip_vs_sync_buff_release(sb);
-	__set_current_state(TASK_RUNNING);
+	}
 
 	/* clean up the current sync_buff */
-	sb = get_curr_sync_buff(ipvs, ms, 0);
-	if (sb)
+	if ((sb = get_curr_sync_buff(0))) {
 		ip_vs_sync_buff_release(sb);
+	}
 
 	/* release the sending multicast socket */
-	sk_release_kernel(tinfo->sock->sk);
+	sock_release(tinfo->sock);
 	kfree(tinfo);
 
 	return 0;
 }
 
-
 static int sync_thread_backup(void *data)
 {
 	struct ip_vs_sync_thread_data *tinfo = data;
-	struct netns_ipvs *ipvs = net_ipvs(tinfo->net);
 	int len;
 
 	pr_info("sync thread started: state = BACKUP, mcast_ifn = %s, "
-		"syncid = %d, id = %d\n",
-		ipvs->backup_mcast_ifn, ipvs->backup_syncid, tinfo->id);
+		"syncid = %d\n", ip_vs_backup_mcast_ifn, ip_vs_backup_syncid);
 
 	while (!kthread_should_stop()) {
 		wait_event_interruptible(*sk_sleep(tinfo->sock->sk),
-			 !skb_queue_empty(&tinfo->sock->sk->sk_receive_queue)
-			 || kthread_should_stop());
+					 !skb_queue_empty(&tinfo->sock->sk->
+							  sk_receive_queue)
+					 || kthread_should_stop());
 
 		/* do we have data now? */
 		while (!skb_queue_empty(&(tinfo->sock->sk->sk_receive_queue))) {
 			len = ip_vs_receive(tinfo->sock, tinfo->buf,
-					ipvs->recv_mesg_maxlen);
+					    sync_recv_mesg_maxlen);
 			if (len <= 0) {
-				if (len != -EAGAIN)
-					pr_err("receiving message error\n");
+				pr_err("receiving message error\n");
 				break;
 			}
 
-			ip_vs_process_message(tinfo->net, tinfo->buf, len);
+			/* disable bottom half, because it accesses the data
+			   shared by softirq while getting/creating conns */
+			local_bh_disable();
+			ip_vs_process_message(tinfo->buf, len);
+			local_bh_enable();
 		}
 	}
 
 	/* release the sending multicast socket */
-	sk_release_kernel(tinfo->sock->sk);
+	sock_release(tinfo->sock);
 	kfree(tinfo->buf);
 	kfree(tinfo);
 
 	return 0;
 }
 
-
-int start_sync_thread(struct net *net, int state, char *mcast_ifn, __u8 syncid)
+int start_sync_thread(int state, char *mcast_ifn, __u8 syncid)
 {
 	struct ip_vs_sync_thread_data *tinfo;
-	struct task_struct **array = NULL, *task;
+	struct task_struct **realtask, *task;
 	struct socket *sock;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	char *name;
-	int (*threadfn)(void *data);
-	int id, count;
+	char *name, *buf = NULL;
+	int (*threadfn) (void *data);
 	int result = -ENOMEM;
 
 	IP_VS_DBG(7, "%s(): pid %d\n", __func__, task_pid_nr(current));
 	IP_VS_DBG(7, "Each ip_vs_sync_conn entry needs %Zd bytes\n",
-		  sizeof(struct ip_vs_sync_conn_v0));
-
-	if (!ipvs->sync_state) {
-		count = clamp(sysctl_sync_ports(ipvs), 1, IPVS_SYNC_PORTS_MAX);
-		ipvs->threads_mask = count - 1;
-	} else
-		count = ipvs->threads_mask + 1;
+		  sizeof(struct ip_vs_sync_conn));
 
 	if (state == IP_VS_STATE_MASTER) {
-		if (ipvs->ms)
+		if (sync_master_thread)
 			return -EEXIST;
 
-		strlcpy(ipvs->master_mcast_ifn, mcast_ifn,
-			sizeof(ipvs->master_mcast_ifn));
-		ipvs->master_syncid = syncid;
-		name = "ipvs-m:%d:%d";
+		strlcpy(ip_vs_master_mcast_ifn, mcast_ifn,
+			sizeof(ip_vs_master_mcast_ifn));
+		ip_vs_master_syncid = syncid;
+		realtask = &sync_master_thread;
+		name = "ipvs_syncmaster";
 		threadfn = sync_thread_master;
+		sock = make_send_sock();
 	} else if (state == IP_VS_STATE_BACKUP) {
-		if (ipvs->backup_threads)
+		if (sync_backup_thread)
 			return -EEXIST;
 
-		strlcpy(ipvs->backup_mcast_ifn, mcast_ifn,
-			sizeof(ipvs->backup_mcast_ifn));
-		ipvs->backup_syncid = syncid;
-		name = "ipvs-b:%d:%d";
+		strlcpy(ip_vs_backup_mcast_ifn, mcast_ifn,
+			sizeof(ip_vs_backup_mcast_ifn));
+		ip_vs_backup_syncid = syncid;
+		realtask = &sync_backup_thread;
+		name = "ipvs_syncbackup";
 		threadfn = sync_thread_backup;
+		sock = make_receive_sock();
 	} else {
 		return -EINVAL;
 	}
 
-	if (state == IP_VS_STATE_MASTER) {
-		struct ipvs_master_sync_state *ms;
-
-		ipvs->ms = kzalloc(count * sizeof(ipvs->ms[0]), GFP_KERNEL);
-		if (!ipvs->ms)
-			goto out;
-		ms = ipvs->ms;
-		for (id = 0; id < count; id++, ms++) {
-			INIT_LIST_HEAD(&ms->sync_queue);
-			ms->sync_queue_len = 0;
-			ms->sync_queue_delay = 0;
-			INIT_DELAYED_WORK(&ms->master_wakeup_work,
-					  master_wakeup_work_handler);
-			ms->ipvs = ipvs;
-		}
-	} else {
-		array = kzalloc(count * sizeof(struct task_struct *),
-				GFP_KERNEL);
-		if (!array)
-			goto out;
+	if (IS_ERR(sock)) {
+		result = PTR_ERR(sock);
+		goto out;
 	}
-	set_sync_mesg_maxlen(net, state);
 
-	tinfo = NULL;
-	for (id = 0; id < count; id++) {
-		if (state == IP_VS_STATE_MASTER)
-			sock = make_send_sock(net, id);
-		else
-			sock = make_receive_sock(net, id);
-		if (IS_ERR(sock)) {
-			result = PTR_ERR(sock);
-			goto outtinfo;
-		}
-		tinfo = kmalloc(sizeof(*tinfo), GFP_KERNEL);
-		if (!tinfo)
+	set_sync_mesg_maxlen(state);
+	if (state == IP_VS_STATE_BACKUP) {
+		buf = kmalloc(sync_recv_mesg_maxlen, GFP_KERNEL);
+		if (!buf)
 			goto outsocket;
-		tinfo->net = net;
-		tinfo->sock = sock;
-		if (state == IP_VS_STATE_BACKUP) {
-			tinfo->buf = kmalloc(ipvs->recv_mesg_maxlen,
-					     GFP_KERNEL);
-			if (!tinfo->buf)
-				goto outtinfo;
-		} else {
-			tinfo->buf = NULL;
-		}
-		tinfo->id = id;
+	}
 
-		task = kthread_run(threadfn, tinfo, name, ipvs->gen, id);
-		if (IS_ERR(task)) {
-			result = PTR_ERR(task);
-			goto outtinfo;
-		}
-		tinfo = NULL;
-		if (state == IP_VS_STATE_MASTER)
-			ipvs->ms[id].master_thread = task;
-		else
-			array[id] = task;
+	tinfo = kmalloc(sizeof(*tinfo), GFP_KERNEL);
+	if (!tinfo)
+		goto outbuf;
+
+	tinfo->sock = sock;
+	tinfo->buf = buf;
+
+	task = kthread_run(threadfn, tinfo, name);
+	if (IS_ERR(task)) {
+		result = PTR_ERR(task);
+		goto outtinfo;
 	}
 
 	/* mark as active */
-
-	if (state == IP_VS_STATE_BACKUP)
-		ipvs->backup_threads = array;
-	spin_lock_bh(&ipvs->sync_buff_lock);
-	ipvs->sync_state |= state;
-	spin_unlock_bh(&ipvs->sync_buff_lock);
+	*realtask = task;
+	ip_vs_sync_state |= state;
 
 	/* increase the module use count */
 	ip_vs_use_count_inc();
 
 	return 0;
 
-outsocket:
-	sk_release_kernel(sock->sk);
-
-outtinfo:
-	if (tinfo) {
-		sk_release_kernel(tinfo->sock->sk);
-		kfree(tinfo->buf);
-		kfree(tinfo);
-	}
-	count = id;
-	while (count-- > 0) {
-		if (state == IP_VS_STATE_MASTER)
-			kthread_stop(ipvs->ms[count].master_thread);
-		else
-			kthread_stop(array[count]);
-	}
-	kfree(array);
-
-out:
-	if (!(ipvs->sync_state & IP_VS_STATE_MASTER)) {
-		kfree(ipvs->ms);
-		ipvs->ms = NULL;
-	}
+      outtinfo:
+	kfree(tinfo);
+      outbuf:
+	kfree(buf);
+      outsocket:
+	sock_release(sock);
+      out:
 	return result;
 }
 
-
-int stop_sync_thread(struct net *net, int state)
+int stop_sync_thread(int state)
 {
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct task_struct **array;
-	int id;
-	int retc = -EINVAL;
-
 	IP_VS_DBG(7, "%s(): pid %d\n", __func__, task_pid_nr(current));
 
 	if (state == IP_VS_STATE_MASTER) {
-		if (!ipvs->ms)
+		if (!sync_master_thread)
 			return -ESRCH;
 
+		pr_info("stopping master sync thread %d ...\n",
+			task_pid_nr(sync_master_thread));
+
 		/*
 		 * The lock synchronizes with sb_queue_tail(), so that we don't
 		 * add sync buffers to the queue, when we are already in
 		 * progress of stopping the master sync daemon.
 		 */
 
-		spin_lock_bh(&ipvs->sync_buff_lock);
-		spin_lock(&ipvs->sync_lock);
-		ipvs->sync_state &= ~IP_VS_STATE_MASTER;
-		spin_unlock(&ipvs->sync_lock);
-		spin_unlock_bh(&ipvs->sync_buff_lock);
-
-		retc = 0;
-		for (id = ipvs->threads_mask; id >= 0; id--) {
-			struct ipvs_master_sync_state *ms = &ipvs->ms[id];
-			int ret;
-
-			pr_info("stopping master sync thread %d ...\n",
-				task_pid_nr(ms->master_thread));
-			cancel_delayed_work_sync(&ms->master_wakeup_work);
-			ret = kthread_stop(ms->master_thread);
-			if (retc >= 0)
-				retc = ret;
-		}
-		kfree(ipvs->ms);
-		ipvs->ms = NULL;
+		spin_lock_bh(&ip_vs_sync_lock);
+		ip_vs_sync_state &= ~IP_VS_STATE_MASTER;
+		spin_unlock_bh(&ip_vs_sync_lock);
+		kthread_stop(sync_master_thread);
+		sync_master_thread = NULL;
 	} else if (state == IP_VS_STATE_BACKUP) {
-		if (!ipvs->backup_threads)
+		if (!sync_backup_thread)
 			return -ESRCH;
 
-		ipvs->sync_state &= ~IP_VS_STATE_BACKUP;
-		array = ipvs->backup_threads;
-		retc = 0;
-		for (id = ipvs->threads_mask; id >= 0; id--) {
-			int ret;
-
-			pr_info("stopping backup sync thread %d ...\n",
-				task_pid_nr(array[id]));
-			ret = kthread_stop(array[id]);
-			if (retc >= 0)
-				retc = ret;
-		}
-		kfree(array);
-		ipvs->backup_threads = NULL;
+		pr_info("stopping backup sync thread %d ...\n",
+			task_pid_nr(sync_backup_thread));
+
+		ip_vs_sync_state &= ~IP_VS_STATE_BACKUP;
+		kthread_stop(sync_backup_thread);
+		sync_backup_thread = NULL;
+	} else {
+		return -EINVAL;
 	}
 
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
 
-	return retc;
-}
-
-/*
- * Initialize data struct for each netns
- */
-int __net_init ip_vs_sync_net_init(struct net *net)
-{
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	__mutex_init(&ipvs->sync_mutex, "ipvs->sync_mutex", &__ipvs_sync_key);
-	spin_lock_init(&ipvs->sync_lock);
-	spin_lock_init(&ipvs->sync_buff_lock);
 	return 0;
 }
-
-void ip_vs_sync_net_cleanup(struct net *net)
-{
-	int retc;
-	struct netns_ipvs *ipvs = net_ipvs(net);
-
-	mutex_lock(&ipvs->sync_mutex);
-	retc = stop_sync_thread(net, IP_VS_STATE_MASTER);
-	if (retc && retc != -ESRCH)
-		pr_err("Failed to stop Master Daemon\n");
-
-	retc = stop_sync_thread(net, IP_VS_STATE_BACKUP);
-	if (retc && retc != -ESRCH)
-		pr_err("Failed to stop Backup Daemon\n");
-	mutex_unlock(&ipvs->sync_mutex);
-}
diff --git a/net/netfilter/ipvs/ip_vs_synproxy.c b/net/netfilter/ipvs/ip_vs_synproxy.c
new file mode 100644
index 0000000..df7d0fb
--- /dev/null
+++ b/net/netfilter/ipvs/ip_vs_synproxy.c
@@ -0,0 +1,1134 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/tcp.h>
+#include <linux/if_arp.h>
+
+#include <net/ip.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/icmp.h>		/* for icmp_send */
+#include <net/route.h>
+
+#include <linux/netfilter.h>
+#include <linux/netfilter_ipv4.h>
+
+#ifdef CONFIG_IP_VS_IPV6
+#include <net/ipv6.h>
+#include <linux/netfilter_ipv6.h>
+#endif
+
+#include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
+
+static inline void
+syn_proxy_seq_csum_update(struct tcphdr *tcph, __u32 old_seq, __u32 new_seq)
+{
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, new_seq,
+						  ~csum_unfold(tcph->check)));
+}
+
+/*
+ * Replace tcp options in tcp header, called by syn_proxy_reuse_skb()
+ *
+ */
+static void
+syn_proxy_parse_set_opts(struct sk_buff *skb, struct tcphdr *th,
+			 struct ip_vs_synproxy_opt *opt)
+{
+	/* mss in received packet */
+	__u16 in_mss;
+	__u32 *tmp;
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	/*tcp_sk(sk)->user_mss. set from proc */
+	__u16 user_mss = sysctl_ip_vs_synproxy_init_mss;
+
+	memset(opt, '\0', sizeof(struct ip_vs_synproxy_opt));
+	opt->mss_clamp = 536;
+	ptr = (unsigned char *)(th + 1);
+
+	while (length > 0) {
+		unsigned char *tmp_opcode = ptr;
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			switch (opcode) {
+			case TCPOPT_MSS:
+				if (opsize == TCPOLEN_MSS) {
+					in_mss = ntohs(*(__u16 *) ptr);
+					if (in_mss) {
+						if (user_mss < in_mss) {
+							in_mss = user_mss;
+						}
+						opt->mss_clamp = in_mss;
+					}
+					*(__u16 *) ptr = htons(opt->mss_clamp);
+				}
+				break;
+			case TCPOPT_WINDOW:
+				if (opsize == TCPOLEN_WINDOW) {
+					if (sysctl_ip_vs_synproxy_wscale) {
+						opt->wscale_ok = 1;
+						opt->snd_wscale = *(__u8 *) ptr;
+						if (opt->snd_wscale >
+						    IP_VS_SYNPROXY_WSCALE_MAX) {
+							IP_VS_DBG(6,
+								  "tcp_parse_options: Illegal window "
+								  "scaling value %d > %d received.",
+								  opt->
+								  snd_wscale,
+								  IP_VS_SYNPROXY_WSCALE_MAX);
+							opt->snd_wscale =
+							    IP_VS_SYNPROXY_WSCALE_MAX;
+						}
+						*(__u8 *) ptr = (__u8)
+						    sysctl_ip_vs_synproxy_wscale;
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_WINDOW);
+					}
+				}
+				break;
+			case TCPOPT_TIMESTAMP:
+				if (opsize == TCPOLEN_TIMESTAMP) {
+					if (sysctl_ip_vs_synproxy_timestamp) {
+						opt->tstamp_ok = 1;
+						tmp = (__u32 *) ptr;
+						*(tmp + 1) = *tmp;
+						*tmp = htonl(tcp_time_stamp);
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_TIMESTAMP);
+					}
+				}
+				break;
+			case TCPOPT_SACK_PERM:
+				if (opsize == TCPOLEN_SACK_PERM) {
+					if (sysctl_ip_vs_synproxy_sack) {
+						opt->sack_ok = 1;
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_SACK_PERM);
+					}
+				}
+				break;
+			}
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * Reuse skb for syn proxy, called by syn_proxy_syn_rcv().
+ * do following things:
+ * 1) set tcp options;
+ * 2) compute seq with cookie func.
+ * 3) set tcp seq and ack_seq;
+ * 4) exchange ip addr and tcp port;
+ * 5) compute iphdr and tcp check.
+ *
+ */
+static void
+syn_proxy_reuse_skb(int af, struct sk_buff *skb, struct ip_vs_synproxy_opt *opt)
+{
+	__u32 isn;
+	unsigned short tmpport;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+
+	th = (void *)skb_network_header(skb) + tcphoff;
+
+	/* deal with tcp options */
+	syn_proxy_parse_set_opts(skb, th, opt);
+
+	/* get cookie */
+	skb_set_transport_header(skb, tcphoff);
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		isn = ip_vs_synproxy_cookie_v6_init_sequence(skb, opt);
+	else
+#endif
+		isn = ip_vs_synproxy_cookie_v4_init_sequence(skb, opt);
+
+	/* Set syn-ack flag
+	 * the tcp opt in syn/ack packet : 00010010 = 0x12
+	 */
+	((u_int8_t *) th)[13] = 0x12;
+
+	/* Exchange ports */
+	tmpport = th->dest;
+	th->dest = th->source;
+	th->source = tmpport;
+
+	/* Set seq(cookie) and ack_seq */
+	th->ack_seq = htonl(ntohl(th->seq) + 1);
+	th->seq = htonl(isn);
+
+	/* Exchange addresses and compute checksums */
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6) {
+		struct ipv6hdr *iph = ipv6_hdr(skb);
+		struct in6_addr tmpAddr;
+
+		memcpy(&tmpAddr, &iph->saddr, sizeof(struct in6_addr));
+		memcpy(&iph->saddr, &iph->daddr, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &tmpAddr, sizeof(struct in6_addr));
+
+		iph->hop_limit = sysctl_ip_vs_synproxy_synack_ttl;
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *iph = ip_hdr(skb);
+		__be32 tmpAddr;
+
+		tmpAddr = iph->saddr;
+		iph->saddr = iph->daddr;
+		iph->daddr = tmpAddr;
+
+		iph->ttl = sysctl_ip_vs_synproxy_synack_ttl;
+		iph->tos = 0;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+	}
+}
+
+/*
+ *  syn-proxy step 1 logic:
+ *  Check if synproxy is enabled for this skb, and
+ *  send Syn/Ack back.
+ *
+ *  Synproxy is enabled when:
+ *  1) skb is a Syn packet.
+ *  2) And the service is synproxy-enable.
+ *  3) And ip_vs_todrop return false.
+ *
+ *  @return 0 means the caller should return at once and use
+ *   verdict as return value, return 1 for nothing.
+ */
+int
+ip_vs_synproxy_syn_rcv(int af, struct sk_buff *skb,
+		       struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct ip_vs_service *svc = NULL;
+	struct tcphdr _tcph, *th;
+	struct ip_vs_synproxy_opt tcp_opt;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	if (unlikely(th == NULL)) {
+		goto syn_rcv_out;
+	}
+
+	if (th->syn && !th->ack && !th->rst && !th->fin &&
+	    (svc =
+	     ip_vs_service_get(af, skb->mark, iph->protocol, &iph->daddr,
+			       th->dest))
+	    && (svc->flags & IP_VS_CONN_F_SYNPROXY)) {
+		// release service here, because don't use it any all.
+		ip_vs_service_put(svc);
+
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			goto syn_rcv_out;
+		}
+	} else {
+		/*
+		 * release service.
+		 */
+		if (svc != NULL) {
+			ip_vs_service_put(svc);
+		}
+		return 1;
+	}
+
+	/* update statistics */
+	IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_SYN_CNT);
+
+	/* Try to reuse skb if possible */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if (unlikely(new_skb == NULL)) {
+			goto syn_rcv_out;
+		}
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+	}
+
+	/* reuse skb here: deal with tcp options, exchage ip, port. */
+	syn_proxy_reuse_skb(af, skb, &tcp_opt);
+
+	if (unlikely(skb->dev == NULL)) {
+		IP_VS_ERR_RL("%s: skb->dev is null !!!\n", __func__);
+		goto syn_rcv_out;
+	}
+
+	/* Send the packet out */
+	if (likely(skb->dev->type == ARPHRD_ETHER)) {
+		unsigned char t_hwaddr[ETH_ALEN];
+
+		/* Move the data pointer to point to the link layer header */
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
+		skb->data = (unsigned char *)skb_mac_header(skb);
+		skb->len += ETH_HLEN;	//sizeof(skb->mac.ethernet);
+
+		memcpy(t_hwaddr, (eth->h_dest), ETH_ALEN);
+		memcpy((eth->h_dest), (eth->h_source), ETH_ALEN);
+		memcpy((eth->h_source), t_hwaddr, ETH_ALEN);
+		skb->pkt_type = PACKET_OUTGOING;
+	} else if (skb->dev->type == ARPHRD_LOOPBACK) {
+		/* set link layer */
+		if (likely(skb_mac_header_was_set(skb))) {
+			skb->data = skb_mac_header(skb);
+			skb->len += sizeof(struct ethhdr);
+		} else {
+			skb_push(skb, sizeof(struct ethhdr));
+			skb_reset_mac_header(skb);
+		}
+	}
+
+	dev_queue_xmit(skb);
+	*verdict = NF_STOLEN;
+	return 0;
+syn_rcv_out:
+	/* Drop the packet when all things are right also,
+	 * then we needn't to kfree_skb() */
+	*verdict = NF_DROP;
+	return 0;
+}
+
+/*
+ * Check if skb has user data.
+ * Attention: decrease iph len also.
+ */
+static inline int
+syn_proxy_ack_has_data(struct sk_buff *skb, struct ip_vs_iphdr *iph,
+		       struct tcphdr *th)
+{
+	IP_VS_DBG(6, "tot_len = %u, iph_len = %u, tcph_len = %u\n",
+		  skb->len, iph->len, th->doff * 4);
+	return (skb->len - iph->len - th->doff * 4) != 0;
+}
+
+static inline void
+syn_proxy_syn_build_options(__be32 * ptr, struct ip_vs_synproxy_opt *opt)
+{
+	*ptr++ =
+	    htonl((TCPOPT_MSS << 24) | (TCPOLEN_MSS << 16) | opt->mss_clamp);
+	if (opt->tstamp_ok) {
+		if (opt->sack_ok)
+			*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |
+				       (TCPOLEN_SACK_PERM << 16) |
+				       (TCPOPT_TIMESTAMP << 8) |
+				       TCPOLEN_TIMESTAMP);
+		else
+			*ptr++ = htonl((TCPOPT_NOP << 24) |
+				       (TCPOPT_NOP << 16) |
+				       (TCPOPT_TIMESTAMP << 8) |
+				       TCPOLEN_TIMESTAMP);
+		*ptr++ = htonl(tcp_time_stamp);	/* TSVAL */
+		*ptr++ = 0;	/* TSECR */
+	} else if (opt->sack_ok)
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_NOP << 16) |
+			       (TCPOPT_SACK_PERM << 8) | TCPOLEN_SACK_PERM);
+	if (opt->wscale_ok)
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_WINDOW << 16) |
+			       (TCPOLEN_WINDOW << 8) | (opt->snd_wscale));
+}
+
+/*
+ * Create syn packet and send it to rs.
+ * ATTENTION: we also store syn skb in cp if syn retransimition
+ * is tured on.
+ */
+static int
+syn_proxy_send_rs_syn(int af, const struct tcphdr *th,
+		      struct ip_vs_conn *cp, struct sk_buff *skb,
+		      struct ip_vs_protocol *pp, struct ip_vs_synproxy_opt *opt)
+{
+	struct sk_buff *syn_skb;
+	int tcp_hdr_size;
+	// TODO check TCPCB_FLAG_SYN
+	__u8 tcp_flags = TCPCB_SACKED_RETRANS;
+	unsigned int tcphoff;
+	struct tcphdr *new_th;
+
+	if (!cp->packet_xmit) {
+		IP_VS_ERR_RL("warning: packet_xmit is null");
+		return 0;
+	}
+
+	syn_skb = alloc_skb(MAX_TCP_HEADER + 15, GFP_ATOMIC);
+	if (unlikely(syn_skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send rs syn packet\n");
+		return 0;
+	}
+
+	/* Reserve space for headers */
+	skb_reserve(syn_skb, MAX_TCP_HEADER);
+	tcp_hdr_size = (sizeof(struct tcphdr) + TCPOLEN_MSS +
+			(opt->tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0) +
+			(opt->wscale_ok ? TCPOLEN_WSCALE_ALIGNED : 0) +
+			/* SACK_PERM is in the place of NOP NOP of TS */
+			((opt->sack_ok
+			  && !opt->tstamp_ok) ? TCPOLEN_SACKPERM_ALIGNED : 0));
+
+	new_th = (struct tcphdr *)skb_push(syn_skb, tcp_hdr_size);
+	/* Compose tcp header */
+	skb_reset_transport_header(syn_skb);
+	syn_skb->csum = 0;
+
+	/* Set tcp hdr */
+	new_th->source = th->source;
+	new_th->dest = th->dest;
+	new_th->seq = htonl(ntohl(th->seq) - 1);
+	new_th->ack_seq = 0;
+	*(((__u16 *) new_th) + 6) =
+	    htons(((tcp_hdr_size >> 2) << 12) | tcp_flags);
+	/* FIX_ME: what window should we use */
+	new_th->window = htons(5000);
+	new_th->check = 0;
+	new_th->urg_ptr = 0;
+	new_th->urg = 0;
+	new_th->ece = 0;
+	new_th->cwr = 0;
+
+	syn_proxy_syn_build_options((__be32 *) (new_th + 1), opt);
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on packet_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6) {
+		struct ipv6hdr *ack_iph = ipv6_hdr(skb);
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(syn_skb, sizeof(struct ipv6hdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(syn_skb);
+		memcpy(&iph->saddr, &ack_iph->saddr, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &ack_iph->daddr, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->payload_len = htons(tcp_hdr_size);
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+
+		new_th->check = 0;
+		syn_skb->csum =
+		    skb_checksum(syn_skb, tcphoff, syn_skb->len - tcphoff, 0);
+		new_th->check =
+		    csum_ipv6_magic(&iph->saddr, &iph->daddr,
+				    syn_skb->len - tcphoff, IPPROTO_TCP,
+				    syn_skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *ack_iph = ip_hdr(skb);
+		u32 rtos = RT_TOS(ack_iph->tos);
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(syn_skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(syn_skb);
+		*((__u16 *) iph) = htons((4 << 12) | (5 << 8) | (rtos & 0xff));
+		iph->tot_len = htons(syn_skb->len);
+		iph->frag_off = htons(IP_DF);
+		/* FIX_ME: what ttl shoule we use */
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = ack_iph->saddr;
+		iph->daddr = ack_iph->daddr;
+
+		ip_send_check(iph);
+
+		new_th->check = 0;
+		syn_skb->csum =
+		    skb_checksum(syn_skb, tcphoff, syn_skb->len - tcphoff, 0);
+		new_th->check =
+		    csum_tcpudp_magic(iph->saddr, iph->daddr,
+				      syn_skb->len - tcphoff, IPPROTO_TCP,
+				      syn_skb->csum);
+	}
+
+	/* Save syn_skb if syn retransmission is on  */
+	if (sysctl_ip_vs_synproxy_syn_retry > 0) {
+		cp->syn_skb = skb_copy(syn_skb, GFP_ATOMIC);
+		atomic_set(&cp->syn_retry_max, sysctl_ip_vs_synproxy_syn_retry);
+	}
+
+	/* Save info for fast_response_xmit */
+	if(sysctl_ip_vs_fast_xmit && skb->dev &&
+				likely(skb->dev->type == ARPHRD_ETHER) &&
+				skb_mac_header_was_set(skb)) {
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
+
+		if(likely(cp->indev == NULL)) {
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
+
+		if (unlikely(cp->indev != skb->dev)) {
+			dev_put(cp->indev);
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
+
+		memcpy(cp->src_hwaddr, eth->h_source, ETH_ALEN);
+		memcpy(cp->dst_hwaddr, eth->h_dest, ETH_ALEN);
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_SYNPROXY_SAVE);
+		IP_VS_DBG_RL("syn_proxy_send_rs_syn netdevice:%s\n",
+						netdev_name(skb->dev));
+	}
+
+	/* count in the syn packet */
+	ip_vs_in_stats(cp, skb);
+
+	/* If xmit failed, syn_skb will be freed correctly. */
+	cp->packet_xmit(syn_skb, cp, pp);
+
+	return 1;
+}
+
+/*
+ * Syn-proxy step 2 logic
+ * Receive client's 3-handshakes  Ack packet, do cookie check
+ * and then send syn to rs after creating a session.
+ *
+ */
+int
+ip_vs_synproxy_ack_rcv(int af, struct sk_buff *skb, struct tcphdr *th,
+		       struct ip_vs_protocol *pp, struct ip_vs_conn **cpp,
+		       struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct ip_vs_synproxy_opt opt;
+	struct ip_vs_service *svc;
+	int res_cookie_check;
+
+	/*
+	 * Don't check svc syn-proxy flag, as it may
+	 * be changed after syn-proxy step 1.
+	 */
+	if (!th->syn && th->ack && !th->rst && !th->fin &&
+	    (svc =
+	     ip_vs_service_get(af, skb->mark, iph->protocol, &iph->daddr,
+			       th->dest))) {
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		if (sysctl_ip_vs_synproxy_defer &&
+		    !syn_proxy_ack_has_data(skb, iph, th)) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_NULL_ACK);
+			/*
+			 * When expecting ack packet with payload,
+			 * we get a pure ack, so have to drop it.
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Import: set tcp hdr before cookie check, as it
+		 * will be used in cookie_check funcs.
+		 */
+		skb_set_transport_header(skb, iph->len);
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			res_cookie_check = ip_vs_synproxy_v6_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		} else
+#endif
+		{
+			res_cookie_check = ip_vs_synproxy_v4_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		}
+
+		if (!res_cookie_check) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_BAD_ACK);
+			/*
+			 * Cookie check fail, drop it.
+			 */
+			IP_VS_DBG(6, "syn_cookie check failed seq=%u\n",
+				  ntohl(th->ack_seq) - 1);
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/* update statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_OK_ACK);
+
+		/*
+		 * Let the virtual server select a real server for the
+		 * incoming connection, and create a connection entry.
+		 */
+		*cpp = ip_vs_schedule(svc, skb, 1);
+		if (!*cpp) {
+			IP_VS_DBG(6, "ip_vs_schedule failed\n");
+			*verdict = ip_vs_leave(svc, skb, pp);
+			return 0;
+		}
+
+		/*
+		 * Release service, we don't need it any more.
+		 */
+		ip_vs_service_put(svc);
+
+		/*
+		 * Do anything but print a error msg when fail.
+		 * Because session will be correctly freed in ip_vs_conn_expire.
+		 */
+		if (!syn_proxy_send_rs_syn(af, th, *cpp, skb, pp, &opt)) {
+			IP_VS_ERR_RL("syn_proxy_send_rs_syn failed!\n");
+		}
+
+		/* count in the ack packet (STOLEN by synproxy) */
+		ip_vs_in_stats(*cpp, skb);
+
+		/*
+		 * Active sesion timer, and dec refcnt.
+		 * Also stole the skb, and let caller return immediately.
+		 */
+		ip_vs_conn_put(*cpp);
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * Update out-in sack seqs, and also correct th->check
+ */
+static inline void
+syn_proxy_filter_opt_outin(struct tcphdr *th, struct ip_vs_seq *sp_seq)
+{
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	__be32 *tmp;
+	__u32 old_ack_seq;
+
+	if (!length)
+		return;
+
+	ptr = (unsigned char *)(th + 1);
+
+	/* Fast path for timestamp-only option */
+	if (length == TCPOLEN_TSTAMP_ALIGNED
+	    && *(__be32 *) ptr == __constant_htonl((TCPOPT_NOP << 24)
+						   | (TCPOPT_NOP << 16)
+						   | (TCPOPT_TIMESTAMP << 8) |
+						   TCPOLEN_TIMESTAMP))
+		return;
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize, i;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				break;	/* don't parse partial options */
+
+			if (opcode == TCPOPT_SACK
+			    && opsize >= (TCPOLEN_SACK_BASE
+					  + TCPOLEN_SACK_PERBLOCK)
+			    && !((opsize - TCPOLEN_SACK_BASE) %
+				 TCPOLEN_SACK_PERBLOCK)) {
+				for (i = 0; i < (opsize - TCPOLEN_SACK_BASE);
+				     i += TCPOLEN_SACK_PERBLOCK) {
+					tmp = (__be32 *) (ptr + i);
+					old_ack_seq = ntohl(*tmp);
+					*tmp = htonl((__u32)
+						     (old_ack_seq -
+						      sp_seq->delta));
+					syn_proxy_seq_csum_update(th,
+								  htonl
+								  (old_ack_seq),
+								  *tmp);
+					IP_VS_DBG(6,
+						  "syn_proxy_filter_opt_outin: sack_left_seq %u => %u, delta = %u \n",
+						  old_ack_seq, ntohl(*tmp),
+						  sp_seq->delta);
+					tmp++;
+					old_ack_seq = ntohl(*tmp);
+					*tmp = htonl((__u32)
+						     (old_ack_seq -
+						      sp_seq->delta));
+					syn_proxy_seq_csum_update(th,
+								  htonl
+								  (old_ack_seq),
+								  *tmp);
+					IP_VS_DBG(6,
+						  "syn_proxy_filter_opt_outin: sack_right_seq %u => %u, delta = %u \n",
+						  old_ack_seq, ntohl(*tmp),
+						  sp_seq->delta);
+				}
+				return;
+			}
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * Update out-in ack_seqs: include th->ack_seq, sack opt
+ * and also correct tcph->check.
+ */
+void ip_vs_synproxy_dnat_handler(struct tcphdr *tcph, struct ip_vs_seq *sp_seq)
+{
+	__u32 old_ack_seq;
+
+	if (sp_seq->delta != 0) {
+		old_ack_seq = ntohl(tcph->ack_seq);
+		tcph->ack_seq = htonl((__u32) (old_ack_seq - sp_seq->delta));
+		syn_proxy_seq_csum_update(tcph, htonl(old_ack_seq),
+					  tcph->ack_seq);
+		syn_proxy_filter_opt_outin(tcph, sp_seq);
+		IP_VS_DBG(6,
+			  "tcp_dnat_handler: tcph->ack_seq %u => %u, delta = %u \n",
+			  old_ack_seq, htonl(tcph->ack_seq), sp_seq->delta);
+	}
+}
+
+/*
+ * Syn-proxy step 3 logic: receive syn-ack from rs
+ * Update syn_proxy_seq.delta and send stored ack skbs
+ * to rs.
+ */
+int
+ip_vs_synproxy_synack_rcv(struct sk_buff *skb, struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp, int ihl, int *verdict)
+{
+	struct tcphdr _tcph, *th;
+	struct sk_buff_head save_skb;
+	struct sk_buff *tmp_skb = NULL;
+	struct ip_vs_dest *dest = cp->dest;
+
+	th = skb_header_pointer(skb, ihl, sizeof(_tcph), &_tcph);
+	if (th == NULL) {
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	IP_VS_DBG(6, "in syn_proxy_synack_rcv, "
+		  "seq = %u ack_seq = %u %c%c%c cp->is_synproxy = %u cp->state = %u\n",
+		  ntohl(th->seq),
+		  ntohl(th->ack_seq),
+		  (th->syn) ? 'S' : '-',
+		  (th->ack) ? 'A' : '-',
+		  (th->rst) ? 'R' : '-',
+		  cp->flags & IP_VS_CONN_F_SYNPROXY, cp->state);
+
+	skb_queue_head_init(&save_skb);
+	spin_lock(&cp->lock);
+	if ((th->syn) && (th->ack) && (!th->rst) &&
+	    (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    cp->state == IP_VS_TCP_S_SYN_SENT) {
+		cp->syn_proxy_seq.delta =
+		    htonl(cp->syn_proxy_seq.init_seq) - htonl(th->seq);
+		cp->timeout = pp->timeout_table[cp->state =
+						IP_VS_TCP_S_ESTABLISHED];
+		if (dest) {
+			atomic_inc(&dest->activeconns);
+			atomic_dec(&dest->inactconns);
+			cp->flags &= ~IP_VS_CONN_F_INACTIVE;
+		}
+
+		/* save tcp sequense for fullnat/nat, INside to OUTside */
+		if (sysctl_ip_vs_conn_expire_tcp_rst == 1) {
+			cp->rs_end_seq = htonl(ntohl(th->seq) + 1);
+			cp->rs_ack_seq = th->ack_seq;
+			IP_VS_DBG_RL("packet from RS, seq:%u ack_seq:%u.",
+				     ntohl(th->seq), ntohl(th->ack_seq));
+			IP_VS_DBG_RL("port:%u->%u", ntohs(th->source),
+				     ntohs(th->dest));
+		}
+
+		/* First: free stored syn skb */
+		if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		if (skb_queue_len(&cp->ack_skb) <= 0) {
+			/*
+			 * FIXME: maybe a bug here, print err msg and go.
+			 * Attention: cp->state has been changed and we
+			 * should still DROP the Syn/Ack skb.
+			 */
+			IP_VS_ERR_RL
+			    ("Got ack_skb NULL pointer in syn_proxy_synack_rcv\n");
+			spin_unlock(&cp->lock);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+			skb_queue_tail(&save_skb, tmp_skb);
+		}
+
+		/*
+		 * Release the lock, because we don't
+		 * touch session any more.
+		 */
+		spin_unlock(&cp->lock);
+
+		while ((tmp_skb = skb_dequeue(&save_skb)) != NULL) {
+			/* If xmit failed, syn_skb will be freed correctly. */
+			cp->packet_xmit(tmp_skb, cp, pp);
+		}
+
+		*verdict = NF_DROP;
+		return 0;
+	} else if ((th->rst) &&
+		   (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		   cp->state == IP_VS_TCP_S_SYN_SENT) {
+		__u32 temp_seq;
+		temp_seq = ntohl(th->seq);
+		IP_VS_DBG(6, "get rst from rs, seq = %u ack_seq= %u\n",
+			  ntohl(th->seq), ntohl(th->ack_seq));
+		/* coute the delta of seq */
+		cp->syn_proxy_seq.delta =
+		    ntohl(cp->syn_proxy_seq.init_seq) - ntohl(th->seq);
+		cp->timeout = pp->timeout_table[cp->state = IP_VS_TCP_S_CLOSE];
+		spin_unlock(&cp->lock);
+		th->seq = htonl(ntohl(th->seq) + 1);
+		syn_proxy_seq_csum_update(th, htonl(temp_seq), th->seq);
+
+		return 1;
+	}
+	spin_unlock(&cp->lock);
+
+	return 1;
+}
+
+static inline void
+__syn_proxy_reuse_conn(struct ip_vs_conn *cp,
+		       struct sk_buff *ack_skb,
+		       struct tcphdr *th, struct ip_vs_protocol *pp)
+{
+	struct sk_buff *tmp_skb = NULL;
+
+	/* Free stored ack packet */
+	while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+		kfree_skb(tmp_skb);
+		tmp_skb = NULL;
+	}
+
+	/* Free stored syn skb */
+	if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+		kfree_skb(tmp_skb);
+		tmp_skb = NULL;
+	}
+
+	/* Store new ack_skb */
+	skb_queue_head_init(&cp->ack_skb);
+	skb_queue_tail(&cp->ack_skb, ack_skb);
+
+	/* Save ack_seq - 1 */
+	cp->syn_proxy_seq.init_seq = htonl((__u32) ((htonl(th->ack_seq) - 1)));
+	/* don't change delta here, so original flow can still be valid */
+
+	/* Save ack_seq */
+	cp->fnat_seq.fdata_seq = ntohl(th->ack_seq);
+
+	cp->fnat_seq.init_seq = 0;
+
+	/* Clean dup ack cnt */
+	atomic_set(&cp->dup_ack_cnt, 0);
+
+	/* Set timeout value */
+	cp->timeout = pp->timeout_table[cp->state = IP_VS_TCP_S_SYN_SENT];
+}
+
+/*
+ * Syn-proxy session reuse function.
+ * Update syn_proxy_seq struct and clean syn-proxy related
+ * members.
+ */
+int
+ip_vs_synproxy_reuse_conn(int af, struct sk_buff *skb,
+			  struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp,
+			  struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct tcphdr _tcph, *th = NULL;
+	struct ip_vs_synproxy_opt opt;
+	int res_cookie_check;
+	u32 tcp_conn_reuse_states = 0;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	if (unlikely(NULL == th)) {
+		IP_VS_ERR_RL("skb has a invalid tcp header\n");
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	tcp_conn_reuse_states =
+	    ((sysctl_ip_vs_synproxy_conn_reuse_cl << IP_VS_TCP_S_CLOSE) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_tw << IP_VS_TCP_S_TIME_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_fw << IP_VS_TCP_S_FIN_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_cw << IP_VS_TCP_S_CLOSE_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_la << IP_VS_TCP_S_LAST_ACK));
+
+	if (((1 << (cp->state)) & tcp_conn_reuse_states) &&
+	    (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (!th->syn && th->ack && !th->rst && !th->fin) &&
+	    (cp->syn_proxy_seq.init_seq !=
+	     htonl((__u32) ((ntohl(th->ack_seq) - 1))))) {
+		/*
+		 * Import: set tcp hdr before cookie check, as it
+		 * will be used in cookie_check funcs.
+		 */
+		skb_set_transport_header(skb, iph->len);
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			res_cookie_check = ip_vs_synproxy_v6_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		} else
+#endif
+		{
+			res_cookie_check = ip_vs_synproxy_v4_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		}
+
+		if (!res_cookie_check) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_BAD_ACK);
+			/*
+			 * Cookie check fail, let it go.
+			 */
+			return 1;
+		}
+
+		/* update statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_OK_ACK);
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_CONN_REUSED);
+		switch (cp->old_state) {
+		case IP_VS_TCP_S_CLOSE:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_CLOSE);
+			break;
+		case IP_VS_TCP_S_TIME_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_TIMEWAIT);
+			break;
+		case IP_VS_TCP_S_FIN_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_FINWAIT);
+			break;
+		case IP_VS_TCP_S_CLOSE_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_CLOSEWAIT);
+			break;
+		case IP_VS_TCP_S_LAST_ACK:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_LASTACK);
+			break;
+		}
+
+		spin_lock(&cp->lock);
+		__syn_proxy_reuse_conn(cp, skb, th, pp);
+		spin_unlock(&cp->lock);
+
+		if (unlikely(!syn_proxy_send_rs_syn(af, th, cp, skb, pp, &opt))) {
+			IP_VS_ERR_RL
+			    ("syn_proxy_send_rs_syn failed when reuse conn!\n");
+			/* release conn immediately */
+			spin_lock(&cp->lock);
+			cp->timeout = 0;
+			spin_unlock(&cp->lock);
+		}
+
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * Check and stop ack storm.
+ * Return 0 if ack storm is found.
+ */
+static int syn_proxy_is_ack_storm(struct tcphdr *tcph, struct ip_vs_conn *cp)
+{
+	/* only for syn-proxy sessions */
+	if (!(cp->flags & IP_VS_CONN_F_SYNPROXY) || !tcph->ack)
+		return 1;
+
+	if (unlikely(sysctl_ip_vs_synproxy_dup_ack_thresh == 0))
+		return 1;
+
+	if (unlikely(tcph->seq == cp->last_seq &&
+		     tcph->ack_seq == cp->last_ack_seq)) {
+		atomic_inc(&cp->dup_ack_cnt);
+		if (atomic_read(&cp->dup_ack_cnt) >=
+		    sysctl_ip_vs_synproxy_dup_ack_thresh) {
+			atomic_set(&cp->dup_ack_cnt,
+				   sysctl_ip_vs_synproxy_dup_ack_thresh);
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_ACK_STORM);
+			return 0;
+		}
+
+		return 1;
+	}
+
+	cp->last_seq = tcph->seq;
+	cp->last_ack_seq = tcph->ack_seq;
+	atomic_set(&cp->dup_ack_cnt, 0);
+
+	return 1;
+}
+
+/*
+ * Syn-proxy snat handler:
+ * 1) check and stop ack storm.
+ * 2)Update in-out seqs: include th->seq
+ * and also correct tcph->check.
+ *
+ * Return 0 if ack storm is found and stoped.
+ */
+int ip_vs_synproxy_snat_handler(struct tcphdr *tcph, struct ip_vs_conn *cp)
+{
+	__u32 old_seq;
+
+	if (syn_proxy_is_ack_storm(tcph, cp) == 0) {
+		return 0;
+	}
+
+	if (cp->syn_proxy_seq.delta != 0) {
+		old_seq = ntohl(tcph->seq);
+		tcph->seq = htonl((__u32) (old_seq + cp->syn_proxy_seq.delta));
+		syn_proxy_seq_csum_update(tcph, htonl(old_seq), tcph->seq);
+		IP_VS_DBG(6,
+			  "tcp_snat_handler: tcph->seq %u => %u, delta = %u \n",
+			  old_seq, htonl(tcph->seq), cp->syn_proxy_seq.delta);
+	}
+
+	return 1;
+}
+
+int
+ip_vs_synproxy_filter_ack(struct sk_buff *skb, struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp,
+			  struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct tcphdr _tcph, *th;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+
+	if (unlikely(NULL == th)) {
+		IP_VS_ERR_RL("skb has a invalid tcp header\n");
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	spin_lock(&cp->lock);
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    cp->state == IP_VS_TCP_S_SYN_SENT) {
+		/*
+		 * Not a ack packet, drop it.
+		 */
+		if (!th->ack) {
+			spin_unlock(&cp->lock);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		if (sysctl_ip_vs_synproxy_skb_store_thresh <
+		    skb_queue_len(&cp->ack_skb)) {
+			spin_unlock(&cp->lock);
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_SYNSEND_QLEN);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Still some space left, store it.
+		 */
+		skb_queue_tail(&cp->ack_skb, skb);
+		spin_unlock(&cp->lock);
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	spin_unlock(&cp->lock);
+	return 1;
+}
diff --git a/net/netfilter/ipvs/ip_vs_wlc.c b/net/netfilter/ipvs/ip_vs_wlc.c
index c60a81c..bbddfdb 100644
--- a/net/netfilter/ipvs/ip_vs_wlc.c
+++ b/net/netfilter/ipvs/ip_vs_wlc.c
@@ -27,6 +27,22 @@
 
 #include <net/ip_vs.h>
 
+
+static inline unsigned int
+ip_vs_wlc_dest_overhead(struct ip_vs_dest *dest)
+{
+	/*
+	 * We think the overhead of processing active connections is 256
+	 * times higher than that of inactive connections in average. (This
+	 * 256 times might not be accurate, we will change it later) We
+	 * use the following formula to estimate the overhead now:
+	 *		  dest->activeconns*256 + dest->inactconns
+	 */
+	return (atomic_read(&dest->activeconns) << 8) +
+		atomic_read(&dest->inactconns);
+}
+
+
 /*
  *	Weighted Least Connection scheduling
  */
@@ -51,25 +67,25 @@ ip_vs_wlc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	 * new connections.
 	 */
 
-	list_for_each_entry_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry(dest, &svc->destinations, n_list) {
 		if (!(dest->flags & IP_VS_DEST_F_OVERLOAD) &&
 		    atomic_read(&dest->weight) > 0) {
 			least = dest;
-			loh = ip_vs_dest_conn_overhead(least);
+			loh = ip_vs_wlc_dest_overhead(least);
 			goto nextstage;
 		}
 	}
-	ip_vs_scheduler_err(svc, "no destination available");
+	IP_VS_ERR_RL("WLC: no destination available\n");
 	return NULL;
 
 	/*
 	 *    Find the destination with the least load.
 	 */
   nextstage:
-	list_for_each_entry_continue_rcu(dest, &svc->destinations, n_list) {
+	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
-		doh = ip_vs_dest_conn_overhead(dest);
+		doh = ip_vs_wlc_dest_overhead(dest);
 		if (loh * atomic_read(&dest->weight) >
 		    doh * atomic_read(&least->weight)) {
 			least = dest;
@@ -106,7 +122,6 @@ static int __init ip_vs_wlc_init(void)
 static void __exit ip_vs_wlc_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_wlc_scheduler);
-	synchronize_rcu();
 }
 
 module_init(ip_vs_wlc_init);
diff --git a/net/netfilter/ipvs/ip_vs_wrr.c b/net/netfilter/ipvs/ip_vs_wrr.c
index 0e68555..6182e8e 100644
--- a/net/netfilter/ipvs/ip_vs_wrr.c
+++ b/net/netfilter/ipvs/ip_vs_wrr.c
@@ -23,54 +23,35 @@
 
 #include <linux/module.h>
 #include <linux/kernel.h>
-#include <linux/slab.h>
 #include <linux/net.h>
-#include <linux/gcd.h>
 
 #include <net/ip_vs.h>
 
-/* The WRR algorithm depends on some caclulations:
- * - mw: maximum weight
- * - di: weight step, greatest common divisor from all weights
- * - cw: current required weight
- * As result, all weights are in the [di..mw] range with a step=di.
- *
- * First, we start with cw = mw and select dests with weight >= cw.
- * Then cw is reduced with di and all dests are checked again.
- * Last pass should be with cw = di. We have mw/di passes in total:
- *
- * pass 1: cw = max weight
- * pass 2: cw = max weight - di
- * pass 3: cw = max weight - 2 * di
- * ...
- * last pass: cw = di
- *
- * Weights are supposed to be >= di but we run in parallel with
- * weight changes, it is possible some dest weight to be reduced
- * below di, bad if it is the only available dest.
- *
- * So, we modify how mw is calculated, now it is reduced with (di - 1),
- * so that last cw is 1 to catch such dests with weight below di:
- * pass 1: cw = max weight - (di - 1)
- * pass 2: cw = max weight - di - (di - 1)
- * pass 3: cw = max weight - 2 * di - (di - 1)
- * ...
- * last pass: cw = 1
- *
- */
-
 /*
  * current destination pointer for weighted round-robin scheduling
  */
 struct ip_vs_wrr_mark {
-	struct ip_vs_dest *cl;	/* current dest or head */
+	struct list_head *cl;	/* current list head */
 	int cw;			/* current weight */
 	int mw;			/* maximum weight */
 	int di;			/* decreasing interval */
-	struct rcu_head		rcu_head;
 };
 
 
+/*
+ *    Get the gcd of server weights
+ */
+static int gcd(int a, int b)
+{
+	int c;
+
+	while ((c = a % b)) {
+		a = b;
+		b = c;
+	}
+	return b;
+}
+
 static int ip_vs_wrr_gcd_weight(struct ip_vs_service *svc)
 {
 	struct ip_vs_dest *dest;
@@ -115,45 +96,41 @@ static int ip_vs_wrr_init_svc(struct ip_vs_service *svc)
 	/*
 	 *    Allocate the mark variable for WRR scheduling
 	 */
-	mark = kmalloc(sizeof(struct ip_vs_wrr_mark), GFP_KERNEL);
-	if (mark == NULL)
+	mark = kmalloc(sizeof(struct ip_vs_wrr_mark), GFP_ATOMIC);
+	if (mark == NULL) {
+		pr_err("%s(): no memory\n", __func__);
 		return -ENOMEM;
-
-	mark->cl = list_entry(&svc->destinations, struct ip_vs_dest, n_list);
+	}
+	mark->cl = &svc->destinations;
+	mark->cw = 0;
+	mark->mw = ip_vs_wrr_max_weight(svc);
 	mark->di = ip_vs_wrr_gcd_weight(svc);
-	mark->mw = ip_vs_wrr_max_weight(svc) - (mark->di - 1);
-	mark->cw = mark->mw;
 	svc->sched_data = mark;
 
 	return 0;
 }
 
 
-static void ip_vs_wrr_done_svc(struct ip_vs_service *svc)
+static int ip_vs_wrr_done_svc(struct ip_vs_service *svc)
 {
-	struct ip_vs_wrr_mark *mark = svc->sched_data;
-
 	/*
 	 *    Release the mark variable
 	 */
-	kfree_rcu(mark, rcu_head);
+	kfree(svc->sched_data);
+
+	return 0;
 }
 
 
-static int ip_vs_wrr_dest_changed(struct ip_vs_service *svc,
-				  struct ip_vs_dest *dest)
+static int ip_vs_wrr_update_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_wrr_mark *mark = svc->sched_data;
 
-	spin_lock_bh(&svc->sched_lock);
-	mark->cl = list_entry(&svc->destinations, struct ip_vs_dest, n_list);
+	mark->cl = &svc->destinations;
+	mark->mw = ip_vs_wrr_max_weight(svc);
 	mark->di = ip_vs_wrr_gcd_weight(svc);
-	mark->mw = ip_vs_wrr_max_weight(svc) - (mark->di - 1);
-	if (mark->cw > mark->mw || !mark->cw)
-		mark->cw = mark->mw;
-	else if (mark->di > 1)
-		mark->cw = (mark->cw / mark->di) * mark->di + 1;
-	spin_unlock_bh(&svc->sched_lock);
+	if (mark->cw > mark->mw)
+		mark->cw = 0;
 	return 0;
 }
 
@@ -164,79 +141,78 @@ static int ip_vs_wrr_dest_changed(struct ip_vs_service *svc,
 static struct ip_vs_dest *
 ip_vs_wrr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 {
-	struct ip_vs_dest *dest, *last, *stop = NULL;
+	struct ip_vs_dest *dest;
 	struct ip_vs_wrr_mark *mark = svc->sched_data;
-	bool last_pass = false, restarted = false;
+	struct list_head *p;
 
 	IP_VS_DBG(6, "%s(): Scheduling...\n", __func__);
 
-	spin_lock_bh(&svc->sched_lock);
-	dest = mark->cl;
-	/* No available dests? */
-	if (mark->mw == 0)
-		goto err_noavail;
-	last = dest;
-	/* Stop only after all dests were checked for weight >= 1 (last pass) */
+	/*
+	 * This loop will always terminate, because mark->cw in (0, max_weight]
+	 * and at least one server has its weight equal to max_weight.
+	 */
+	write_lock(&svc->sched_lock);
+	p = mark->cl;
 	while (1) {
-		list_for_each_entry_continue_rcu(dest,
-						 &svc->destinations,
-						 n_list) {
+		if (mark->cl == &svc->destinations) {
+			/* it is at the head of the destination list */
+
+			if (mark->cl == mark->cl->next) {
+				/* no dest entry */
+				IP_VS_ERR_RL("WRR: no destination available: "
+					     "no destinations present\n");
+				dest = NULL;
+				goto out;
+			}
+
+			mark->cl = svc->destinations.next;
+			mark->cw -= mark->di;
+			if (mark->cw <= 0) {
+				mark->cw = mark->mw;
+				/*
+				 * Still zero, which means no available servers.
+				 */
+				if (mark->cw == 0) {
+					mark->cl = &svc->destinations;
+					IP_VS_ERR_RL("WRR: no destination "
+						     "available\n");
+					dest = NULL;
+					goto out;
+				}
+			}
+		} else
+			mark->cl = mark->cl->next;
+
+		if (mark->cl != &svc->destinations) {
+			/* not at the head of the list */
+			dest = list_entry(mark->cl, struct ip_vs_dest, n_list);
 			if (!(dest->flags & IP_VS_DEST_F_OVERLOAD) &&
-			    atomic_read(&dest->weight) >= mark->cw)
-				goto found;
-			if (dest == stop)
-				goto err_over;
+			    atomic_read(&dest->weight) >= mark->cw) {
+				/* got it */
+				break;
+			}
 		}
-		mark->cw -= mark->di;
-		if (mark->cw <= 0) {
-			mark->cw = mark->mw;
-			/* Stop if we tried last pass from first dest:
-			 * 1. last_pass: we started checks when cw > di but
-			 *	then all dests were checked for w >= 1
-			 * 2. last was head: the first and only traversal
-			 *	was for weight >= 1, for all dests.
-			 */
-			if (last_pass ||
-			    &last->n_list == &svc->destinations)
-				goto err_over;
-			restarted = true;
-		}
-		last_pass = mark->cw <= mark->di;
-		if (last_pass && restarted &&
-		    &last->n_list != &svc->destinations) {
-			/* First traversal was for w >= 1 but only
-			 * for dests after 'last', now do the same
-			 * for all dests up to 'last'.
-			 */
-			stop = last;
+
+		if (mark->cl == p && mark->cw == mark->di) {
+			/* back to the start, and no dest is found.
+			   It is only possible when all dests are OVERLOADED */
+			dest = NULL;
+			IP_VS_ERR_RL("WRR: no destination available: "
+				     "all destinations are overloaded\n");
+			goto out;
 		}
 	}
 
-found:
 	IP_VS_DBG_BUF(6, "WRR: server %s:%u "
 		      "activeconns %d refcnt %d weight %d\n",
 		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port),
 		      atomic_read(&dest->activeconns),
 		      atomic_read(&dest->refcnt),
 		      atomic_read(&dest->weight));
-	mark->cl = dest;
 
   out:
-	spin_unlock_bh(&svc->sched_lock);
+	write_unlock(&svc->sched_lock);
 	return dest;
-
-err_noavail:
-	mark->cl = dest;
-	dest = NULL;
-	ip_vs_scheduler_err(svc, "no destination available");
-	goto out;
-
-err_over:
-	mark->cl = dest;
-	dest = NULL;
-	ip_vs_scheduler_err(svc, "no destination available: "
-			    "all destinations are overloaded");
-	goto out;
 }
 
 
@@ -247,9 +223,7 @@ static struct ip_vs_scheduler ip_vs_wrr_scheduler = {
 	.n_list =		LIST_HEAD_INIT(ip_vs_wrr_scheduler.n_list),
 	.init_service =		ip_vs_wrr_init_svc,
 	.done_service =		ip_vs_wrr_done_svc,
-	.add_dest =		ip_vs_wrr_dest_changed,
-	.del_dest =		ip_vs_wrr_dest_changed,
-	.upd_dest =		ip_vs_wrr_dest_changed,
+	.update_service =	ip_vs_wrr_update_svc,
 	.schedule =		ip_vs_wrr_schedule,
 };
 
@@ -261,7 +235,6 @@ static int __init ip_vs_wrr_init(void)
 static void __exit ip_vs_wrr_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_wrr_scheduler);
-	synchronize_rcu();
 }
 
 module_init(ip_vs_wrr_init);
diff --git a/net/netfilter/ipvs/ip_vs_xmit.c b/net/netfilter/ipvs/ip_vs_xmit.c
index 033f021..9e5f34c 100644
--- a/net/netfilter/ipvs/ip_vs_xmit.c
+++ b/net/netfilter/ipvs/ip_vs_xmit.c
@@ -11,673 +11,1436 @@
  *
  * Changes:
  *
- * Description of forwarding methods:
- * - all transmitters are called from LOCAL_IN (remote clients) and
- * LOCAL_OUT (local clients) but for ICMP can be called from FORWARD
- * - not all connections have destination server, for example,
- * connections in backup server when fwmark is used
- * - bypass connections use daddr from packet
- * - we can use dst without ref while sending in RCU section, we use
- * ref when returning NF_ACCEPT for NAT-ed packet via loopback
- * LOCAL_OUT rules:
- * - skb->dev is NULL, skb->protocol is not set (both are set in POST_ROUTING)
- * - skb->pkt_type is not set yet
- * - the only place where we can see skb->sk != NULL
  */
 
 #define KMSG_COMPONENT "IPVS"
 #define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
 
 #include <linux/kernel.h>
-#include <linux/slab.h>
-#include <linux/tcp.h>                  /* for tcphdr */
+#include <linux/tcp.h>		/* for tcphdr */
 #include <net/ip.h>
-#include <net/tcp.h>                    /* for csum_tcpudp_magic */
+#include <net/tcp.h>		/* for csum_tcpudp_magic */
 #include <net/udp.h>
-#include <net/icmp.h>                   /* for icmp_send */
-#include <net/route.h>                  /* for ip_route_output */
+#include <net/icmp.h>		/* for icmp_send */
+#include <net/route.h>		/* for ip_route_output */
 #include <net/ipv6.h>
 #include <net/ip6_route.h>
-#include <net/addrconf.h>
 #include <linux/icmpv6.h>
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
+#include <linux/netfilter_ipv6.h>
 
 #include <net/ip_vs.h>
-
-enum {
-	IP_VS_RT_MODE_LOCAL	= 1, /* Allow local dest */
-	IP_VS_RT_MODE_NON_LOCAL	= 2, /* Allow non-local dest */
-	IP_VS_RT_MODE_RDR	= 4, /* Allow redirect from remote daddr to
-				      * local
-				      */
-	IP_VS_RT_MODE_CONNECT	= 8, /* Always bind route to saddr */
-	IP_VS_RT_MODE_KNOWN_NH	= 16,/* Route via remote addr */
-	IP_VS_RT_MODE_TUNNEL	= 32,/* Tunnel mode */
-};
-
-static inline struct ip_vs_dest_dst *ip_vs_dest_dst_alloc(void)
-{
-	return kmalloc(sizeof(struct ip_vs_dest_dst), GFP_ATOMIC);
-}
-
-static inline void ip_vs_dest_dst_free(struct ip_vs_dest_dst *dest_dst)
-{
-	kfree(dest_dst);
-}
+#include <linux/if_arp.h>
 
 /*
  *      Destination cache to speed up outgoing route lookup
  */
 static inline void
-__ip_vs_dst_set(struct ip_vs_dest *dest, struct ip_vs_dest_dst *dest_dst,
-		struct dst_entry *dst, u32 dst_cookie)
+__ip_vs_dst_set(struct ip_vs_dest *dest, u32 rtos, struct dst_entry *dst)
 {
-	struct ip_vs_dest_dst *old;
+	struct dst_entry *old_dst;
 
-	old = rcu_dereference_protected(dest->dest_dst,
-					lockdep_is_held(&dest->dst_lock));
-
-	if (dest_dst) {
-		dest_dst->dst_cache = dst;
-		dest_dst->dst_cookie = dst_cookie;
-	}
-	rcu_assign_pointer(dest->dest_dst, dest_dst);
-
-	if (old)
-		call_rcu(&old->rcu_head, ip_vs_dest_dst_rcu_free);
+	old_dst = dest->dst_cache;
+	dest->dst_cache = dst;
+	dest->dst_rtos = rtos;
+	dst_release(old_dst);
 }
 
-static inline struct ip_vs_dest_dst *
-__ip_vs_dst_check(struct ip_vs_dest *dest)
+static inline struct dst_entry *__ip_vs_dst_check(struct ip_vs_dest *dest,
+						  u32 rtos, u32 cookie)
 {
-	struct ip_vs_dest_dst *dest_dst = rcu_dereference(dest->dest_dst);
-	struct dst_entry *dst;
+	struct dst_entry *dst = dest->dst_cache;
 
-	if (!dest_dst)
+	if (!dst)
 		return NULL;
-	dst = dest_dst->dst_cache;
-	if (dst->obsolete &&
-	    dst->ops->check(dst, dest_dst->dst_cookie) == NULL)
+	if ((dst->obsolete
+	     || (dest->af == AF_INET && rtos != dest->dst_rtos)) &&
+	    dst->ops->check(dst, cookie) == NULL) {
+		dest->dst_cache = NULL;
+		dst_release(dst);
 		return NULL;
-	return dest_dst;
+	}
+	dst_hold(dst);
+	return dst;
 }
 
-static inline bool
-__mtu_check_toobig_v6(const struct sk_buff *skb, u32 mtu)
+static struct rtable *__ip_vs_get_out_rt(struct ip_vs_conn *cp, u32 rtos)
 {
-	if (IP6CB(skb)->frag_max_size) {
-		/* frag_max_size tell us that, this packet have been
-		 * defragmented by netfilter IPv6 conntrack module.
-		 */
-		if (IP6CB(skb)->frag_max_size > mtu)
-			return true; /* largest fragment violate MTU */
-	}
-	else if (skb->len > mtu && !skb_is_gso(skb)) {
-		return true; /* Packet size violate MTU size */
+	struct rtable *rt;	/* Route to the other host */
+	struct ip_vs_dest *dest = cp->dest;
+
+	if (dest) {
+		spin_lock(&dest->dst_lock);
+		if (!(rt = (struct rtable *)
+		      __ip_vs_dst_check(dest, rtos, 0))) {
+			struct flowi4 fl = {
+				.daddr = dest->addr.ip,
+				.saddr = 0,
+				.flowi4_tos = rtos,
+			};
+
+			rt = ip_route_output_key(&init_net, &fl);
+			if (IS_ERR(rt)) {
+				spin_unlock(&dest->dst_lock);
+				IP_VS_DBG_RL
+				    ("ip_route_output error, dest: %pI4\n",
+				     &dest->addr.ip);
+				return NULL;
+			}
+			__ip_vs_dst_set(dest, rtos, dst_clone(&rt->dst));
+			IP_VS_DBG(10, "new dst %pI4, refcnt=%d, rtos=%X\n",
+				  &dest->addr.ip,
+				  atomic_read(&rt->dst.__refcnt), rtos);
+		}
+		spin_unlock(&dest->dst_lock);
+	} else {
+		struct flowi4 fl = {
+			.daddr = cp->daddr.ip,
+			.saddr = 0,
+			.flowi4_tos = rtos,
+		};
+
+		rt = ip_route_output_key(&init_net, &fl);
+		if (IS_ERR(rt)) {
+			IP_VS_DBG_RL("ip_route_output error, dest: %pI4\n",
+				     &cp->daddr.ip);
+			return NULL;
+		}
 	}
-	return false;
+
+	return rt;
 }
 
-/* Get route to daddr, update *saddr, optionally bind route to saddr */
-static struct rtable *do_output_route4(struct net *net, __be32 daddr,
-				       int rt_mode, __be32 *saddr)
+struct rtable *ip_vs_get_rt(union nf_inet_addr *addr, u32 rtos)
 {
-	struct flowi4 fl4;
-	struct rtable *rt;
-	int loop = 0;
+	struct rtable *rt;	/* Route to the other host */
 
-	memset(&fl4, 0, sizeof(fl4));
-	fl4.daddr = daddr;
-	fl4.saddr = (rt_mode & IP_VS_RT_MODE_CONNECT) ? *saddr : 0;
-	fl4.flowi4_flags = (rt_mode & IP_VS_RT_MODE_KNOWN_NH) ?
-			   FLOWI_FLAG_KNOWN_NH : 0;
+	struct flowi4 fl = {
+		.daddr = addr->ip,
+		.saddr = 0,
+		.flowi4_tos = rtos,
+	};
 
-retry:
-	rt = ip_route_output_key(net, &fl4);
+	rt = ip_route_output_key(&init_net, &fl);
 	if (IS_ERR(rt)) {
-		/* Invalid saddr ? */
-		if (PTR_ERR(rt) == -EINVAL && *saddr &&
-		    rt_mode & IP_VS_RT_MODE_CONNECT && !loop) {
-			*saddr = 0;
-			flowi4_update_output(&fl4, 0, 0, daddr, 0);
-			goto retry;
-		}
-		IP_VS_DBG_RL("ip_route_output error, dest: %pI4\n", &daddr);
+		IP_VS_DBG_RL("ip_route_output error, dest: %pI4\n", &addr->ip);
 		return NULL;
-	} else if (!*saddr && rt_mode & IP_VS_RT_MODE_CONNECT && fl4.saddr) {
-		ip_rt_put(rt);
-		*saddr = fl4.saddr;
-		flowi4_update_output(&fl4, 0, 0, daddr, fl4.saddr);
-		loop++;
-		goto retry;
 	}
-	*saddr = fl4.saddr;
+
 	return rt;
 }
 
-/* Get route to destination or remote server */
-static int
-__ip_vs_get_out_rt(struct sk_buff *skb, struct ip_vs_dest *dest,
-		   __be32 daddr, int rt_mode, __be32 *ret_saddr)
+#ifdef CONFIG_IP_VS_IPV6
+static struct rt6_info *__ip_vs_get_out_rt_v6(struct ip_vs_conn *cp)
 {
-	struct net *net = dev_net(skb_dst(skb)->dev);
-	struct netns_ipvs *ipvs = net_ipvs(net);
-	struct ip_vs_dest_dst *dest_dst;
-	struct rtable *rt;			/* Route to the other host */
-	struct rtable *ort;			/* Original route */
-	struct iphdr *iph;
-	__be16 df;
-	int mtu;
-	int local, noref = 1;
+	struct rt6_info *rt;	/* Route to the other host */
+	struct ip_vs_dest *dest = cp->dest;
 
 	if (dest) {
-		dest_dst = __ip_vs_dst_check(dest);
-		if (likely(dest_dst))
-			rt = (struct rtable *) dest_dst->dst_cache;
-		else {
-			dest_dst = ip_vs_dest_dst_alloc();
-			spin_lock_bh(&dest->dst_lock);
-			if (!dest_dst) {
-				__ip_vs_dst_set(dest, NULL, NULL, 0);
-				spin_unlock_bh(&dest->dst_lock);
-				goto err_unreach;
-			}
-			rt = do_output_route4(net, dest->addr.ip, rt_mode,
-					      &dest_dst->dst_saddr.ip);
+		spin_lock(&dest->dst_lock);
+		rt = (struct rt6_info *)__ip_vs_dst_check(dest, 0, 0);
+		if (!rt) {
+			struct flowi6 fl = {
+				.daddr = dest->addr.in6,
+				.saddr = {.s6_addr32 = {0, 0, 0.0},},
+			};
+
+			rt = (struct rt6_info *)ip6_route_output(&init_net,
+								 NULL, &fl);
 			if (!rt) {
-				__ip_vs_dst_set(dest, NULL, NULL, 0);
-				spin_unlock_bh(&dest->dst_lock);
-				ip_vs_dest_dst_free(dest_dst);
-				goto err_unreach;
+				spin_unlock(&dest->dst_lock);
+				IP_VS_DBG_RL
+				    ("ip6_route_output error, dest: %pI6\n",
+				     &dest->addr.in6);
+				return NULL;
 			}
-			__ip_vs_dst_set(dest, dest_dst, &rt->dst, 0);
-			spin_unlock_bh(&dest->dst_lock);
-			IP_VS_DBG(10, "new dst %pI4, src %pI4, refcnt=%d\n",
-				  &dest->addr.ip, &dest_dst->dst_saddr.ip,
+			__ip_vs_dst_set(dest, 0, dst_clone(&rt->dst));
+			IP_VS_DBG(10, "new dst %pI6, refcnt=%d\n",
+				  &dest->addr.in6,
 				  atomic_read(&rt->dst.__refcnt));
 		}
-		daddr = dest->addr.ip;
-		if (ret_saddr)
-			*ret_saddr = dest_dst->dst_saddr.ip;
+		spin_unlock(&dest->dst_lock);
 	} else {
-		__be32 saddr = htonl(INADDR_ANY);
-
-		noref = 0;
-
-		/* For such unconfigured boxes avoid many route lookups
-		 * for performance reasons because we do not remember saddr
-		 */
-		rt_mode &= ~IP_VS_RT_MODE_CONNECT;
-		rt = do_output_route4(net, daddr, rt_mode, &saddr);
-		if (!rt)
-			goto err_unreach;
-		if (ret_saddr)
-			*ret_saddr = saddr;
+		struct flowi6 fl = {
+			.daddr = cp->daddr.in6,
+			.saddr = {.s6_addr32 = {0, 0, 0.0},},
+		};
+
+		rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
+		if (!rt) {
+			IP_VS_DBG_RL("ip6_route_output error, dest: %pI6\n",
+				     &cp->daddr.in6);
+			return NULL;
+		}
 	}
 
-	local = (rt->rt_flags & RTCF_LOCAL) ? 1 : 0;
-	if (!((local ? IP_VS_RT_MODE_LOCAL : IP_VS_RT_MODE_NON_LOCAL) &
-	      rt_mode)) {
-		IP_VS_DBG_RL("Stopping traffic to %s address, dest: %pI4\n",
-			     (rt->rt_flags & RTCF_LOCAL) ?
-			     "local":"non-local", &daddr);
-		goto err_put;
+	return rt;
+}
+
+struct rt6_info *ip_vs_get_rt_v6(union nf_inet_addr *addr)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+
+	struct flowi6 fl = {
+		.daddr = addr->in6,
+		.saddr = {.s6_addr32 = {0, 0, 0.0},},
+	};
+
+	rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
+	if (!rt) {
+		IP_VS_DBG_RL("ip6_route_output error, dest: %pI6\n",
+			     &addr->in6);
+		return NULL;
 	}
-	iph = ip_hdr(skb);
-	if (likely(!local)) {
-		if (unlikely(ipv4_is_loopback(iph->saddr))) {
-			IP_VS_DBG_RL("Stopping traffic from loopback address "
-				     "%pI4 to non-local address, dest: %pI4\n",
-				     &iph->saddr, &daddr);
-			goto err_put;
+
+	return rt;
+}
+#endif
+
+/*
+ *	Release dest->dst_cache before a dest is removed
+ */
+void ip_vs_dst_reset(struct ip_vs_dest *dest)
+{
+	struct dst_entry *old_dst;
+
+	old_dst = dest->dst_cache;
+	dest->dst_cache = NULL;
+	dst_release(old_dst);
+}
+
+#define IP_VS_XMIT(pf, skb, rt)				\
+do {							\
+	(skb)->ipvs_property = 1;			\
+	skb_forward_csum(skb);				\
+	NF_HOOK(pf, NF_INET_LOCAL_OUT, (skb)->sk, (skb), NULL,	\
+		(rt)->dst.dev, dst_output_sk);		\
+} while (0)
+
+/* check if gso can handle the skb */
+static int gso_ok(struct sk_buff *skb, struct net_device *dev)
+{
+	if (skb_is_gso(skb)) {
+		/* LRO check */
+		if (unlikely(skb_shinfo(skb)->gso_type == 0)) {
+			IP_VS_ERR_RL("%s:LRO is enabled."
+					"Cannot be forwarded\n", dev->name);
+			IP_VS_INC_ESTATS(ip_vs_esmib, LRO_REJECT);
+			goto gso_err;
+		}
+
+		/* GRO check */
+		if (net_gso_ok(dev->features, skb_shinfo(skb)->gso_type)) {
+			/* the skb has frag_list, need do sth here */
+			if (skb_has_frag_list(skb) &&
+					!(dev->features & NETIF_F_FRAGLIST) &&
+							__skb_linearize(skb))
+				goto gso_err;
+
+			IP_VS_DBG_RL("skb length: %d . GSO is ok."
+					"can be forwarded\n", skb->len);
+			IP_VS_INC_ESTATS(ip_vs_esmib, GRO_PASS);
+			goto gso_ok;
+		}
+	}
+
+gso_err:
+	return 0;
+gso_ok:
+	return 1;
+}
+
+/*
+ * Packet has been made sufficiently writable in caller
+ * - inout: 1=in->out, 0=out->in
+ */
+static void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			   struct ip_vs_conn *cp, int inout)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	unsigned int icmp_offset = iph->ihl * 4;
+	struct icmphdr *icmph = (struct icmphdr *)(skb_network_header(skb) +
+						   icmp_offset);
+	struct iphdr *ciph = (struct iphdr *)(icmph + 1);
+	__u32 fullnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT);
+
+	if (fullnat) {
+		if (inout) {
+			iph->daddr = cp->caddr.ip;
+			ciph->saddr = cp->caddr.ip;
+		} else {
+			iph->saddr = cp->laddr.ip;
+			ciph->daddr = cp->laddr.ip;
 		}
+	}
+
+	if (inout) {
+		iph->saddr = cp->vaddr.ip;
+		ip_send_check(iph);
+		ciph->daddr = cp->vaddr.ip;
+		ip_send_check(ciph);
 	} else {
-		ort = skb_rtable(skb);
-		if (!(rt_mode & IP_VS_RT_MODE_RDR) &&
-		    !(ort->rt_flags & RTCF_LOCAL)) {
-			IP_VS_DBG_RL("Redirect from non-local address %pI4 to "
-				     "local requires NAT method, dest: %pI4\n",
-				     &iph->daddr, &daddr);
-			goto err_put;
+		iph->daddr = cp->daddr.ip;
+		ip_send_check(iph);
+		ciph->saddr = cp->daddr.ip;
+		ip_send_check(ciph);
+	}
+
+	/* the TCP/UDP port */
+	if (IPPROTO_TCP == ciph->protocol || IPPROTO_UDP == ciph->protocol) {
+		__be16 *ports = (void *)ciph + ciph->ihl * 4;
+
+		if (fullnat) {
+			if (inout) {
+				ports[0] = cp->cport;
+				/* The seq of packet form client
+				 *  has been changed by fullnat.
+				 * we must fix here to
+				 * ensure a valid icmp PKT */
+				if (IPPROTO_TCP == ciph->protocol) {
+					__be32 *seqs = (__be32 *)ports;
+					seqs[1] = htonl(ntohl(seqs[1]) -
+							cp->fnat_seq.delta);
+				}
+			} else
+				ports[1] = cp->lport;
+		}
+
+		if (inout)
+			ports[1] = cp->vport;
+		else {
+			ports[0] = cp->dport;
+			/* synproxy may modify the seq of packet form RS.
+			 * we fix here to ensure a valid icmp PKT*/
+			if (IPPROTO_TCP == ciph->protocol) {
+				__be32 *seqs = (__be32 *)ports;
+				seqs[1] = htonl(ntohl(seqs[1]) -
+						cp->syn_proxy_seq.delta);
+			}
 		}
-		/* skb to local stack, preserve old route */
-		if (!noref)
-			ip_rt_put(rt);
-		return local;
 	}
 
-	if (likely(!(rt_mode & IP_VS_RT_MODE_TUNNEL))) {
-		mtu = dst_mtu(&rt->dst);
-		df = iph->frag_off & htons(IP_DF);
+	/* And finally the ICMP checksum */
+	icmph->checksum = 0;
+	icmph->checksum = ip_vs_checksum_complete(skb, icmp_offset);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	if (inout)
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered outgoing ICMP");
+	else
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered incoming ICMP");
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int inout)
+{
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	unsigned int icmp_offset = sizeof(struct ipv6hdr);
+	struct icmp6hdr *icmph = (struct icmp6hdr *)(skb_network_header(skb) +
+						     icmp_offset);
+	struct ipv6hdr *ciph = (struct ipv6hdr *)(icmph + 1);
+	__u32 fullnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT);
+
+	if (fullnat) {
+		if (inout) {
+			iph->daddr = cp->caddr.in6;
+			ciph->saddr = cp->caddr.in6;
+		} else {
+			iph->saddr = cp->laddr.in6;
+			ciph->daddr = cp->laddr.in6;
+		}
+	}
+
+	if (inout) {
+		iph->saddr = cp->vaddr.in6;
+		ciph->daddr = cp->vaddr.in6;
 	} else {
-		struct sock *sk = skb->sk;
+		iph->daddr = cp->daddr.in6;
+		ciph->saddr = cp->daddr.in6;
+	}
 
-		mtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);
-		if (mtu < 68) {
-			IP_VS_DBG_RL("%s(): mtu less than 68\n", __func__);
-			goto err_put;
+	/* the TCP/UDP port */
+	if (IPPROTO_TCP == ciph->nexthdr || IPPROTO_UDP == ciph->nexthdr) {
+		__be16 *ports = (void *)ciph + sizeof(struct ipv6hdr);
+
+		if (fullnat) {
+			if (inout) {
+				ports[0] = cp->cport;
+				if (IPPROTO_TCP == ciph->nexthdr) {
+					__be32 *seqs = (__be32 *)ports;
+					seqs[1] = htonl(ntohl(seqs[1]) -
+							cp->fnat_seq.delta);
+				}
+			} else
+				ports[1] = cp->lport;
+		}
+
+		if (inout)
+			ports[1] = cp->vport;
+		else {
+			ports[0] = cp->dport;
+			if (IPPROTO_TCP == ciph->nexthdr) {
+				__be32 *seqs = (__be32 *)ports;
+				seqs[1] = htonl(ntohl(seqs[1]) -
+						cp->syn_proxy_seq.delta);
+			}
 		}
-		ort = skb_rtable(skb);
-		if (!skb->dev && sk && sk->sk_state != TCP_TIME_WAIT)
-			ort->dst.ops->update_pmtu(&ort->dst, sk, NULL, mtu);
-		/* MTU check allowed? */
-		df = sysctl_pmtu_disc(ipvs) ? iph->frag_off & htons(IP_DF) : 0;
 	}
 
+	/* And finally the ICMP checksum */
+	icmph->icmp6_cksum = 0;
+	/* TODO IPv6: is this correct for ICMPv6? */
+	ip_vs_checksum_complete(skb, icmp_offset);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	if (inout)
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered outgoing ICMPv6");
+	else
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered incoming ICMPv6");
+}
+#endif
+
+/* Response transmit icmp to client
+ * Used for NAT/LOCAL.
+ */
+int
+ip_vs_normal_response_icmp_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+				struct ip_vs_conn *cp, int offset)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	if (!skb_make_writable(skb, offset))
+		goto out;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto out;
+
 	/* MTU checking */
-	if (unlikely(df && skb->len > mtu && !skb_is_gso(skb))) {
-		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
-		IP_VS_DBG(1, "frag needed for %pI4\n", &iph->saddr);
-		goto err_put;
+	mtu = dst_mtu(&rt->dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "nat_response_icmp(): frag needed for");
+		goto out;
 	}
 
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto error_put;
+
+	/* drop old route */
 	skb_dst_drop(skb);
-	if (noref) {
-		if (!local)
-			skb_dst_set_noref(skb, &rt->dst);
-		else
-			skb_dst_set(skb, dst_clone(&rt->dst));
-	} else
-		skb_dst_set(skb, &rt->dst);
+	skb_dst_set(skb, &rt->dst);
+
+	ip_vs_nat_icmp(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+error_put:
+	ip_rt_put(rt);
+out:
+	return NF_DROP;
+}
 
-	return local;
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_normal_response_icmp_xmit_v6(struct sk_buff *skb,
+				   struct ip_vs_protocol *pp,
+				   struct ip_vs_conn *cp, int offset)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	if (!skb_make_writable(skb, offset))
+		goto out;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto out;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto out;
+	}
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	ip_vs_nat_icmp_v6(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+error_put:
+	dst_release(&rt->dst);
+out:
+	return NF_DROP;
+}
 
-err_put:
-	if (!noref)
+#endif
+
+/* Response transmit icmp to client
+ * Used for NAT / local client / FULLNAT.
+ */
+int
+ip_vs_fnat_response_icmp_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int offset)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
 		ip_rt_put(rt);
-	return -1;
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "fnat_response_icmp(): frag needed for");
+		goto tx_error;
+	}
 
-err_unreach:
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, offset))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	ip_vs_nat_icmp(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
 	dst_link_failure(skb);
-	return -1;
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
 }
 
 #ifdef CONFIG_IP_VS_IPV6
 
-static inline int __ip_vs_is_local_route6(struct rt6_info *rt)
+int
+ip_vs_fnat_response_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+				 struct ip_vs_conn *cp, int offset)
 {
-	return rt->dst.dev && rt->dst.dev->flags & IFF_LOOPBACK;
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, offset))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	ip_vs_nat_icmp_v6(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->dst);
+	goto tx_error;
 }
 
-static struct dst_entry *
-__ip_vs_route_output_v6(struct net *net, struct in6_addr *daddr,
-			struct in6_addr *ret_saddr, int do_xfrm)
+#endif
+
+/* just for nat/fullnat mode */
+int
+ip_vs_fast_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+						struct ip_vs_conn *cp)
 {
-	struct dst_entry *dst;
-	struct flowi6 fl6 = {
-		.daddr = *daddr,
-	};
+	struct ethhdr *eth;
 
-	dst = ip6_route_output(net, NULL, &fl6);
-	if (dst->error)
-		goto out_err;
-	if (!ret_saddr)
-		return dst;
-	if (ipv6_addr_any(&fl6.saddr) &&
-	    ipv6_dev_get_saddr(net, ip6_dst_idev(dst)->dev,
-			       &fl6.daddr, 0, &fl6.saddr) < 0)
-		goto out_err;
-	if (do_xfrm) {
-		dst = xfrm_lookup(net, dst, flowi6_to_flowi(&fl6), NULL, 0);
-		if (IS_ERR(dst)) {
-			dst = NULL;
-			goto out_err;
-		}
+	if (!cp->indev)
+		goto err;
+	if (!gso_ok(skb, cp->indev) && (skb->len > cp->indev->mtu))
+		goto err;
+
+	/* Try to reuse skb */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if(unlikely(new_skb == NULL))
+			goto err;
+
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_SKB_COPY);
 	}
-	*ret_saddr = fl6.saddr;
-	return dst;
 
-out_err:
-	dst_release(dst);
-	IP_VS_DBG_RL("ip6_route_output error, dest: %pI6\n", daddr);
-	return NULL;
+	/* change ip, port. */
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
+		if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+			goto err;
+
+		ip_hdr(skb)->saddr = cp->vaddr.ip;
+		ip_hdr(skb)->daddr = cp->caddr.ip;
+	} else {
+		IP_VS_ERR_RL("L2 fast xmit support fullnat only!\n");
+		goto err;
+		/*if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+			goto err;
+
+		ip_hdr(skb)->saddr = cp->vaddr.ip;*/
+	}
+
+	ip_send_check(ip_hdr(skb));
+
+	skb->dev = cp->indev;
+
+	if(unlikely(skb_headroom(skb) < LL_RESERVED_SPACE(skb->dev))){
+		struct sk_buff *skb2;
+
+		IP_VS_ERR_RL("need more headroom! realloc skb\n");
+		skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(skb->dev));
+		if (skb2 == NULL)
+			goto err;
+		kfree_skb(skb);
+		skb = skb2;
+	}
+
+	if(likely(skb_mac_header_was_set(skb))) {
+		eth = eth_hdr(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+		skb->data = (unsigned char *)eth_hdr(skb);
+		skb->len += sizeof(struct ethhdr);
+	} else {
+		eth = (struct ethhdr *)skb_push(skb, sizeof(struct ethhdr));
+		skb_reset_mac_header(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+	}
+	skb->protocol = eth->h_proto = htons(ETH_P_IP);
+	skb->pkt_type = PACKET_OUTGOING;
+
+	IP_VS_DBG_RL("%s: send skb to client!\n", __func__);
+
+	/* Send the packet out */
+	do {
+		int ret = dev_queue_xmit(skb);
+		if (ret != 0)
+			IP_VS_ERR_RL("dev_queue_xmit failed! code:%d\n", ret);
+	}while(0);
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_PASS);
+	return 0;
+err:
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_REJECT);
+	return 1;
 }
 
-/*
- * Get route to destination or remote server
- */
-static int
-__ip_vs_get_out_rt_v6(struct sk_buff *skb, struct ip_vs_dest *dest,
-		      struct in6_addr *daddr, struct in6_addr *ret_saddr,
-		      struct ip_vs_iphdr *ipvsh, int do_xfrm, int rt_mode)
+#ifdef CONFIG_IP_VS_IPV6
+/* just for nat/fullnat mode */
+int
+ip_vs_fast_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+						struct ip_vs_conn *cp)
 {
-	struct net *net = dev_net(skb_dst(skb)->dev);
-	struct ip_vs_dest_dst *dest_dst;
-	struct rt6_info *rt;			/* Route to the other host */
-	struct rt6_info *ort;			/* Original route */
-	struct dst_entry *dst;
-	int mtu;
-	int local, noref = 1;
+	struct ethhdr *eth;
 
-	if (dest) {
-		dest_dst = __ip_vs_dst_check(dest);
-		if (likely(dest_dst))
-			rt = (struct rt6_info *) dest_dst->dst_cache;
-		else {
-			u32 cookie;
-
-			dest_dst = ip_vs_dest_dst_alloc();
-			spin_lock_bh(&dest->dst_lock);
-			if (!dest_dst) {
-				__ip_vs_dst_set(dest, NULL, NULL, 0);
-				spin_unlock_bh(&dest->dst_lock);
-				goto err_unreach;
-			}
-			dst = __ip_vs_route_output_v6(net, &dest->addr.in6,
-						      &dest_dst->dst_saddr.in6,
-						      do_xfrm);
-			if (!dst) {
-				__ip_vs_dst_set(dest, NULL, NULL, 0);
-				spin_unlock_bh(&dest->dst_lock);
-				ip_vs_dest_dst_free(dest_dst);
-				goto err_unreach;
-			}
-			rt = (struct rt6_info *) dst;
-			cookie = rt->rt6i_node ? rt->rt6i_node->fn_sernum : 0;
-			__ip_vs_dst_set(dest, dest_dst, &rt->dst, cookie);
-			spin_unlock_bh(&dest->dst_lock);
-			IP_VS_DBG(10, "new dst %pI6, src %pI6, refcnt=%d\n",
-				  &dest->addr.in6, &dest_dst->dst_saddr.in6,
-				  atomic_read(&rt->dst.__refcnt));
-		}
-		if (ret_saddr)
-			*ret_saddr = dest_dst->dst_saddr.in6;
+	if (!cp->indev)
+		goto err;
+	if (!gso_ok(skb, cp->indev) && (skb->len > cp->indev->mtu))
+		goto err;
+
+	/* Try to reuse skb if possible */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if(unlikely(new_skb == NULL))
+			goto err;
+
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_SKB_COPY);
+	}
+
+	/* change ip, port. */
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
+		if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+			goto err;
+
+		ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+		ipv6_hdr(skb)->daddr = cp->caddr.in6;
 	} else {
-		noref = 0;
-		dst = __ip_vs_route_output_v6(net, daddr, ret_saddr, do_xfrm);
-		if (!dst)
-			goto err_unreach;
-		rt = (struct rt6_info *) dst;
+		IP_VS_ERR_RL("L2 fast xmit support fullnat only!\n");
+		goto err;
+		/*if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+			goto err;
+
+		ipv6_hdr(skb)->saddr = cp->vaddr.in6;*/
 	}
 
-	local = __ip_vs_is_local_route6(rt);
-	if (!((local ? IP_VS_RT_MODE_LOCAL : IP_VS_RT_MODE_NON_LOCAL) &
-	      rt_mode)) {
-		IP_VS_DBG_RL("Stopping traffic to %s address, dest: %pI6c\n",
-			     local ? "local":"non-local", daddr);
-		goto err_put;
+	skb->dev = cp->indev;
+
+	if(unlikely(skb_headroom(skb) < LL_RESERVED_SPACE(skb->dev))){
+		struct sk_buff *skb2;
+
+		IP_VS_ERR_RL("need more headroom! realloc skb\n");
+		skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(skb->dev));
+		if (skb2 == NULL)
+			goto err;
+		kfree_skb(skb);
+		skb = skb2;
 	}
-	if (likely(!local)) {
-		if (unlikely((!skb->dev || skb->dev->flags & IFF_LOOPBACK) &&
-			     ipv6_addr_type(&ipv6_hdr(skb)->saddr) &
-					    IPV6_ADDR_LOOPBACK)) {
-			IP_VS_DBG_RL("Stopping traffic from loopback address "
-				     "%pI6c to non-local address, "
-				     "dest: %pI6c\n",
-				     &ipv6_hdr(skb)->saddr, daddr);
-			goto err_put;
-		}
+
+	if(likely(skb_mac_header_was_set(skb))) {
+		eth = eth_hdr(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+		skb->data = (unsigned char *)eth_hdr(skb);
+		skb->len += sizeof(struct ethhdr);
 	} else {
-		ort = (struct rt6_info *) skb_dst(skb);
-		if (!(rt_mode & IP_VS_RT_MODE_RDR) &&
-		    !__ip_vs_is_local_route6(ort)) {
-			IP_VS_DBG_RL("Redirect from non-local address %pI6c "
-				     "to local requires NAT method, "
-				     "dest: %pI6c\n",
-				     &ipv6_hdr(skb)->daddr, daddr);
-			goto err_put;
-		}
-		/* skb to local stack, preserve old route */
-		if (!noref)
-			dst_release(&rt->dst);
-		return local;
+		eth = (struct ethhdr *)skb_push(skb, sizeof(struct ethhdr));
+		skb_reset_mac_header(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
 	}
+	skb->protocol = eth->h_proto = htons(ETH_P_IPV6);
+	skb->pkt_type = PACKET_OUTGOING;
+
+	IP_VS_DBG_RL("%s: send skb to client!\n", __func__);
+	/* Send the packet out */
+	do {
+		int ret = dev_queue_xmit(skb);
+		if (ret != 0)
+			IP_VS_ERR_RL("dev_queue_xmit failed! code:%d\n", ret);
+	}while(0);
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_PASS);
+	return 0;
+err:
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_REJECT);
+	return 1;
+}
+#endif
+
+/* Response transmit to client
+ * Used for NAT/Local.
+ */
+int
+ip_vs_normal_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			   struct ip_vs_conn *cp, int ihl)
+{
+	struct rtable *rt;
+	int mtu;
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto drop;
+
+	/* mangle the packet */
+	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+		goto drop;
+
+	ip_hdr(skb)->saddr = cp->vaddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	/* For policy routing, packets originating from this
+	 * machine itself may be routed differently to packets
+	 * passing through.  We want this packet to be routed as
+	 * if it came from this machine itself.  So re-compute
+	 * the routing information.
+	 */
+//	if (ip_route_me_harder(skb, RTN_LOCAL) != 0)
+//		goto drop;
+
+	/* lookup route table */
+	if(!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(ip_hdr(skb)->tos))))
+		goto drop;
 
 	/* MTU checking */
-	if (likely(!(rt_mode & IP_VS_RT_MODE_TUNNEL)))
-		mtu = dst_mtu(&rt->dst);
-	else {
-		struct sock *sk = skb->sk;
-
-		mtu = dst_mtu(&rt->dst) - sizeof(struct ipv6hdr);
-		if (mtu < IPV6_MIN_MTU) {
-			IP_VS_DBG_RL("%s(): mtu less than %d\n", __func__,
-				     IPV6_MIN_MTU);
-			goto err_put;
-		}
-		ort = (struct rt6_info *) skb_dst(skb);
-		if (!skb->dev && sk && sk->sk_state != TCP_TIME_WAIT)
-			ort->dst.ops->update_pmtu(&ort->dst, sk, NULL, mtu);
+	mtu = dst_mtu(&rt->dst);
+	if ((skb->len > mtu) && (ip_hdr(skb)->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_nat_response(): frag needed for");
+		goto drop;
 	}
 
-	if (unlikely(__mtu_check_toobig_v6(skb, mtu))) {
-		if (!skb->dev)
-			skb->dev = net->loopback_dev;
-		/* only send ICMP too big on first fragment */
-		if (!ipvsh->fragoffs)
-			icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
-		IP_VS_DBG(1, "frag needed for %pI6c\n", &ipv6_hdr(skb)->saddr);
-		goto err_put;
+	if (skb_cow(skb, rt->dst.dev->hard_header_len)) {
+		ip_rt_put(rt);
+		goto drop;
 	}
 
+	/* drop old route */
 	skb_dst_drop(skb);
-	if (noref) {
-		if (!local)
-			skb_dst_set_noref(skb, &rt->dst);
-		else
-			skb_dst_set(skb, dst_clone(&rt->dst));
-	} else
-		skb_dst_set(skb, &rt->dst);
+	skb_dst_set(skb, &rt->dst);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+drop:
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_normal_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int ihl)
+{
+	struct rt6_info *rt;
+	int mtu;
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto drop;
+
+	/* mangle the packet */
+	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+		goto drop;
+
+	ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+
+	/* For policy routing, packets originating from this
+	 * machine itself may be routed differently to packets
+	 * passing through.  We want this packet to be routed as
+	 * if it came from this machine itself.  So re-compute
+	 * the routing information.
+	 */
+//	if (ip6_route_me_harder(skb) != 0)
+//		goto drop;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto drop;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response_v6(): frag needed for");
+		goto drop;
+	}
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len)) {
+		dst_release(&rt->dst);
+		goto drop;
+	}
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+drop:
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+#endif
+
+/* Response transmit to client
+ * Used for FULLNAT.
+ */
+int
+ip_vs_fnat_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			 struct ip_vs_conn *cp, int ihl)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	if(sysctl_ip_vs_fast_xmit && !ip_vs_fast_response_xmit(skb, pp, cp))
+		return NF_STOLEN;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (!gso_ok(skb, rt->dst.dev) && (skb->len > mtu) &&
+					(iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* mangle the packet */
+	if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+		goto tx_error;
+
+	ip_hdr(skb)->saddr = cp->vaddr.ip;
+	ip_hdr(skb)->daddr = cp->caddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_fnat_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			    struct ip_vs_conn *cp, int ihl)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	if(sysctl_ip_vs_fast_xmit && !ip_vs_fast_response_xmit_v6(skb, pp, cp))
+		return NF_STOLEN;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (!gso_ok(skb, rt->dst.dev) && (skb->len > mtu)) {
+		dst_release(&rt->dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response_v6(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* mangle the packet */
+	if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+		goto tx_error;
+
+	ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+	ipv6_hdr(skb)->daddr = cp->caddr.in6;
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->dst);
+	goto tx_error;
+}
+
+#endif
+
+/*
+ *      NULL transmitter (do nothing except return NF_ACCEPT)
+ */
+int
+ip_vs_null_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
+		struct ip_vs_protocol *pp)
+{
+	/* we do not touch skb and do not need pskb ptr */
+	return NF_ACCEPT;
+}
+
+/*
+ *      Bypass transmitter
+ *      Let packets bypass the destination when the destination is not
+ *      available, it may be only used in transparent cache cluster.
+ */
+int
+ip_vs_bypass_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
+		  struct ip_vs_protocol *pp)
+{
+	struct rtable *rt;	/* Route to the other host */
+	struct iphdr *iph = ip_hdr(skb);
+	u8 tos = iph->tos;
+	int mtu;
+	struct flowi4 fl = {
+		.daddr = iph->daddr,
+		.saddr = 0,
+		.flowi4_tos = RT_TOS(tos),
+	};
+
+	EnterFunction(10);
+
+	rt = ip_route_output_key(&init_net, &fl);
+	if (IS_ERR(rt)) {
+		IP_VS_DBG_RL("%s(): ip_route_output error, dest: %pI4\n",
+			     __func__, &iph->daddr);
+		goto tx_error_icmp;
+	}
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
+
+	/*
+	 * Call ip_send_check because we are not sure it is called
+	 * after ip_defrag. Is copy-on-write needed?
+	 */
+	if (unlikely((skb = skb_share_check(skb, GFP_ATOMIC)) == NULL)) {
+		ip_rt_put(rt);
+		return NF_STOLEN;
+	}
+	ip_send_check(ip_hdr(skb));
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	LeaveFunction(10);
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	LeaveFunction(10);
+	return NF_STOLEN;
+}
 
-	return local;
+#ifdef CONFIG_IP_VS_IPV6
+int
+ip_vs_bypass_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
+		     struct ip_vs_protocol *pp)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	int mtu;
+	struct flowi6 fl = {
+		.daddr = iph->daddr,
+		.saddr = {.s6_addr32 = {0, 0, 0.0},},
+	};
+
+	EnterFunction(10);
 
-err_put:
-	if (!noref)
+	rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
+	if (!rt) {
+		IP_VS_DBG_RL("%s(): ip6_route_output error, dest: %pI6\n",
+			     __func__, &iph->daddr);
+		goto tx_error_icmp;
+	}
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
 		dst_release(&rt->dst);
-	return -1;
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
+
+	/*
+	 * Call ip_send_check because we are not sure it is called
+	 * after ip_defrag. Is copy-on-write needed?
+	 */
+	skb = skb_share_check(skb, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		dst_release(&rt->dst);
+		return NF_STOLEN;
+	}
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->ignore_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	LeaveFunction(10);
+	return NF_STOLEN;
 
-err_unreach:
+      tx_error_icmp:
 	dst_link_failure(skb);
-	return -1;
+      tx_error:
+	kfree_skb(skb);
+	LeaveFunction(10);
+	return NF_STOLEN;
 }
 #endif
 
-
-/* return NF_ACCEPT to allow forwarding or other NF_xxx on error */
-static inline int ip_vs_tunnel_xmit_prepare(struct sk_buff *skb,
-					    struct ip_vs_conn *cp)
+void
+ip_vs_save_xmit_info(struct sk_buff *skb, struct ip_vs_protocol *pp,
+					struct ip_vs_conn *cp)
 {
-	int ret = NF_ACCEPT;
-
-	skb->ipvs_property = 1;
-	if (unlikely(cp->flags & IP_VS_CONN_F_NFCT))
-		ret = ip_vs_confirm_conntrack(skb);
-	if (ret == NF_ACCEPT) {
-		nf_reset(skb);
-		skb_forward_csum(skb);
-	}
-	return ret;
-}
+	if(!sysctl_ip_vs_fast_xmit)
+		return;
 
-/* return NF_STOLEN (sent) or NF_ACCEPT if local=1 (not sent) */
-static inline int ip_vs_nat_send_or_cont(int pf, struct sk_buff *skb,
-					 struct ip_vs_conn *cp, int local)
-{
-	int ret = NF_STOLEN;
+	if(!skb->dev) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_DEV_LOST);
+		IP_VS_DBG_RL("save_xmit_info, skb->dev is NULL. \n");
+		return;
+	}
+	IP_VS_DBG_RL("save_xmit_info, netdevice:%s\n", netdev_name(skb->dev));
 
-	skb->ipvs_property = 1;
-	if (likely(!(cp->flags & IP_VS_CONN_F_NFCT)))
-		ip_vs_notrack(skb);
-	else
-		ip_vs_update_conntrack(skb, cp, 1);
-	if (!local) {
-		skb_forward_csum(skb);
-		NF_HOOK(pf, NF_INET_LOCAL_OUT, NULL, skb,
-			NULL, skb_dst(skb)->dev, dst_output_sk);
-	} else
-		ret = NF_ACCEPT;
-	return ret;
-}
+	if(likely((skb->dev->type == ARPHRD_ETHER) &&
+					skb_mac_header_was_set(skb))) {
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
 
-/* return NF_STOLEN (sent) or NF_ACCEPT if local=1 (not sent) */
-static inline int ip_vs_send_or_cont(int pf, struct sk_buff *skb,
-				     struct ip_vs_conn *cp, int local)
-{
-	int ret = NF_STOLEN;
-
-	skb->ipvs_property = 1;
-	if (likely(!(cp->flags & IP_VS_CONN_F_NFCT)))
-		ip_vs_notrack(skb);
-	if (!local) {
-		skb_forward_csum(skb);
-		NF_HOOK(pf, NF_INET_LOCAL_OUT, NULL, skb,
-			NULL, skb_dst(skb)->dev, dst_output_sk);
-	} else
-		ret = NF_ACCEPT;
-	return ret;
-}
+		if(unlikely(cp->indev == NULL)) {
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
 
+		if (unlikely(cp->indev != skb->dev)) {
+			dev_put(cp->indev);
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
 
-/*
- *      NULL transmitter (do nothing except return NF_ACCEPT)
- */
-int
-ip_vs_null_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
-{
-	/* we do not touch skb and do not need pskb ptr */
-	return ip_vs_send_or_cont(NFPROTO_IPV4, skb, cp, 1);
+		memcpy(cp->src_hwaddr, eth->h_source, ETH_ALEN);
+		memcpy(cp->dst_hwaddr, eth->h_dest, ETH_ALEN);
+	} else {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_NO_MAC);
+		IP_VS_DBG_RL("save dev and mac failed!\n");
+	}
 }
 
-
 /*
- *      Bypass transmitter
- *      Let packets bypass the destination when the destination is not
- *      available, it may be only used in transparent cache cluster.
+ *      NAT transmitter (only for outside-to-inside nat forwarding)
+ *      Not used for related ICMP
  */
 int
-ip_vs_bypass_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		  struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+ip_vs_nat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
+	       struct ip_vs_protocol *pp)
 {
-	struct iphdr  *iph = ip_hdr(skb);
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
 
 	EnterFunction(10);
 
-	rcu_read_lock();
-	if (__ip_vs_get_out_rt(skb, NULL, iph->daddr, IP_VS_RT_MODE_NON_LOCAL,
-			       NULL) < 0)
+	/* check if it is a connection of no-client-port */
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
+		__be16 _pt, *p;
+		p = skb_header_pointer(skb, iph->ihl * 4, sizeof(_pt), &_pt);
+		if (p == NULL)
+			goto tx_error;
+		ip_vs_conn_fill_cport(cp, *p);
+		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
+	}
+
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_nat_xmit(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, sizeof(struct iphdr)))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* mangle the packet */
+	if (pp->dnat_handler && !pp->dnat_handler(skb, pp, cp))
 		goto tx_error;
+	ip_hdr(skb)->daddr = cp->daddr.ip;
+	ip_send_check(ip_hdr(skb));
 
-	ip_send_check(iph);
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After DNAT");
+
+	/* FIXME: when application helper enlarges the packet and the length
+	   is larger than the MTU of outgoing device, there will be still
+	   MTU problem. */
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	ip_vs_send_or_cont(NFPROTO_IPV4, skb, cp, 0);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET, skb, rt);
 
 	LeaveFunction(10);
 	return NF_STOLEN;
 
- tx_error:
-	kfree_skb(skb);
-	rcu_read_unlock();
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	LeaveFunction(10);
+	kfree_skb(skb);
 	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
 }
 
 #ifdef CONFIG_IP_VS_IPV6
 int
-ip_vs_bypass_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		     struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+ip_vs_nat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
+		  struct ip_vs_protocol *pp)
 {
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
 	EnterFunction(10);
 
-	rcu_read_lock();
-	if (__ip_vs_get_out_rt_v6(skb, NULL, &ipvsh->daddr.in6, NULL,
-				  ipvsh, 0, IP_VS_RT_MODE_NON_LOCAL) < 0)
+	/* check if it is a connection of no-client-port */
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
+		__be16 _pt, *p;
+		p = skb_header_pointer(skb, sizeof(struct ipv6hdr),
+				       sizeof(_pt), &_pt);
+		if (p == NULL)
+			goto tx_error;
+		ip_vs_conn_fill_cport(cp, *p);
+		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
+	}
+
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_nat_xmit_v6(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, sizeof(struct ipv6hdr)))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
+	/* mangle the packet */
+	if (pp->dnat_handler && !pp->dnat_handler(skb, pp, cp))
 		goto tx_error;
+	ipv6_hdr(skb)->daddr = cp->daddr.in6;
+
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After DNAT");
+
+	/* FIXME: when application helper enlarges the packet and the length
+	   is larger than the MTU of outgoing device, there will be still
+	   MTU problem. */
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	ip_vs_send_or_cont(NFPROTO_IPV6, skb, cp, 0);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET6, skb, rt);
 
 	LeaveFunction(10);
 	return NF_STOLEN;
 
- tx_error:
-	kfree_skb(skb);
-	rcu_read_unlock();
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	LeaveFunction(10);
+	kfree_skb(skb);
 	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->dst);
+	goto tx_error;
 }
 #endif
 
 /*
- *      NAT transmitter (only for outside-to-inside nat forwarding)
+ *      FULLNAT transmitter (only for outside-to-inside fullnat forwarding)
  *      Not used for related ICMP
  */
 int
-ip_vs_nat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-	       struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+ip_vs_fnat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
+		struct ip_vs_protocol *pp)
 {
-	struct rtable *rt;		/* Route to the other host */
-	int local, rc, was_input;
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
 
 	EnterFunction(10);
 
-	rcu_read_lock();
 	/* check if it is a connection of no-client-port */
 	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
 		__be16 _pt, *p;
-
-		p = skb_header_pointer(skb, ipvsh->len, sizeof(_pt), &_pt);
+		p = skb_header_pointer(skb, iph->ihl * 4, sizeof(_pt), &_pt);
 		if (p == NULL)
 			goto tx_error;
 		ip_vs_conn_fill_cport(cp, *p);
 		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
 	}
 
-	was_input = rt_is_input_route(skb_rtable(skb));
-	local = __ip_vs_get_out_rt(skb, cp->dest, cp->daddr.ip,
-				   IP_VS_RT_MODE_LOCAL |
-				   IP_VS_RT_MODE_NON_LOCAL |
-				   IP_VS_RT_MODE_RDR, NULL);
-	if (local < 0)
-		goto tx_error;
-	rt = skb_rtable(skb);
-	/*
-	 * Avoid duplicate tuple in reply direction for NAT traffic
-	 * to local address when connection is sync-ed
-	 */
-#if IS_ENABLED(CONFIG_NF_CONNTRACK)
-	if (cp->flags & IP_VS_CONN_F_SYNC && local) {
-		enum ip_conntrack_info ctinfo;
-		struct nf_conn *ct = nf_ct_get(skb, &ctinfo);
-
-		if (ct && !nf_ct_is_untracked(ct)) {
-			IP_VS_DBG_RL_PKT(10, AF_INET, pp, skb, 0,
-					 "ip_vs_nat_xmit(): "
-					 "stopping DNAT to local address");
-			goto tx_error;
-		}
-	}
-#endif
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
 
-	/* From world but DNAT to loopback address? */
-	if (local && ipv4_is_loopback(cp->daddr.ip) && was_input) {
-		IP_VS_DBG_RL_PKT(1, AF_INET, pp, skb, 0, "ip_vs_nat_xmit(): "
-				 "stopping DNAT to loopback address");
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (!gso_ok(skb, rt->dst.dev) && (skb->len > mtu) &&
+					(iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_fnat_xmit(): frag needed for");
 		goto tx_error;
 	}
 
+	ip_vs_save_xmit_info(skb, pp, cp);
+
 	/* copy-on-write the packet before mangling it */
 	if (!skb_make_writable(skb, sizeof(struct iphdr)))
-		goto tx_error;
+		goto tx_error_put;
 
 	if (skb_cow(skb, rt->dst.dev->hard_header_len))
-		goto tx_error;
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
 
 	/* mangle the packet */
-	if (pp->dnat_handler && !pp->dnat_handler(skb, pp, cp, ipvsh))
+	if (pp->fnat_in_handler && !pp->fnat_in_handler(&skb, pp, cp))
 		goto tx_error;
+	ip_hdr(skb)->saddr = cp->laddr.ip;
 	ip_hdr(skb)->daddr = cp->daddr.ip;
 	ip_send_check(ip_hdr(skb));
 
-	IP_VS_DBG_PKT(10, AF_INET, pp, skb, 0, "After DNAT");
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After FNAT-IN");
 
 	/* FIXME: when application helper enlarges the packet and the length
 	   is larger than the MTU of outgoing device, there will be still
@@ -686,88 +1449,78 @@ ip_vs_nat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	rc = ip_vs_nat_send_or_cont(NFPROTO_IPV4, skb, cp, local);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET, skb, rt);
 
 	LeaveFunction(10);
-	return rc;
+	return NF_STOLEN;
 
-  tx_error:
-	kfree_skb(skb);
-	rcu_read_unlock();
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	LeaveFunction(10);
+	kfree_skb(skb);
 	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
 }
 
 #ifdef CONFIG_IP_VS_IPV6
 int
-ip_vs_nat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		  struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+ip_vs_fnat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
+		   struct ip_vs_protocol *pp)
 {
-	struct rt6_info *rt;		/* Route to the other host */
-	int local, rc;
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
 
 	EnterFunction(10);
 
-	rcu_read_lock();
 	/* check if it is a connection of no-client-port */
-	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT && !ipvsh->fragoffs)) {
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
 		__be16 _pt, *p;
-		p = skb_header_pointer(skb, ipvsh->len, sizeof(_pt), &_pt);
+		p = skb_header_pointer(skb, sizeof(struct ipv6hdr),
+				       sizeof(_pt), &_pt);
 		if (p == NULL)
 			goto tx_error;
 		ip_vs_conn_fill_cport(cp, *p);
 		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
 	}
 
-	local = __ip_vs_get_out_rt_v6(skb, cp->dest, &cp->daddr.in6, NULL,
-				      ipvsh, 0,
-				      IP_VS_RT_MODE_LOCAL |
-				      IP_VS_RT_MODE_NON_LOCAL |
-				      IP_VS_RT_MODE_RDR);
-	if (local < 0)
-		goto tx_error;
-	rt = (struct rt6_info *) skb_dst(skb);
-	/*
-	 * Avoid duplicate tuple in reply direction for NAT traffic
-	 * to local address when connection is sync-ed
-	 */
-#if IS_ENABLED(CONFIG_NF_CONNTRACK)
-	if (cp->flags & IP_VS_CONN_F_SYNC && local) {
-		enum ip_conntrack_info ctinfo;
-		struct nf_conn *ct = nf_ct_get(skb, &ctinfo);
-
-		if (ct && !nf_ct_is_untracked(ct)) {
-			IP_VS_DBG_RL_PKT(10, AF_INET6, pp, skb, 0,
-					 "ip_vs_nat_xmit_v6(): "
-					 "stopping DNAT to local address");
-			goto tx_error;
-		}
-	}
-#endif
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
 
-	/* From world but DNAT to loopback address? */
-	if (local && skb->dev && !(skb->dev->flags & IFF_LOOPBACK) &&
-	    ipv6_addr_type(&rt->rt6i_dst.addr) & IPV6_ADDR_LOOPBACK) {
-		IP_VS_DBG_RL_PKT(1, AF_INET6, pp, skb, 0,
-				 "ip_vs_nat_xmit_v6(): "
-				 "stopping DNAT to loopback address");
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (!gso_ok(skb, rt->dst.dev) && (skb->len > mtu)) {
+		dst_release(&rt->dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_fnat_xmit_v6(): frag needed for");
 		goto tx_error;
 	}
 
+	ip_vs_save_xmit_info(skb, pp, cp);
+
 	/* copy-on-write the packet before mangling it */
 	if (!skb_make_writable(skb, sizeof(struct ipv6hdr)))
-		goto tx_error;
+		goto tx_error_put;
 
 	if (skb_cow(skb, rt->dst.dev->hard_header_len))
-		goto tx_error;
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
 
 	/* mangle the packet */
-	if (pp->dnat_handler && !pp->dnat_handler(skb, pp, cp, ipvsh))
+	if (pp->fnat_in_handler && !pp->fnat_in_handler(&skb, pp, cp))
 		goto tx_error;
+	ipv6_hdr(skb)->saddr = cp->laddr.in6;
 	ipv6_hdr(skb)->daddr = cp->daddr.in6;
 
-	IP_VS_DBG_PKT(10, AF_INET6, pp, skb, 0, "After DNAT");
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After FNAT-IN");
 
 	/* FIXME: when application helper enlarges the packet and the length
 	   is larger than the MTU of outgoing device, there will be still
@@ -776,21 +1529,23 @@ ip_vs_nat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	rc = ip_vs_nat_send_or_cont(NFPROTO_IPV6, skb, cp, local);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET6, skb, rt);
 
 	LeaveFunction(10);
-	return rc;
+	return NF_STOLEN;
 
-tx_error:
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	LeaveFunction(10);
 	kfree_skb(skb);
-	rcu_read_unlock();
 	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->dst);
+	goto tx_error;
 }
 #endif
 
-
 /*
  *   IP Tunneling transmitter
  *
@@ -812,57 +1567,76 @@ tx_error:
  */
 int
 ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		  struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+		  struct ip_vs_protocol *pp)
 {
-	struct netns_ipvs *ipvs = net_ipvs(skb_net(skb));
-	struct rtable *rt;			/* Route to the other host */
-	__be32 saddr;				/* Source for tunnel */
-	struct net_device *tdev;		/* Device to other host */
-	struct iphdr  *old_iph = ip_hdr(skb);
-	u8     tos = old_iph->tos;
-	__be16 df;
-	struct iphdr  *iph;			/* Our new IP header */
-	unsigned int max_headroom;		/* The extra header space needed */
-	int ret, local;
+	struct rtable *rt;	/* Route to the other host */
+	struct rt6_info *ort;
+	struct net_device *tdev;	/* Device to other host */
+	struct iphdr *old_iph = ip_hdr(skb);
+	u8 tos = old_iph->tos;
+	__be16 df = old_iph->frag_off;
+	sk_buff_data_t old_transport_header = skb->transport_header;
+	struct iphdr *iph;	/* Our new IP header */
+	unsigned int max_headroom;	/* The extra header space needed */
+	int mtu;
 
 	EnterFunction(10);
 
-	rcu_read_lock();
-	local = __ip_vs_get_out_rt(skb, cp->dest, cp->daddr.ip,
-				   IP_VS_RT_MODE_LOCAL |
-				   IP_VS_RT_MODE_NON_LOCAL |
-				   IP_VS_RT_MODE_CONNECT |
-				   IP_VS_RT_MODE_TUNNEL, &saddr);
-	if (local < 0)
+	if (skb->protocol != htons(ETH_P_IP)) {
+		IP_VS_DBG_RL("%s(): protocol error, "
+			     "ETH_P_IP: %d, skb protocol: %d\n",
+			     __func__, htons(ETH_P_IP), skb->protocol);
 		goto tx_error;
-	if (local) {
-		rcu_read_unlock();
-		return ip_vs_send_or_cont(NFPROTO_IPV4, skb, cp, 1);
 	}
 
-	rt = skb_rtable(skb);
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(tos))))
+		goto tx_error_icmp;
+
 	tdev = rt->dst.dev;
 
-	/* Copy DF, reset fragment offset and MF */
-	df = sysctl_pmtu_disc(ipvs) ? old_iph->frag_off & htons(IP_DF) : 0;
+	mtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);
+	if (mtu < 68) {
+		ip_rt_put(rt);
+		IP_VS_DBG_RL("%s(): mtu less than 68\n", __func__);
+		goto tx_error;
+	}
+
+        ort = (struct rt6_info *) skb_dst(skb);
+        if (!skb->dev && skb->sk && skb->sk->sk_state != TCP_TIME_WAIT)
+                ort->dst.ops->update_pmtu(&ort->dst, skb->sk, NULL, mtu);
+
+	df |= (old_iph->frag_off & htons(IP_DF));
+
+	if ((old_iph->frag_off & htons(IP_DF))
+	    && mtu < ntohs(old_iph->tot_len)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		ip_rt_put(rt);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
 
 	/*
 	 * Okay, now see if we can stuff it in the buffer as-is.
 	 */
 	max_headroom = LL_RESERVED_SPACE(tdev) + sizeof(struct iphdr);
 
-	if (skb_headroom(skb) < max_headroom || skb_cloned(skb)) {
+	if (skb_headroom(skb) < max_headroom
+	    || skb_cloned(skb) || skb_shared(skb)) {
 		struct sk_buff *new_skb =
-			skb_realloc_headroom(skb, max_headroom);
-
-		if (!new_skb)
-			goto tx_error;
-		consume_skb(skb);
+		    skb_realloc_headroom(skb, max_headroom);
+		if (!new_skb) {
+			ip_rt_put(rt);
+			kfree_skb(skb);
+			IP_VS_ERR_RL("%s(): no memory\n", __func__);
+			return NF_STOLEN;
+		}
+		kfree_skb(skb);
 		skb = new_skb;
 		old_iph = ip_hdr(skb);
 	}
 
-	skb->transport_header = skb->network_header;
+	skb->transport_header = old_transport_header;
 
 	/* fix old IP header checksum */
 	ip_send_check(old_iph);
@@ -871,37 +1645,38 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	skb_reset_network_header(skb);
 	memset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));
 
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
 	/*
-	 *	Push down and install the IPIP header.
+	 *      Push down and install the IPIP header.
 	 */
-	iph			=	ip_hdr(skb);
-	iph->version		=	4;
-	iph->ihl		=	sizeof(struct iphdr)>>2;
-	iph->frag_off		=	df;
-	iph->protocol		=	IPPROTO_IPIP;
-	iph->tos		=	tos;
-	iph->daddr		=	cp->daddr.ip;
-	iph->saddr		=	saddr;
-	iph->ttl		=	old_iph->ttl;
+	iph = ip_hdr(skb);
+	iph->version = 4;
+	iph->ihl = sizeof(struct iphdr) >> 2;
+	iph->frag_off = df;
+	iph->protocol = IPPROTO_IPIP;
+	iph->tos = tos;
+	//TODO
+	//iph->daddr = rt->rt_dst;
+	//iph->saddr = rt->rt_src;
+	iph->ttl = old_iph->ttl;
 	ip_select_ident(skb, &rt->dst, NULL);
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	ret = ip_vs_tunnel_xmit_prepare(skb, cp);
-	if (ret == NF_ACCEPT)
-		ip_local_out(skb);
-	else if (ret == NF_DROP)
-		kfree_skb(skb);
-	rcu_read_unlock();
+	ip_local_out(skb);
 
 	LeaveFunction(10);
 
 	return NF_STOLEN;
 
-  tx_error:
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	kfree_skb(skb);
-	rcu_read_unlock();
 	LeaveFunction(10);
 	return NF_STOLEN;
 }
@@ -909,131 +1684,166 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 #ifdef CONFIG_IP_VS_IPV6
 int
 ip_vs_tunnel_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		     struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+		     struct ip_vs_protocol *pp)
 {
-	struct rt6_info *rt;		/* Route to the other host */
-	struct in6_addr saddr;		/* Source for tunnel */
+	struct rt6_info *rt;	/* Route to the other host */
+	struct rt6_info *ort;			/* Original route */
 	struct net_device *tdev;	/* Device to other host */
-	struct ipv6hdr  *old_iph = ipv6_hdr(skb);
-	struct ipv6hdr  *iph;		/* Our new IP header */
+	struct ipv6hdr *old_iph = ipv6_hdr(skb);
+	sk_buff_data_t old_transport_header = skb->transport_header;
+	struct ipv6hdr *iph;	/* Our new IP header */
 	unsigned int max_headroom;	/* The extra header space needed */
-	int ret, local;
+	int mtu;
 
 	EnterFunction(10);
 
-	rcu_read_lock();
-	local = __ip_vs_get_out_rt_v6(skb, cp->dest, &cp->daddr.in6,
-				      &saddr, ipvsh, 1,
-				      IP_VS_RT_MODE_LOCAL |
-				      IP_VS_RT_MODE_NON_LOCAL |
-				      IP_VS_RT_MODE_TUNNEL);
-	if (local < 0)
+	if (skb->protocol != htons(ETH_P_IPV6)) {
+		IP_VS_DBG_RL("%s(): protocol error, "
+			     "ETH_P_IPV6: %d, skb protocol: %d\n",
+			     __func__, htons(ETH_P_IPV6), skb->protocol);
 		goto tx_error;
-	if (local) {
-		rcu_read_unlock();
-		return ip_vs_send_or_cont(NFPROTO_IPV6, skb, cp, 1);
 	}
 
-	rt = (struct rt6_info *) skb_dst(skb);
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
+
 	tdev = rt->dst.dev;
 
+	mtu = dst_mtu(&rt->dst) - sizeof(struct ipv6hdr);
+	/* TODO IPv6: do we need this check in IPv6? */
+	if (mtu < 1280) {
+		dst_release(&rt->dst);
+		IP_VS_DBG_RL("%s(): mtu less than 1280\n", __func__);
+		goto tx_error;
+	}
+	ort = (struct rt6_info *) skb_dst(skb);
+	if (!skb->dev && skb->sk && skb->sk->sk_state != TCP_TIME_WAIT)
+		ort->dst.ops->update_pmtu(&ort->dst, skb->sk, NULL, mtu);
+
+	if (mtu < ntohs(old_iph->payload_len) + sizeof(struct ipv6hdr)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		dst_release(&rt->dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
+
 	/*
 	 * Okay, now see if we can stuff it in the buffer as-is.
 	 */
 	max_headroom = LL_RESERVED_SPACE(tdev) + sizeof(struct ipv6hdr);
 
-	if (skb_headroom(skb) < max_headroom || skb_cloned(skb)) {
+	if (skb_headroom(skb) < max_headroom
+	    || skb_cloned(skb) || skb_shared(skb)) {
 		struct sk_buff *new_skb =
-			skb_realloc_headroom(skb, max_headroom);
-
-		if (!new_skb)
-			goto tx_error;
-		consume_skb(skb);
+		    skb_realloc_headroom(skb, max_headroom);
+		if (!new_skb) {
+			dst_release(&rt->dst);
+			kfree_skb(skb);
+			IP_VS_ERR_RL("%s(): no memory\n", __func__);
+			return NF_STOLEN;
+		}
+		kfree_skb(skb);
 		skb = new_skb;
 		old_iph = ipv6_hdr(skb);
 	}
 
-	skb->transport_header = skb->network_header;
+	skb->transport_header = old_transport_header;
 
 	skb_push(skb, sizeof(struct ipv6hdr));
 	skb_reset_network_header(skb);
 	memset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));
 
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
 	/*
-	 *	Push down and install the IPIP header.
+	 *      Push down and install the IPIP header.
 	 */
-	iph			=	ipv6_hdr(skb);
-	iph->version		=	6;
-	iph->nexthdr		=	IPPROTO_IPV6;
-	iph->payload_len	=	old_iph->payload_len;
+	iph = ipv6_hdr(skb);
+	iph->version = 6;
+	iph->nexthdr = IPPROTO_IPV6;
+	iph->payload_len = old_iph->payload_len;
 	be16_add_cpu(&iph->payload_len, sizeof(*old_iph));
-	iph->priority		=	old_iph->priority;
+	iph->priority = old_iph->priority;
 	memset(&iph->flow_lbl, 0, sizeof(iph->flow_lbl));
-	iph->daddr = cp->daddr.in6;
-	iph->saddr = saddr;
-	iph->hop_limit		=	old_iph->hop_limit;
+	iph->daddr = rt->rt6i_dst.addr;
+	iph->saddr = cp->vaddr.in6;	/* rt->rt6i_src.addr; */
+	iph->hop_limit = old_iph->hop_limit;
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	ret = ip_vs_tunnel_xmit_prepare(skb, cp);
-	if (ret == NF_ACCEPT)
-		ip6_local_out(skb);
-	else if (ret == NF_DROP)
-		kfree_skb(skb);
-	rcu_read_unlock();
+	ip6_local_out(skb);
 
 	LeaveFunction(10);
 
 	return NF_STOLEN;
 
-tx_error:
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	kfree_skb(skb);
-	rcu_read_unlock();
 	LeaveFunction(10);
 	return NF_STOLEN;
 }
 #endif
 
-
 /*
  *      Direct Routing transmitter
  *      Used for ANY protocol
  */
 int
 ip_vs_dr_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-	      struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+	      struct ip_vs_protocol *pp)
 {
-	int local;
+	struct rtable *rt;	/* Route to the other host */
+	struct iphdr *iph = ip_hdr(skb);
+	int mtu;
 
 	EnterFunction(10);
 
-	rcu_read_lock();
-	local = __ip_vs_get_out_rt(skb, cp->dest, cp->daddr.ip,
-				   IP_VS_RT_MODE_LOCAL |
-				   IP_VS_RT_MODE_NON_LOCAL |
-				   IP_VS_RT_MODE_KNOWN_NH, NULL);
-	if (local < 0)
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if ((iph->frag_off & htons(IP_DF)) && skb->len > mtu) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		ip_rt_put(rt);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
-	if (local) {
-		rcu_read_unlock();
-		return ip_vs_send_or_cont(NFPROTO_IPV4, skb, cp, 1);
 	}
 
+	/*
+	 * Call ip_send_check because we are not sure it is called
+	 * after ip_defrag. Is copy-on-write needed?
+	 */
+	if (unlikely((skb = skb_share_check(skb, GFP_ATOMIC)) == NULL)) {
+		ip_rt_put(rt);
+		return NF_STOLEN;
+	}
 	ip_send_check(ip_hdr(skb));
 
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	ip_vs_send_or_cont(NFPROTO_IPV4, skb, cp, 0);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET, skb, rt);
 
 	LeaveFunction(10);
 	return NF_STOLEN;
 
-  tx_error:
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	kfree_skb(skb);
-	rcu_read_unlock();
 	LeaveFunction(10);
 	return NF_STOLEN;
 }
@@ -1041,64 +1851,79 @@ ip_vs_dr_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 #ifdef CONFIG_IP_VS_IPV6
 int
 ip_vs_dr_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		 struct ip_vs_protocol *pp, struct ip_vs_iphdr *ipvsh)
+		 struct ip_vs_protocol *pp)
 {
-	int local;
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
 
 	EnterFunction(10);
 
-	rcu_read_lock();
-	local = __ip_vs_get_out_rt_v6(skb, cp->dest, &cp->daddr.in6, NULL,
-				      ipvsh, 0,
-				      IP_VS_RT_MODE_LOCAL |
-				      IP_VS_RT_MODE_NON_LOCAL);
-	if (local < 0)
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		dst_release(&rt->dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
-	if (local) {
-		rcu_read_unlock();
-		return ip_vs_send_or_cont(NFPROTO_IPV6, skb, cp, 1);
 	}
 
+	/*
+	 * Call ip_send_check because we are not sure it is called
+	 * after ip_defrag. Is copy-on-write needed?
+	 */
+	skb = skb_share_check(skb, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		dst_release(&rt->dst);
+		return NF_STOLEN;
+	}
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
+
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	ip_vs_send_or_cont(NFPROTO_IPV6, skb, cp, 0);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET6, skb, rt);
 
 	LeaveFunction(10);
 	return NF_STOLEN;
 
-tx_error:
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
 	kfree_skb(skb);
-	rcu_read_unlock();
 	LeaveFunction(10);
 	return NF_STOLEN;
 }
 #endif
 
-
 /*
  *	ICMP packet transmitter
  *	called by the ip_vs_in_icmp
  */
 int
 ip_vs_icmp_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
-		struct ip_vs_protocol *pp, int offset, unsigned int hooknum,
-		struct ip_vs_iphdr *iph)
+		struct ip_vs_protocol *pp, int offset)
 {
-	struct rtable	*rt;	/* Route to the other host */
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
 	int rc;
-	int local;
-	int rt_mode, was_input;
 
 	EnterFunction(10);
 
 	/* The ICMP packet for VS/TUN, VS/DR and LOCALNODE will be
 	   forwarded directly here, because there is no need to
 	   translate address/port back */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) &&
+	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		if (cp->packet_xmit)
-			rc = cp->packet_xmit(skb, cp, pp, iph);
+			rc = cp->packet_xmit(skb, cp, pp);
 		else
 			rc = NF_ACCEPT;
 		/* do not touch skb anymore */
@@ -1109,88 +1934,72 @@ ip_vs_icmp_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/*
 	 * mangle and send the packet here (only for VS/NAT)
 	 */
-	was_input = rt_is_input_route(skb_rtable(skb));
-
-	/* LOCALNODE from FORWARD hook is not supported */
-	rt_mode = (hooknum != NF_INET_FORWARD) ?
-		  IP_VS_RT_MODE_LOCAL | IP_VS_RT_MODE_NON_LOCAL |
-		  IP_VS_RT_MODE_RDR : IP_VS_RT_MODE_NON_LOCAL;
-	rcu_read_lock();
-	local = __ip_vs_get_out_rt(skb, cp->dest, cp->daddr.ip, rt_mode, NULL);
-	if (local < 0)
-		goto tx_error;
-	rt = skb_rtable(skb);
 
-	/*
-	 * Avoid duplicate tuple in reply direction for NAT traffic
-	 * to local address when connection is sync-ed
-	 */
-#if IS_ENABLED(CONFIG_NF_CONNTRACK)
-	if (cp->flags & IP_VS_CONN_F_SYNC && local) {
-		enum ip_conntrack_info ctinfo;
-		struct nf_conn *ct = nf_ct_get(skb, &ctinfo);
-
-		if (ct && !nf_ct_is_untracked(ct)) {
-			IP_VS_DBG(10, "%s(): "
-				  "stopping DNAT to local address %pI4\n",
-				  __func__, &cp->daddr.ip);
-			goto tx_error;
-		}
-	}
-#endif
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(ip_hdr(skb)->tos))))
+		goto tx_error_icmp;
 
-	/* From world but DNAT to loopback address? */
-	if (local && ipv4_is_loopback(cp->daddr.ip) && was_input) {
-		IP_VS_DBG(1, "%s(): "
-			  "stopping DNAT to loopback %pI4\n",
-			  __func__, &cp->daddr.ip);
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if ((skb->len > mtu) && (ip_hdr(skb)->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
 	}
 
 	/* copy-on-write the packet before mangling it */
 	if (!skb_make_writable(skb, offset))
-		goto tx_error;
+		goto tx_error_put;
 
 	if (skb_cow(skb, rt->dst.dev->hard_header_len))
-		goto tx_error;
+		goto tx_error_put;
+
+	/* drop the old route when skb is not shared */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
 
 	ip_vs_nat_icmp(skb, pp, cp, 0);
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	rc = ip_vs_nat_send_or_cont(NFPROTO_IPV4, skb, cp, local);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	rc = NF_STOLEN;
 	goto out;
 
-  tx_error:
-	kfree_skb(skb);
-	rcu_read_unlock();
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	dev_kfree_skb(skb);
 	rc = NF_STOLEN;
-  out:
+      out:
 	LeaveFunction(10);
 	return rc;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
 }
 
 #ifdef CONFIG_IP_VS_IPV6
 int
 ip_vs_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		struct ip_vs_protocol *pp, int offset, unsigned int hooknum,
-		struct ip_vs_iphdr *ipvsh)
+		   struct ip_vs_protocol *pp, int offset)
 {
-	struct rt6_info	*rt;	/* Route to the other host */
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
 	int rc;
-	int local;
-	int rt_mode;
 
 	EnterFunction(10);
 
 	/* The ICMP packet for VS/TUN, VS/DR and LOCALNODE will be
 	   forwarded directly here, because there is no need to
 	   translate address/port back */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) &&
+	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		if (cp->packet_xmit)
-			rc = cp->packet_xmit(skb, cp, pp, ipvsh);
+			rc = cp->packet_xmit(skb, cp, pp);
 		else
 			rc = NF_ACCEPT;
 		/* do not touch skb anymore */
@@ -1202,65 +2011,51 @@ ip_vs_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	 * mangle and send the packet here (only for VS/NAT)
 	 */
 
-	/* LOCALNODE from FORWARD hook is not supported */
-	rt_mode = (hooknum != NF_INET_FORWARD) ?
-		  IP_VS_RT_MODE_LOCAL | IP_VS_RT_MODE_NON_LOCAL |
-		  IP_VS_RT_MODE_RDR : IP_VS_RT_MODE_NON_LOCAL;
-	rcu_read_lock();
-	local = __ip_vs_get_out_rt_v6(skb, cp->dest, &cp->daddr.in6, NULL,
-				      ipvsh, 0, rt_mode);
-	if (local < 0)
-		goto tx_error;
-	rt = (struct rt6_info *) skb_dst(skb);
-	/*
-	 * Avoid duplicate tuple in reply direction for NAT traffic
-	 * to local address when connection is sync-ed
-	 */
-#if IS_ENABLED(CONFIG_NF_CONNTRACK)
-	if (cp->flags & IP_VS_CONN_F_SYNC && local) {
-		enum ip_conntrack_info ctinfo;
-		struct nf_conn *ct = nf_ct_get(skb, &ctinfo);
-
-		if (ct && !nf_ct_is_untracked(ct)) {
-			IP_VS_DBG(10, "%s(): "
-				  "stopping DNAT to local address %pI6\n",
-				  __func__, &cp->daddr.in6);
-			goto tx_error;
-		}
-	}
-#endif
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
 
-	/* From world but DNAT to loopback address? */
-	if (local && skb->dev && !(skb->dev->flags & IFF_LOOPBACK) &&
-	    ipv6_addr_type(&rt->rt6i_dst.addr) & IPV6_ADDR_LOOPBACK) {
-		IP_VS_DBG(1, "%s(): "
-			  "stopping DNAT to loopback %pI6\n",
-			  __func__, &cp->daddr.in6);
+	/* MTU checking */
+	mtu = dst_mtu(&rt->dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
 	}
 
 	/* copy-on-write the packet before mangling it */
 	if (!skb_make_writable(skb, offset))
-		goto tx_error;
+		goto tx_error_put;
 
 	if (skb_cow(skb, rt->dst.dev->hard_header_len))
-		goto tx_error;
+		goto tx_error_put;
+
+	/* drop the old route when skb is not shared */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->dst);
 
 	ip_vs_nat_icmp_v6(skb, pp, cp, 0);
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->ignore_df = 1;
 
-	rc = ip_vs_nat_send_or_cont(NFPROTO_IPV6, skb, cp, local);
-	rcu_read_unlock();
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	rc = NF_STOLEN;
 	goto out;
 
-tx_error:
-	kfree_skb(skb);
-	rcu_read_unlock();
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	dev_kfree_skb(skb);
 	rc = NF_STOLEN;
-out:
+      out:
 	LeaveFunction(10);
 	return rc;
+      tx_error_put:
+	dst_release(&rt->dst);
+	goto tx_error;
 }
 #endif
-- 
1.8.3.1

